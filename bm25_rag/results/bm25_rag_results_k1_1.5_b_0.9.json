{
  "system_name": "BM25 RAG",
  "total_chunks": 21430,
  "chunk_size": 500,
  "chunk_overlap": 50,
  "k1": 1.5,
  "b": 0.9,
  "llm_model": "fallback (–±–µ–∑ LLM)",
  "queries": [
    {
      "question": "What is Retrieval-Augmented Generation (RAG)?",
      "answer": "h.\n4.4\nOpen-domain QA Evaluation on\nRetrieval-Augmented Language Models\nAnother aspect of the choice of granularity lies\nin what units should be used in the prompt for\nretrieval-augmented language models. For large\nlanguage models, retrieval-augmented generation\nis achieved by prepending retrieved units to user in-\nstruction and taking them as the input for language\nmodels. We aim to understand the implications of\nusing retrieved units of different granularity within\nthe same computational budge\n\nSPRING is adaptable to different retrievers and various\nnumbers of retrieved results. Consequently, there is no need\nto retrain SPRING with each update to the retrieval system,\nenhancing its practicality and efficiency.\nRelated Work\nRetrieval-Augmented Generation\nCompared to standard\ntext generation, retrieval-augmented generation (RAG) in-\ncorporates a retrieval module that accesses external knowl-\nedge to enhance generation quality (Lewis et al. 2020; Guu\net al. 2020; Zhu et al. 2023; Jin et a\n\n a few words.\nQuestion: when was the last time anyone was on the moon\nThe answer is\nFigure 9: Prompt for retrieval-augmented generation of open-domain QA for the LLaMA-2-7B model.\n19\n",
      "contexts": [
        "h.\n4.4\nOpen-domain QA Evaluation on\nRetrieval-Augmented Language Models\nAnother aspect of the choice of granularity lies\nin what units should be used in the prompt for\nretrieval-augmented language models. For large\nlanguage models, retrieval-augmented generation\nis achieved by prepending retrieved units to user in-\nstruction and taking them as the input for language\nmodels. We aim to understand the implications of\nusing retrieved units of different granularity within\nthe same computational budge",
        "SPRING is adaptable to different retrievers and various\nnumbers of retrieved results. Consequently, there is no need\nto retrain SPRING with each update to the retrieval system,\nenhancing its practicality and efficiency.\nRelated Work\nRetrieval-Augmented Generation\nCompared to standard\ntext generation, retrieval-augmented generation (RAG) in-\ncorporates a retrieval module that accesses external knowl-\nedge to enhance generation quality (Lewis et al. 2020; Guu\net al. 2020; Zhu et al. 2023; Jin et a",
        " a few words.\nQuestion: when was the last time anyone was on the moon\nThe answer is\nFigure 9: Prompt for retrieval-augmented generation of open-domain QA for the LLaMA-2-7B model.\n19\n",
        "et al.\n2022).\nInterpretability and Transparency: Like many AI systems, RAG models are often treated as black\nboxes, with limited transparency in how retrieval influences generation. Improving the interpretability of\nthese models is crucial to fostering trust, especially in critical applications (Roller et al. 2020).\n6. Future Research Directions for Retrieval-Augmented Generation (RAG)\nRetrieval-augmented generation (RAG) represents a significant advancement in natural language\nprocessing and re",
        "AG has also seen steep\nadoption across various applications. However, there is a gap in a sufficient survey of this space tracking\nthe evolution and recent changes in this space. The current survey intends to fill this gap.\n1.2 Overview of Retrieval-Augmented Generation (RAG)\nRetrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address the\nlimitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism,\nwhich retrieves relevant d"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        14.444132198278409,
        12.147398296908788,
        11.571560518283922
      ],
      "sources": [
        "2312.06648.pdf",
        "2405.19670.pdf",
        "2312.06648.pdf",
        "2410.12837.pdf",
        "2410.12837.pdf"
      ],
      "execution_time": 4.355884313583374,
      "k1": 1.5,
      "b": 0.9,
      "category": "definition",
      "query_id": 1,
      "difficulty": "easy"
    },
    {
      "question": "What are the main components of a RAG system?",
      "answer": " Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n‚Ä¢ What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n‚Ä¢ A catalogue of failure points (FP) that occur in RAG systems.\n‚Ä¢ An experience repo\n\n the three case studies are shown in Table 2.\nWe present our findings for the research question: What are the\nkey considerations when engineering a RAG system? Based on our\ntakeaways we identified multiple potential research areas linked to\nRAG as follows:\n6.1\nChunking and Embeddings\nChunking documents sounds trivial. However, the quality of chunk-\ning affects the retrieval process in many ways and in particular\non the embeddings of the chunk then affects the similarity and\nmatching of chunks to\n\nreating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n‚Ä¢ What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi",
      "contexts": [
        " Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n‚Ä¢ What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n‚Ä¢ A catalogue of failure points (FP) that occur in RAG systems.\n‚Ä¢ An experience repo",
        " the three case studies are shown in Table 2.\nWe present our findings for the research question: What are the\nkey considerations when engineering a RAG system? Based on our\ntakeaways we identified multiple potential research areas linked to\nRAG as follows:\n6.1\nChunking and Embeddings\nChunking documents sounds trivial. However, the quality of chunk-\ning affects the retrieval process in many ways and in particular\non the embeddings of the chunk then affects the similarity and\nmatching of chunks to",
        "reating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n‚Ä¢ What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi",
        "ation was more pessimistic than a human rater for this domain.\nHowever, one threat to validity with this finding is that BioASQ is\na domain specific dataset and the reviewers were not experts i.e.\nthe large language model may know more than a non-expert.\n5\nFAILURE POINTS OF RAG SYSTEMS\nFrom the case studies we identified a set of failure points presented\nbelow. The following section addresses the research question What\nare the failure points that occur when engineering a RAG system?\nFP1 Missing ",
        "me about the Economic Cooperation Organization?\nE.6.13\nElectrical Engineering\nCan you provide details about the Eaton Cutler-Hammer C30CNE Lighting Contactor in an electrical\ncircuit system?\nCan you explain what the Keysight 33500B Series waveform generators are used for?\nWhat are the capabilities of the Texas Instruments LM741 Operational Amplifier in signal processing?\nCan you explain the use of the National Instruments CompactRIO controller in control systems and\ndata acquisition?\nCan you des"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        20.111663925166994,
        19.296476357031047,
        19.20044044918589
      ],
      "sources": [
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2403.18802.pdf"
      ],
      "execution_time": 4.488525867462158,
      "k1": 1.5,
      "b": 0.9,
      "category": "definition",
      "query_id": 2,
      "difficulty": "easy"
    },
    {
      "question": "How does RAG differ from traditional LLMs?",
      "answer": "of how recommendation\nsystems can be evaluated. Answer 1, while detailed, is more focused on traditional metrics and lacks the\ninclusion of engagement metrics which are crucial for understanding user interaction.\"\nDiversity: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"Answer 2 not only covers a wide variety of metrics but also includes nuanced explanations\nof how some metrics interrelate and differ from one another, like the inclusion of both RMSE and MSE, as\nwell as the details behind AUC.\n\nept of Universal\nJurisdiction function in international\nlaw and how has it been applied in\nvarious cases across the globe?‚Äù\n‚ÄúCan you share some information\nabout the Convention on the Elimi-\nnation of All Forms of Discrimina-\ntion Against Women (CEDAW)?‚Äù\nJurisprudence\n‚ÄúWhat is legal positivism and how\ndoes it differ from other legal the-\nories? Can you provide a detailed\nanalysis of its historical evolution,\nkey proponents, and significant con-\ntributions to the field of jurispru-\ndence?‚Äù\n‚ÄúCan y\n\ner will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Infor-\nmation Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of",
      "contexts": [
        "of how recommendation\nsystems can be evaluated. Answer 1, while detailed, is more focused on traditional metrics and lacks the\ninclusion of engagement metrics which are crucial for understanding user interaction.\"\nDiversity: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"Answer 2 not only covers a wide variety of metrics but also includes nuanced explanations\nof how some metrics interrelate and differ from one another, like the inclusion of both RMSE and MSE, as\nwell as the details behind AUC.",
        "ept of Universal\nJurisdiction function in international\nlaw and how has it been applied in\nvarious cases across the globe?‚Äù\n‚ÄúCan you share some information\nabout the Convention on the Elimi-\nnation of All Forms of Discrimina-\ntion Against Women (CEDAW)?‚Äù\nJurisprudence\n‚ÄúWhat is legal positivism and how\ndoes it differ from other legal the-\nories? Can you provide a detailed\nanalysis of its historical evolution,\nkey proponents, and significant con-\ntributions to the field of jurispru-\ndence?‚Äù\n‚ÄúCan y",
        "er will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Infor-\nmation Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of",
        "er will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Infor-\nmation Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of",
        "er will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Infor-\nmation Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.65564952893264,
        15.29875138819937,
        13.333452224030406
      ],
      "sources": [
        "2410.05779.pdf",
        "2403.18802.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf"
      ],
      "execution_time": 4.203841686248779,
      "k1": 1.5,
      "b": 0.9,
      "category": "definition",
      "query_id": 3,
      "difficulty": "medium"
    },
    {
      "question": "What is the purpose of the retrieval component in RAG?",
      "answer": "f each retrieval granularity fails to provide the correct\nanswer. The underlined text is the correct answer. The gray text is the context of propositions, but it is for illustration\npurpose only and not provided to the retrievers and downstream QA models.\nPassage Retrieval\nSentence Retrieval\nProposition Retrieval\nQ3: What is the function of the pericardial sac?\nTitle: Pericardium\n‚úì\nThe pericardium, also called pericardial sac\n... It separates the heart from interference of\nother structures, prot\n\nnd medicine questions:\n1. What does the EB-5 Immigrant Investor Program under US Immigration Law entail?\n2. Can you provide some information about the UN Convention on the Law of the Sea?\n3. What is the purpose of the I-765 Application for Employment Authorization in US Immigration\nLaw?\n4. What is the Orphan Drug Act?\n43\n5. What is the role of the protein Fibroblast Growth Factor 21 in regulating metabolism?\n6. What are the primary functions of the neuromodulatory system in the human brainstem?\n\n\nnltk/nltk/blob/develop/LICENSE.txt\n18https://github.com/Mimino666/langdetect/blob/master/LICENSE\n80\nU\nDatasheet\nU.1\nMotivation\nQ1 For what purpose was the dataset created? Was there a specific task in mind? Was there\na specific gap that needed to be filled? Please provide a description.\n‚Ä¢ The purpose of DCLM and the associated DCLM-POOL and DCLM-BASELINE\ndatasets are to enable the study of what makes a strong pretraining dataset for large\nlanguage models. These models are transformative to socie",
      "contexts": [
        "f each retrieval granularity fails to provide the correct\nanswer. The underlined text is the correct answer. The gray text is the context of propositions, but it is for illustration\npurpose only and not provided to the retrievers and downstream QA models.\nPassage Retrieval\nSentence Retrieval\nProposition Retrieval\nQ3: What is the function of the pericardial sac?\nTitle: Pericardium\n‚úì\nThe pericardium, also called pericardial sac\n... It separates the heart from interference of\nother structures, prot",
        "nd medicine questions:\n1. What does the EB-5 Immigrant Investor Program under US Immigration Law entail?\n2. Can you provide some information about the UN Convention on the Law of the Sea?\n3. What is the purpose of the I-765 Application for Employment Authorization in US Immigration\nLaw?\n4. What is the Orphan Drug Act?\n43\n5. What is the role of the protein Fibroblast Growth Factor 21 in regulating metabolism?\n6. What are the primary functions of the neuromodulatory system in the human brainstem?\n",
        "nltk/nltk/blob/develop/LICENSE.txt\n18https://github.com/Mimino666/langdetect/blob/master/LICENSE\n80\nU\nDatasheet\nU.1\nMotivation\nQ1 For what purpose was the dataset created? Was there a specific task in mind? Was there\na specific gap that needed to be filled? Please provide a description.\n‚Ä¢ The purpose of DCLM and the associated DCLM-POOL and DCLM-BASELINE\ndatasets are to enable the study of what makes a strong pretraining dataset for large\nlanguage models. These models are transformative to socie",
        " Despite this, the methods above usually\nignore a question, what if the retrieval goes wrong?\nSince the purpose of introducing a retrieval is to\nsecure that generative LMs can obtain relevant and\naccurate knowledge. If retrieved documents are\nirrelevant, the retrieval system can even exacerbate\nthe factual error that LMs make.\nAdvanced RAG\nMany advanced approaches\nhave been developed from the original RAG in\nrecent years (Zhang et al., 2024; Kim et al., 2024;\nWang et al., 2024; Liu et al., 2024)",
        " where 5 means the answer has perfect relevance. An example is\nprovided in Table 7.\n‚Ä¢ Groundedness: The metric defines weather the answer follows logically from the information contained in\nthe context or not and provides and integer score to determine how grounded the answer is. An example is\nprovided in Table 8.\n17\nQuestion\nRelevance\nScore ‚Üë\nAnswer\nWhat is the purpose of collecting soil compaction\ndata?\n5\nThe purpose of collecting soil compaction data\nis to determine the extent of soil compact"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.54086445047332,
        15.83915969610026,
        15.63287827351729
      ],
      "sources": [
        "2312.06648.pdf",
        "2403.18802.pdf",
        "2406.11794.pdf",
        "Corrective_RAG_2024.pdf",
        "2401.08406.pdf"
      ],
      "execution_time": 4.422984600067139,
      "k1": 1.5,
      "b": 0.9,
      "category": "definition",
      "query_id": 4,
      "difficulty": "easy"
    },
    {
      "question": "Explain the concept of grounding in RAG systems",
      "answer": "g misinformation or providing incorrect advice (Ji et al. 2022).\nRAG models directly address these concerns by grounding their generative process in external,\nup-to-date knowledge sources. This grounding improves the factual accuracy of the output and enhances\nthe relevance of responses by incorporating real-world data that is directly tied to the query. Additionally,\nRAG models are less likely to propagate biases present in static training data, as they can retrieve more\ndiverse and balanced in\n\nme about the Economic Cooperation Organization?\nE.6.13\nElectrical Engineering\nCan you provide details about the Eaton Cutler-Hammer C30CNE Lighting Contactor in an electrical\ncircuit system?\nCan you explain what the Keysight 33500B Series waveform generators are used for?\nWhat are the capabilities of the Texas Instruments LM741 Operational Amplifier in signal processing?\nCan you explain the use of the National Instruments CompactRIO controller in control systems and\ndata acquisition?\nCan you des\n\non - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept",
      "contexts": [
        "g misinformation or providing incorrect advice (Ji et al. 2022).\nRAG models directly address these concerns by grounding their generative process in external,\nup-to-date knowledge sources. This grounding improves the factual accuracy of the output and enhances\nthe relevance of responses by incorporating real-world data that is directly tied to the query. Additionally,\nRAG models are less likely to propagate biases present in static training data, as they can retrieve more\ndiverse and balanced in",
        "me about the Economic Cooperation Organization?\nE.6.13\nElectrical Engineering\nCan you provide details about the Eaton Cutler-Hammer C30CNE Lighting Contactor in an electrical\ncircuit system?\nCan you explain what the Keysight 33500B Series waveform generators are used for?\nWhat are the capabilities of the Texas Instruments LM741 Operational Amplifier in signal processing?\nCan you explain the use of the National Instruments CompactRIO controller in control systems and\ndata acquisition?\nCan you des",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        14.929596493051502,
        13.520296636602392,
        13.020969318650687
      ],
      "sources": [
        "2410.12837.pdf",
        "2403.18802.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf"
      ],
      "execution_time": 4.459773063659668,
      "k1": 1.5,
      "b": 0.9,
      "category": "definition",
      "query_id": 5,
      "difficulty": "medium"
    },
    {
      "question": "How does the retrieval mechanism work in RAG systems?",
      "answer": "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: ‚Ä¢ (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? ‚Ä¢ (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? ‚Ä¢ (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? ‚Ä¢\n(RQ4): What are the costs associated w\n\ne the RAG system and their use\ncases, which guide the generation of corpus-specific sensemaking queries.\n2.4\nRAG evaluation criteria\nOur evaluation relies on the LLM to evaluate how well the RAG system answers the generated ques-\ntions. Prior work has shown LLMs to be good evaluators of natural language generation, includ-\ning work where LLMs evaluations were competitive with human evaluations (Wang et al., 2023a;\nZheng et al., 2024). Some prior work proposes criteria for having LLMs quantify th\n\nreating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n‚Ä¢ What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi",
      "contexts": [
        "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: ‚Ä¢ (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? ‚Ä¢ (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? ‚Ä¢ (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? ‚Ä¢\n(RQ4): What are the costs associated w",
        "e the RAG system and their use\ncases, which guide the generation of corpus-specific sensemaking queries.\n2.4\nRAG evaluation criteria\nOur evaluation relies on the LLM to evaluate how well the RAG system answers the generated ques-\ntions. Prior work has shown LLMs to be good evaluators of natural language generation, includ-\ning work where LLMs evaluations were competitive with human evaluations (Wang et al., 2023a;\nZheng et al., 2024). Some prior work proposes criteria for having LLMs quantify th",
        "reating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n‚Ä¢ What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi",
        " chemist Paul L. Modrich.\nWho is Gertrude B. Elion and how has her groundbreaking work on the synthesis of medical drugs\nfrom regular compounds influenced pharmaceutical chemistry?\nWho is Glenn T. Seaborg and how did his discovery of plutonium and other transuranic elements\nchange the study of heavy elements in chemistry?\nWho is Richard J. Roberts and how did his discovery of introns in eukaryotic DNA and the mechanism\nof gene-splicing revolutionize molecular biology and chemistry?\nWho is Kary M",
        "ated Work.\nAppendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an\nextended related work of SSMs and other related models.\n16\nTable 9: (Ablations: Expressivity of Œî.)\nThe selection mechanism of Œî constructs it\nwith a projection of the input. Projecting it\neven to dim. 1 provides a large increase in\nperformance; increasing it further provides\nfurther improvements at the cost of a mod-\nest increase in parameters. State size fixed\nto ùëÅ= 16.\nSize of Œî proj.\nPa"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.35725102086509,
        14.132478150587154,
        13.853445111752318
      ],
      "sources": [
        "2410.05779.pdf",
        "2404.16130.pdf",
        "2401.05856.pdf",
        "2403.18802.pdf",
        "2312.00752.pdf"
      ],
      "execution_time": 4.47609543800354,
      "k1": 1.5,
      "b": 0.9,
      "category": "technical",
      "query_id": 6,
      "difficulty": "medium"
    },
    {
      "question": "What is the role of embeddings in RAG?",
      "answer": "ngs and what was his role at Netflix?\nWhat can you tell me about Elon Musk‚Äôs role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson‚Äôs tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association‚Äôs \"Ice Bucket Challenge\" marketing\ncampaign?\nWhat are the specifics of IBM‚Äôs \"Smarter Planet\" marketing c\n\nam?\nWhat is the NLTK library for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger‚Äôs leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon‚Äôs leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you\n\nnd medicine questions:\n1. What does the EB-5 Immigrant Investor Program under US Immigration Law entail?\n2. Can you provide some information about the UN Convention on the Law of the Sea?\n3. What is the purpose of the I-765 Application for Employment Authorization in US Immigration\nLaw?\n4. What is the Orphan Drug Act?\n43\n5. What is the role of the protein Fibroblast Growth Factor 21 in regulating metabolism?\n6. What are the primary functions of the neuromodulatory system in the human brainstem?\n",
      "contexts": [
        "ngs and what was his role at Netflix?\nWhat can you tell me about Elon Musk‚Äôs role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson‚Äôs tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association‚Äôs \"Ice Bucket Challenge\" marketing\ncampaign?\nWhat are the specifics of IBM‚Äôs \"Smarter Planet\" marketing c",
        "am?\nWhat is the NLTK library for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger‚Äôs leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon‚Äôs leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you",
        "nd medicine questions:\n1. What does the EB-5 Immigrant Investor Program under US Immigration Law entail?\n2. Can you provide some information about the UN Convention on the Law of the Sea?\n3. What is the purpose of the I-765 Application for Employment Authorization in US Immigration\nLaw?\n4. What is the Orphan Drug Act?\n43\n5. What is the role of the protein Fibroblast Growth Factor 21 in regulating metabolism?\n6. What are the primary functions of the neuromodulatory system in the human brainstem?\n",
        "stein Veblen?\nWho is Vilfredo Pareto?\nE.6.35\nSports\nWhat is the Stanley Cup?\nWhat is the Green Jacket in the Masters Golf Tournament?\nWhat is the Ryder Cup?\nWho is Serena Williams?\nWhat is the America‚Äôs Cup?\nE.6.36\nUS Foreign Policy\nWho is John Bolton?\nWho is Zalmay Khalilzad?\n64\nHow is the United States related to the East Asia Summit (EAS)?\nWho is Richard N. Haass?\nWhat was the United States‚Äô role in the Kyoto Protocol?\nE.6.37\nVirology\nWho is Thomas Wellems?\nWhat is the Avian influenza A(H7N9)",
        "uring the Samsung‚Äôs Galaxy Note 7 crisis?\nWhat role did the public relations firm, Bell Pottinger, play in the South African Gupta family\nscandal?\nWhat do you know about FleishmanHillard International Communications?\nWhat actions did PepsiCo‚Äôs public relations team take in response to the backlash for the Kendall\nJenner ad in 2017?\nE.6.34\nSociology\nWho is Peter L. Berger?\nWho is Anthony Giddens?\nWho is Norbert Elias?\nWho is Max Weber?\nWho is Thorstein Veblen?\nWho is Vilfredo Pareto?\nE.6.35\nSport"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.575828223747873,
        16.52889335350789,
        14.528370108927762
      ],
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "execution_time": 4.44244122505188,
      "k1": 1.5,
      "b": 0.9,
      "category": "technical",
      "query_id": 7,
      "difficulty": "medium"
    },
    {
      "question": "Explain how dense retrieval works in RAG",
      "answer": "ing user\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\nintegrate different retrieval methods to achieve optimal performance for diverse tasks.\n6.7 Integration with Emerging Technologies: Integrating RAG models with brain-computer interfaces\n(BCIs) could lead to novel applications in human-computer interaction and assistive technologies.\n\n\n In addition to sparse retrieval and dense re-\ntrieval, there are alternative methods for retrieving relevant\nobjects [77], [78]. Instead of calculating representations, some\nresearch works directly use the edit distance between natural\nlanguage texts [79] or abstract syntax trees (AST) of code snip-\npets [80], [81]. In knowledge graphs, entities are connected\nby relations, serving as a pre-built index for retrieval. Thus,\nRAG methods utilizing knowledge graphs can employ k-hop\nneighbor searches\n\n2021; Gao and Callan, 2022; Liu\nand Shao, 2022) have been proposed to improve the\neffectiveness of supervised dense retrieval models.\nOn the other hand, zero-shot dense retrieval still\nremains difÔ¨Åcult. Many recent works consider the\nalternative transfer learning setup, where the dense\nretrievers are trained on a high-resource dataset and\nthen evaluated on queries from new tasks. The MS-\nMARCO collection (Bajaj et al., 2016), a massive\njudged dataset with a large number of judged query-\ndocument",
      "contexts": [
        "ing user\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\nintegrate different retrieval methods to achieve optimal performance for diverse tasks.\n6.7 Integration with Emerging Technologies: Integrating RAG models with brain-computer interfaces\n(BCIs) could lead to novel applications in human-computer interaction and assistive technologies.\n",
        " In addition to sparse retrieval and dense re-\ntrieval, there are alternative methods for retrieving relevant\nobjects [77], [78]. Instead of calculating representations, some\nresearch works directly use the edit distance between natural\nlanguage texts [79] or abstract syntax trees (AST) of code snip-\npets [80], [81]. In knowledge graphs, entities are connected\nby relations, serving as a pre-built index for retrieval. Thus,\nRAG methods utilizing knowledge graphs can employ k-hop\nneighbor searches",
        "2021; Gao and Callan, 2022; Liu\nand Shao, 2022) have been proposed to improve the\neffectiveness of supervised dense retrieval models.\nOn the other hand, zero-shot dense retrieval still\nremains difÔ¨Åcult. Many recent works consider the\nalternative transfer learning setup, where the dense\nretrievers are trained on a high-resource dataset and\nthen evaluated on queries from new tasks. The MS-\nMARCO collection (Bajaj et al., 2016), a massive\njudged dataset with a large number of judged query-\ndocument",
        "ng Long-tail and Real-time Knowledge:\nWhile a key motivation of RAG is to harness real-time and\nlong-tail knowledge, few studies have explored the pipeline\nfor knowledge updating and expansion. Many existing works\nuse merely the generators‚Äô training data as retrieval sources,\nneglecting the dynamic and flexible information that retrieval\ncould offer. As a consequence, there is a growing research on\ndesigning RAG systems with continuously updated knowledge\nand flexible sources. We also expect RAG",
        "he\ntask of representation learning for documents with multiple Ô¨Åelds,\nand formalize the task. We then introduce a high-level overview\nof our framework, and further describe how we implement each\ncomponent of the proposed framework. We Ô¨Ånally explain how we\noptimize our neural ranking model.\n3.1\nMotivation and Problem Statement\nIn many retrieval scenarios, there exist various sources of textual in-\nformation (Ô¨Åelds) associated with each documentd. In web search in\nparticular, these sources of inf"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.254344348590365,
        15.014526680155608,
        14.323431540301769
      ],
      "sources": [
        "2410.12837.pdf",
        "2402.19473.pdf",
        "Atlas_2022.pdf",
        "2402.19473.pdf",
        "arxiv_1711.09174_2017.pdf"
      ],
      "execution_time": 4.382944107055664,
      "k1": 1.5,
      "b": 0.9,
      "category": "technical",
      "query_id": 8,
      "difficulty": "hard"
    },
    {
      "question": "What is the difference between sparse and dense retrieval?",
      "answer": "n should I use an em-dash, an en-dash, and a hyphen?\n4\nWhen to use -, ‚Äì and ‚Äî?\n6\nWhat is the difference between `-` and `--`\n5\n55\nDoes ‚Äúcost-benefit ratio‚Äù use a hyphen or an en-dash?\n0\nWhat kind of dash character should I use at the end of a famous saying to mark of the author?\n-1\ndash non-restrictive element in the middle of a sentence\n1\nem dash followed by a comma\n0\nwhat's the difference between a hyphen, a dash and a minus sign?\n0\nUsing comma to delimit the name of a group and its constituen\n\nasing the number of hypothetical\ndocuments does not yield significant benefits and substantially raises latency, indicating that using a\nsingle hypothetical document is sufficient.\n3.4.3\nHybrid Search with Different Weight on Sparse Retrieval\nTable 8 presents the impact of different Œ± values in hybrid search, where Œ± controls the weighting\nbetween sparse retrieval and dense retrieval components. The relevance score is calculated as follows:\nSh = Œ± ¬∑ Ss + Sd\n(1)\nwhere Ss, Sd are the normalized re\n\ne store,\nwhere each key corresponds to a value (keys and values\ncan be identical). Given a query, the objective is to search\nfor the top-k most similar keys using a similarity function,\nand obtain the paired values. Based on different similarity\nfunctions, existing retrieval methods can be categorized into\nsparse retrieval, dense retrieval, and others. In widely used\nsparse and dense retrieval, the entire process can be divided\ninto two distinct phases: (i) each object is first encoded into\na sp",
      "contexts": [
        "n should I use an em-dash, an en-dash, and a hyphen?\n4\nWhen to use -, ‚Äì and ‚Äî?\n6\nWhat is the difference between `-` and `--`\n5\n55\nDoes ‚Äúcost-benefit ratio‚Äù use a hyphen or an en-dash?\n0\nWhat kind of dash character should I use at the end of a famous saying to mark of the author?\n-1\ndash non-restrictive element in the middle of a sentence\n1\nem dash followed by a comma\n0\nwhat's the difference between a hyphen, a dash and a minus sign?\n0\nUsing comma to delimit the name of a group and its constituen",
        "asing the number of hypothetical\ndocuments does not yield significant benefits and substantially raises latency, indicating that using a\nsingle hypothetical document is sufficient.\n3.4.3\nHybrid Search with Different Weight on Sparse Retrieval\nTable 8 presents the impact of different Œ± values in hybrid search, where Œ± controls the weighting\nbetween sparse retrieval and dense retrieval components. The relevance score is calculated as follows:\nSh = Œ± ¬∑ Ss + Sd\n(1)\nwhere Ss, Sd are the normalized re",
        "e store,\nwhere each key corresponds to a value (keys and values\ncan be identical). Given a query, the objective is to search\nfor the top-k most similar keys using a similarity function,\nand obtain the paired values. Based on different similarity\nfunctions, existing retrieval methods can be categorized into\nsparse retrieval, dense retrieval, and others. In widely used\nsparse and dense retrieval, the entire process can be divided\ninto two distinct phases: (i) each object is first encoded into\na sp",
        "ls are better suited for particular use cases.\n1) Mix/hybrid Retrieval : Sparse and dense embedding\napproaches capture different relevance features and can ben-\nefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n6https://github.com/aurelio-labs/semantic-router\n7https://huggingface.co/spaces/mteb/leaderboard\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)",
        "ls are better suited for particular use cases.\n1) Mix/hybrid Retrieval : Sparse and dense embedding\napproaches capture different relevance features and can ben-\nefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n6https://github.com/aurelio-labs/semantic-router\n7https://huggingface.co/spaces/mteb/leaderboard\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.033607695247877,
        15.134536400322336,
        14.095218956443428
      ],
      "sources": [
        "2406.11794.pdf",
        "2407.01219.pdf",
        "2402.19473.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "execution_time": 4.496419191360474,
      "k1": 1.5,
      "b": 0.9,
      "category": "technical",
      "query_id": 9,
      "difficulty": "medium"
    },
    {
      "question": "How does chunking strategy affect RAG performance?",
      "answer": "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: ‚Ä¢ (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? ‚Ä¢ (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? ‚Ä¢ (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? ‚Ä¢\n(RQ4): What are the costs associated w\n\ns, even\nsentences. However, inappropriate retrieval granularity can compromise the semantic integrity\nand affect the relevance of retrieved information [224], thereby affecting the performance of\nLLMs. Fixed-size chunking, which typically breaks down the documents into chunks of a specified\nlength such as 100-word paragraphs, serves as the most crude and prevalent strategy of chunking,\nwhich is widely used in RAG systems [24, 109, 165]. Considering fixed-size chunking falls short in\ncapture stru\n\n, 20, 21]. Post-processing retrieved documents is another method to enhance generator\noutput, with techniques like hierarchical prompt summarization [22] and using abstractive and\nextractive compressors [23] to reduce context length and remove redundancy [24].\n2.2\nRetriever Enhancement Strategy\nDocument chunking and embedding methods significantly impact retrieval performance. Common\nchunking strategies divide documents into chunks, but determining optimal chunk length can be\nchallenging. Small ",
      "contexts": [
        "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: ‚Ä¢ (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? ‚Ä¢ (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? ‚Ä¢ (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? ‚Ä¢\n(RQ4): What are the costs associated w",
        "s, even\nsentences. However, inappropriate retrieval granularity can compromise the semantic integrity\nand affect the relevance of retrieved information [224], thereby affecting the performance of\nLLMs. Fixed-size chunking, which typically breaks down the documents into chunks of a specified\nlength such as 100-word paragraphs, serves as the most crude and prevalent strategy of chunking,\nwhich is widely used in RAG systems [24, 109, 165]. Considering fixed-size chunking falls short in\ncapture stru",
        ", 20, 21]. Post-processing retrieved documents is another method to enhance generator\noutput, with techniques like hierarchical prompt summarization [22] and using abstractive and\nextractive compressors [23] to reduce context length and remove redundancy [24].\n2.2\nRetriever Enhancement Strategy\nDocument chunking and embedding methods significantly impact retrieval performance. Common\nchunking strategies divide documents into chunks, but determining optimal chunk length can be\nchallenging. Small ",
        "by-side\ncomparisons.\nExpertise QA\nIn Expertise QA, we engage with in-house experts that have formal training and\nexperience in various domains (e.g., history, literature, psychology). They produced hard, sometimes\ncomplex questions (e.g., As films began using sound for dialogue, how did the changing use of visual\nmetaphor affect the ways audiences expected narratives to develop in movies? or How does Vygotsky‚Äôs\ntheory of internal speech apply across multilingual language acquisition?).\nThe same ",
        "\nof long-form factuality.14\nA.7\nHow does the prompt postamble affect model responses?\nSection 6 evaluated language models on LongFact-Objects with a fixed postamble that asks the model\nto provide as many specific details as possible.15 Here, we discuss how the fixed postamble affects\nthe model responses. We intuitively expect that the postamble should encourage models to provide\nmore facts in their responses and to provide more-specific facts rather than general facts.\nTo analyze the effect of t"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        22.406023614060878,
        20.56061998103465,
        14.680334050807547
      ],
      "sources": [
        "2410.05779.pdf",
        "2311.05232.pdf",
        "2407.01219.pdf",
        "2403.05530.pdf",
        "2403.18802.pdf"
      ],
      "execution_time": 4.144319772720337,
      "k1": 1.5,
      "b": 0.9,
      "category": "technical",
      "query_id": 10,
      "difficulty": "hard"
    },
    {
      "question": "What is the role of the retriever in RAG architecture?",
      "answer": "ngs and what was his role at Netflix?\nWhat can you tell me about Elon Musk‚Äôs role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson‚Äôs tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association‚Äôs \"Ice Bucket Challenge\" marketing\ncampaign?\nWhat are the specifics of IBM‚Äôs \"Smarter Planet\" marketing c\n\nam?\nWhat is the NLTK library for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger‚Äôs leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon‚Äôs leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you\n\nler size.\nThe Internet as a knowledge base\nMore related\nto our work, the search engine can assume the role\nof the retriever and use the Internet as the source of\nInput\nRetriever\nOutput\nDocuments\nInput\nWeb Search\nDocuments\nBlack-box LLM\nQuery\nInput\nDocuments\nQuery\nOutput\nOutput\nReward\nInput:\nWhat profession does Nicholas Ray and \nElia Kazan have in common?\nQuery: Nicholas Ray profession\nNicholas Ray American author and \ndirector, original name Raymond \nNicholas Kienzle, born August 7, \n1911, Gale",
      "contexts": [
        "ngs and what was his role at Netflix?\nWhat can you tell me about Elon Musk‚Äôs role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson‚Äôs tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association‚Äôs \"Ice Bucket Challenge\" marketing\ncampaign?\nWhat are the specifics of IBM‚Äôs \"Smarter Planet\" marketing c",
        "am?\nWhat is the NLTK library for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger‚Äôs leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon‚Äôs leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you",
        "ler size.\nThe Internet as a knowledge base\nMore related\nto our work, the search engine can assume the role\nof the retriever and use the Internet as the source of\nInput\nRetriever\nOutput\nDocuments\nInput\nWeb Search\nDocuments\nBlack-box LLM\nQuery\nInput\nDocuments\nQuery\nOutput\nOutput\nReward\nInput:\nWhat profession does Nicholas Ray and \nElia Kazan have in common?\nQuery: Nicholas Ray profession\nNicholas Ray American author and \ndirector, original name Raymond \nNicholas Kienzle, born August 7, \n1911, Gale",
        "nd medicine questions:\n1. What does the EB-5 Immigrant Investor Program under US Immigration Law entail?\n2. Can you provide some information about the UN Convention on the Law of the Sea?\n3. What is the purpose of the I-765 Application for Employment Authorization in US Immigration\nLaw?\n4. What is the Orphan Drug Act?\n43\n5. What is the role of the protein Fibroblast Growth Factor 21 in regulating metabolism?\n6. What are the primary functions of the neuromodulatory system in the human brainstem?\n",
        "l proceedings. Furthermore,\nstudies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks,\nincluding document summarization and, knowledge-grounded dialogues\n2.2 Retriever Mechanisms in RAG Systems\nThe retriever in RAG systems is essential for fetching relevant documents from an external corpus.\nEffective retrieval ensures that the model's output is grounded in accurate information. Several retrieval\nmechanisms are commonly used, ranging from traditional m"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.85472842326752,
        16.90872095338588,
        15.975423513693443
      ],
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2305.14283.pdf",
        "2403.18802.pdf",
        "2410.12837.pdf"
      ],
      "execution_time": 4.430037021636963,
      "k1": 1.5,
      "b": 0.9,
      "category": "technical",
      "query_id": 41,
      "difficulty": "easy"
    },
    {
      "question": "Explain the concept of semantic search in RAG",
      "answer": "ds.‚Äù In: WikiWorkshop@ICWSM. 2016.\n[9]\nJason V. Davis et al. ‚ÄúInformation-theoretic metric learning.‚Äù In: ICML. 2007.\n[10]\nScott Deerwester et al. ‚ÄúIndexing by latent semantic analysis.‚Äù In: Journal of the\nAmerican Society for Information Science 41.6 (1990).\n[11]\nManaal Faruqui et al. ‚ÄúRetroÔ¨Åtting Word Vectors to Semantic Lexicons.‚Äù In: CoRR\n(2014).\n[12]\nLev Finkelstein et al. ‚ÄúPlacing Search in Context: the Concept Revisited.‚Äù In:\nWWW. 2001.\n[13]\nEvgeniy Gabrilovich and Shaul Markovitch. ‚ÄúComp\n\nr a versatile RAG\nsystem designed to accommodate diverse scenarios.\nMetadata Router/ Filter. The first step involves extracting\nkeywords (entity) from the query, followed by filtering based\non the keywords and metadata within the chunks to narrow\ndown the search scope.\nSemantic Router is another method of routing involves\nleveraging the semantic information of the query. Specific\napprach see Semantic Router 6. Certainly, a hybrid routing\napproach can also be employed, combining both semantic and\n\nr a versatile RAG\nsystem designed to accommodate diverse scenarios.\nMetadata Router/ Filter. The first step involves extracting\nkeywords (entity) from the query, followed by filtering based\non the keywords and metadata within the chunks to narrow\ndown the search scope.\nSemantic Router is another method of routing involves\nleveraging the semantic information of the query. Specific\napprach see Semantic Router 6. Certainly, a hybrid routing\napproach can also be employed, combining both semantic and",
      "contexts": [
        "ds.‚Äù In: WikiWorkshop@ICWSM. 2016.\n[9]\nJason V. Davis et al. ‚ÄúInformation-theoretic metric learning.‚Äù In: ICML. 2007.\n[10]\nScott Deerwester et al. ‚ÄúIndexing by latent semantic analysis.‚Äù In: Journal of the\nAmerican Society for Information Science 41.6 (1990).\n[11]\nManaal Faruqui et al. ‚ÄúRetroÔ¨Åtting Word Vectors to Semantic Lexicons.‚Äù In: CoRR\n(2014).\n[12]\nLev Finkelstein et al. ‚ÄúPlacing Search in Context: the Concept Revisited.‚Äù In:\nWWW. 2001.\n[13]\nEvgeniy Gabrilovich and Shaul Markovitch. ‚ÄúComp",
        "r a versatile RAG\nsystem designed to accommodate diverse scenarios.\nMetadata Router/ Filter. The first step involves extracting\nkeywords (entity) from the query, followed by filtering based\non the keywords and metadata within the chunks to narrow\ndown the search scope.\nSemantic Router is another method of routing involves\nleveraging the semantic information of the query. Specific\napprach see Semantic Router 6. Certainly, a hybrid routing\napproach can also be employed, combining both semantic and",
        "r a versatile RAG\nsystem designed to accommodate diverse scenarios.\nMetadata Router/ Filter. The first step involves extracting\nkeywords (entity) from the query, followed by filtering based\non the keywords and metadata within the chunks to narrow\ndown the search scope.\nSemantic Router is another method of routing involves\nleveraging the semantic information of the query. Specific\napprach see Semantic Router 6. Certainly, a hybrid routing\napproach can also be employed, combining both semantic and",
        "r a versatile RAG\nsystem designed to accommodate diverse scenarios.\nMetadata Router/ Filter. The first step involves extracting\nkeywords (entity) from the query, followed by filtering based\non the keywords and metadata within the chunks to narrow\ndown the search scope.\nSemantic Router is another method of routing involves\nleveraging the semantic information of the query. Specific\napprach see Semantic Router 6. Certainly, a hybrid routing\napproach can also be employed, combining both semantic and",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        14.188546291158742,
        13.06347673937808,
        13.06347673937808
      ],
      "sources": [
        "arxiv_1705.07425_2017.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "2312.10997.pdf"
      ],
      "execution_time": 4.476807355880737,
      "k1": 1.5,
      "b": 0.9,
      "category": "technical",
      "query_id": 42,
      "difficulty": "medium"
    },
    {
      "question": "How do attention mechanisms work in RAG systems?",
      "answer": "hter).\n3.2\nHierarchical Attention\nThe global token techniques [6, 17, 78] and the inter-block attention [198] mentioned above are essentially introducing\nhierarchical features to self-attention to compensate with more global information from the higher-level attention\nwhile keeping the low computation cost from the low-level local attention at the same time. From this view, more work\nhas explored various hierarchical mechanisms that introduce a structured hierarchy into self-attention, leveragin\n\ntion (Sec. 3.4) and IO-Aware Attention (Sec. 3.5).\n3.1\nLocal Attention\nThe traditional attention mechanism is characterized by its global and full attention nature, wherein every token is\nexpected to attend to every other token, resulting in quadratic time and space complexities. Considering the significance\nof local context in certain applications [246], various approaches have been introduced to implement local attention\nmechanisms in recent years. These mechanisms restrict each token‚Äôs attent\n\nl proceedings. Furthermore,\nstudies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks,\nincluding document summarization and, knowledge-grounded dialogues\n2.2 Retriever Mechanisms in RAG Systems\nThe retriever in RAG systems is essential for fetching relevant documents from an external corpus.\nEffective retrieval ensures that the model's output is grounded in accurate information. Several retrieval\nmechanisms are commonly used, ranging from traditional m",
      "contexts": [
        "hter).\n3.2\nHierarchical Attention\nThe global token techniques [6, 17, 78] and the inter-block attention [198] mentioned above are essentially introducing\nhierarchical features to self-attention to compensate with more global information from the higher-level attention\nwhile keeping the low computation cost from the low-level local attention at the same time. From this view, more work\nhas explored various hierarchical mechanisms that introduce a structured hierarchy into self-attention, leveragin",
        "tion (Sec. 3.4) and IO-Aware Attention (Sec. 3.5).\n3.1\nLocal Attention\nThe traditional attention mechanism is characterized by its global and full attention nature, wherein every token is\nexpected to attend to every other token, resulting in quadratic time and space complexities. Considering the significance\nof local context in certain applications [246], various approaches have been introduced to implement local attention\nmechanisms in recent years. These mechanisms restrict each token‚Äôs attent",
        "l proceedings. Furthermore,\nstudies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks,\nincluding document summarization and, knowledge-grounded dialogues\n2.2 Retriever Mechanisms in RAG Systems\nThe retriever in RAG systems is essential for fetching relevant documents from an external corpus.\nEffective retrieval ensures that the model's output is grounded in accurate information. Several retrieval\nmechanisms are commonly used, ranging from traditional m",
        "t completely exclusive\nmodels. It is also shown that linear self-attention can be\nachieved in theory and practice by learning gated RNNs\nwith multiplicative interactions, bridging the gap between\nthese two architectures. Ali et al. [143] explore the learning\nmechanisms of Mamba models, in particular how depen-\ndencies are captured and their similarity to other established\nlayers, such as RNN, CNN, or attention mechanisms. An\nimportant relationship between the Mamba and the self-\nattention layer ",
        ".1), then explain how to incorporate\nthis mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting\na technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits\nthe memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or\neven MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        15.839357408014338,
        15.503899695726014,
        14.389095837023905
      ],
      "sources": [
        "2311.12351.pdf",
        "2311.12351.pdf",
        "2410.12837.pdf",
        "2404.09516.pdf",
        "2312.00752.pdf"
      ],
      "execution_time": 4.387248992919922,
      "k1": 1.5,
      "b": 0.9,
      "category": "technical",
      "query_id": 43,
      "difficulty": "hard"
    },
    {
      "question": "What is the purpose of document encoding in RAG?",
      "answer": "nd medicine questions:\n1. What does the EB-5 Immigrant Investor Program under US Immigration Law entail?\n2. Can you provide some information about the UN Convention on the Law of the Sea?\n3. What is the purpose of the I-765 Application for Employment Authorization in US Immigration\nLaw?\n4. What is the Orphan Drug Act?\n43\n5. What is the role of the protein Fibroblast Growth Factor 21 in regulating metabolism?\n6. What are the primary functions of the neuromodulatory system in the human brainstem?\n\n\nnltk/nltk/blob/develop/LICENSE.txt\n18https://github.com/Mimino666/langdetect/blob/master/LICENSE\n80\nU\nDatasheet\nU.1\nMotivation\nQ1 For what purpose was the dataset created? Was there a specific task in mind? Was there\na specific gap that needed to be filled? Please provide a description.\n‚Ä¢ The purpose of DCLM and the associated DCLM-POOL and DCLM-BASELINE\ndatasets are to enable the study of what makes a strong pretraining dataset for large\nlanguage models. These models are transformative to socie\n\n position embedding\ncharacterizes the position of a word in a sequence, while the ordinal\nembedding in SetRank characterizes the rank of a document w.r.t.\na ranking algorithm being applied to a document set. Therefore,\neach word inputted to Transformer has only one position embed-\nding while each document in SetRank may have multiple ordinal\nembeddings.\n4.3\nDocument Encoding with (Induced)\nMulti-head Self Attention Block\nThe key of SetRank is the encoding component which takes the\ndocument repre",
      "contexts": [
        "nd medicine questions:\n1. What does the EB-5 Immigrant Investor Program under US Immigration Law entail?\n2. Can you provide some information about the UN Convention on the Law of the Sea?\n3. What is the purpose of the I-765 Application for Employment Authorization in US Immigration\nLaw?\n4. What is the Orphan Drug Act?\n43\n5. What is the role of the protein Fibroblast Growth Factor 21 in regulating metabolism?\n6. What are the primary functions of the neuromodulatory system in the human brainstem?\n",
        "nltk/nltk/blob/develop/LICENSE.txt\n18https://github.com/Mimino666/langdetect/blob/master/LICENSE\n80\nU\nDatasheet\nU.1\nMotivation\nQ1 For what purpose was the dataset created? Was there a specific task in mind? Was there\na specific gap that needed to be filled? Please provide a description.\n‚Ä¢ The purpose of DCLM and the associated DCLM-POOL and DCLM-BASELINE\ndatasets are to enable the study of what makes a strong pretraining dataset for large\nlanguage models. These models are transformative to socie",
        " position embedding\ncharacterizes the position of a word in a sequence, while the ordinal\nembedding in SetRank characterizes the rank of a document w.r.t.\na ranking algorithm being applied to a document set. Therefore,\neach word inputted to Transformer has only one position embed-\nding while each document in SetRank may have multiple ordinal\nembeddings.\n4.3\nDocument Encoding with (Induced)\nMulti-head Self Attention Block\nThe key of SetRank is the encoding component which takes the\ndocument repre",
        " where 5 means the answer has perfect relevance. An example is\nprovided in Table 7.\n‚Ä¢ Groundedness: The metric defines weather the answer follows logically from the information contained in\nthe context or not and provides and integer score to determine how grounded the answer is. An example is\nprovided in Table 8.\n17\nQuestion\nRelevance\nScore ‚Üë\nAnswer\nWhat is the purpose of collecting soil compaction\ndata?\n5\nThe purpose of collecting soil compaction data\nis to determine the extent of soil compact",
        "hat can be deÔ¨Åned as a document aware query\nFig. 3. Multi-Resolution n-Gram Attention\nFig. 4. Document Aware Query Attention\nencoding of i-th position of a query. The Ô¨Ånal output represen-\ntations of encoder fe(¬∑, ¬∑) is a document aware query encoding\nattention vector qe that is evaluated as:\nqe = [qe1, qe2, ¬∑ ¬∑ ¬∑ , qehq] ‚ààRhq\nIn other words, if the document includes more positions of\nmradj that are very much alike to the position of mraqi\nin the query, the document aware query attention compone"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        15.390400591026486,
        15.252730042979952,
        14.982786734301897
      ],
      "sources": [
        "2403.18802.pdf",
        "2406.11794.pdf",
        "arxiv_1912.05891_2019.pdf",
        "2401.08406.pdf",
        "arxiv_1911.00964_2019.pdf"
      ],
      "execution_time": 4.44869327545166,
      "k1": 1.5,
      "b": 0.9,
      "category": "technical",
      "query_id": 44,
      "difficulty": "medium"
    },
    {
      "question": "How does RAG handle out-of-domain queries?",
      "answer": "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: ‚Ä¢ (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? ‚Ä¢ (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? ‚Ä¢ (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? ‚Ä¢\n(RQ4): What are the costs associated w\n\nchanisms\nwith generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs.\nThe study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated\nto handle knowledge-intensive tasks. A detailed review of the significant technological advancements in\nRAG is provided, including key innovations in retrieval-augmented language models and applications\nacross various domains such as question-answering, summarization, and knowledg\n\n against all of the\ndocuments. Cognitive Reviewer is currently used by PhD students\nfrom Deakin University to support their literature reviews. The\nCognitive Reviewer does the Index process at run time and relies\n5https://figshare.com/s/fbf7805b5f20d7f7e356\non a robust data processing pipeline to handle uploaded documents\ni.e. no quality control possible at development time. This system\nalso uses a ranking algorithm to sort the uploaded documents.\n4.2\nAI Tutor\nThe AI Tutor is a RAG system where ",
      "contexts": [
        "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: ‚Ä¢ (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? ‚Ä¢ (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? ‚Ä¢ (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? ‚Ä¢\n(RQ4): What are the costs associated w",
        "chanisms\nwith generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs.\nThe study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated\nto handle knowledge-intensive tasks. A detailed review of the significant technological advancements in\nRAG is provided, including key innovations in retrieval-augmented language models and applications\nacross various domains such as question-answering, summarization, and knowledg",
        " against all of the\ndocuments. Cognitive Reviewer is currently used by PhD students\nfrom Deakin University to support their literature reviews. The\nCognitive Reviewer does the Index process at run time and relies\n5https://figshare.com/s/fbf7805b5f20d7f7e356\non a robust data processing pipeline to handle uploaded documents\ni.e. no quality control possible at development time. This system\nalso uses a ranking algorithm to sort the uploaded documents.\n4.2\nAI Tutor\nThe AI Tutor is a RAG system where ",
        " able to gather Ô¨Çexible size of n-gram\nrepresentations that are needed for better understanding\nof the text that depends on context, syntax, and semantics.\n‚Ä¢ In order to handle the extraction of Ô¨Çexible size n-\ngram representations, one can employ the kernels with\nvarious window sizes, but another issue is Ô¨Åred up with\nsuch settings: What would be the right architecture of\nusing different kernel sizes? In other terms, how much\nexpanding does the network require for different kernel\nsizes to prod",
        " significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.\nQuery\nPassage\nAnswer\nContext\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        13.011896854959103,
        12.435733924890055,
        12.05614334086374
      ],
      "sources": [
        "2410.05779.pdf",
        "2410.12837.pdf",
        "2401.05856.pdf",
        "arxiv_1911.00964_2019.pdf",
        "2311.09476.pdf"
      ],
      "execution_time": 4.130276203155518,
      "k1": 1.5,
      "b": 0.9,
      "category": "technical",
      "query_id": 45,
      "difficulty": "hard"
    },
    {
      "question": "What is Self-RAG and how does it differ from standard RAG?",
      "answer": "ept of Universal\nJurisdiction function in international\nlaw and how has it been applied in\nvarious cases across the globe?‚Äù\n‚ÄúCan you share some information\nabout the Convention on the Elimi-\nnation of All Forms of Discrimina-\ntion Against Women (CEDAW)?‚Äù\nJurisprudence\n‚ÄúWhat is legal positivism and how\ndoes it differ from other legal the-\nories? Can you provide a detailed\nanalysis of its historical evolution,\nkey proponents, and significant con-\ntributions to the field of jurispru-\ndence?‚Äù\n‚ÄúCan y\n\nre vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?\nWhat is the primary focus of the Famous Marshmallow Experiment conducted by Walter Mischel?\nWho is Aaron T. Beck known for his work in cognitive therapy?\nWhat is the Thematic Apperception Test (TAT) and how is it commonly utilized in uncovering a\nperson‚Äôs und\n\ncerns or special precautions about DRUG?\n‚Ä¢ What is the action of DRUG and how does it work?\n‚Ä¢ Who should get DRUG and why is it prescribed?\n‚Ä¢ What to do in case of a severe reaction to DRUG?\n3. Question Type for other medical entities (e.g. Procedure, Exam, Treatment): Information.\n‚Ä¢ What is Coronary Artery Bypass Surgery?\n‚Ä¢ What are Liver Function Tests?\n4.3\nMedical Resources\nWe used 12 trusted websites to construct a collection of question-answer pairs. For each website, we extracted the free\n",
      "contexts": [
        "ept of Universal\nJurisdiction function in international\nlaw and how has it been applied in\nvarious cases across the globe?‚Äù\n‚ÄúCan you share some information\nabout the Convention on the Elimi-\nnation of All Forms of Discrimina-\ntion Against Women (CEDAW)?‚Äù\nJurisprudence\n‚ÄúWhat is legal positivism and how\ndoes it differ from other legal the-\nories? Can you provide a detailed\nanalysis of its historical evolution,\nkey proponents, and significant con-\ntributions to the field of jurispru-\ndence?‚Äù\n‚ÄúCan y",
        "re vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?\nWhat is the primary focus of the Famous Marshmallow Experiment conducted by Walter Mischel?\nWho is Aaron T. Beck known for his work in cognitive therapy?\nWhat is the Thematic Apperception Test (TAT) and how is it commonly utilized in uncovering a\nperson‚Äôs und",
        "cerns or special precautions about DRUG?\n‚Ä¢ What is the action of DRUG and how does it work?\n‚Ä¢ Who should get DRUG and why is it prescribed?\n‚Ä¢ What to do in case of a severe reaction to DRUG?\n3. Question Type for other medical entities (e.g. Procedure, Exam, Treatment): Information.\n‚Ä¢ What is Coronary Artery Bypass Surgery?\n‚Ä¢ What are Liver Function Tests?\n4.3\nMedical Resources\nWe used 12 trusted websites to construct a collection of question-answer pairs. For each website, we extracted the free\n",
        "s and supplements, Important warning, Special instructions, Brand names, How does it work, How\neffective is it, Indication, Contraindication, Learn more, Side effects, Emergency or overdose, Severe reaction,\nForget a dose, Dietary, Why get vaccinated, Storage and disposal, Usage, Dose.\nExamples:\n‚Ä¢ Are there interactions between DRUG and herbs and supplements?\n‚Ä¢ What important warning or information should I know about DRUG?\n‚Ä¢ Are there safety concerns or special precautions about DRUG?\n‚Ä¢ What is",
        "een that CRAG still showed\ncompetitive performance when the underlying\nLLMs was changed from SelfRAG-LLaMA2-7b\nto LLaMA2-hf-7b, while the performance of Self-\nRAG dropped significantly, even underperforming\nthe standard RAG on several benchmarks. The\nreason for these results is that Self-RAG needs to be\ninstruction-tuned using human or LLM annotated\ndata to learn to output special critic tokens as\nneeded, while this ability is not learned in common\nLLMs. CRAG does not have any requirements\nfor t"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        19.22699199735289,
        18.974990277908603,
        16.951896534041012
      ],
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "arxiv_1901.08079_2019.pdf",
        "arxiv_1901.08079_2019.pdf",
        "Corrective_RAG_2024.pdf"
      ],
      "execution_time": 4.434725999832153,
      "k1": 1.5,
      "b": 0.9,
      "category": "approaches",
      "query_id": 11,
      "difficulty": "hard"
    },
    {
      "question": "Explain the concept of Corrective RAG (CRAG)",
      "answer": "itably\nexhibit hallucinations since the accuracy of\ngenerated texts cannot be secured solely by\nthe parametric knowledge they encapsulate. Al-\nthough retrieval-augmented generation (RAG)\nis a practicable complement to LLMs, it relies\nheavily on the relevance of retrieved docu-\nments, raising concerns about how the model\nbehaves if retrieval goes wrong. To this end, we\npropose the Corrective Retrieval Augmented\nGeneration (CRAG) to improve the robustness\nof generation.\nSpecifically, a lightweight\n\nen equally referred to and involved in RAG.\nOn account of the above issues, this paper\nparticularly\nstudies\nthe\nscenarios\nwhere\nthe retriever returns inaccurate results.\nA\nmethod named Corrective Retrieval-Augmented\nGeneration (CRAG) is proposed to self-correct\nthe results of retriever and improve the utilization\nof documents for augmenting generation.\nA\nlightweight retrieval evaluator is designed to\nassess the overall quality of retrieved documents\nfor a query. This serves as a crucial componen\n\n results that the system judges to\nbe of low quality.\nThis can be analogous to\nRAG‚Äôs augmentation to standalone parameterized\nlanguage models and we further augment RAG\nwith corrective strategies.\n5.2\nBaselines\nWe primarily compared CRAG with both ap-\nproaches with and without retrieval, where the\nlatter can be further split into standard RAG and\nlatest advanced RAG, including:\nBaselines without retrieval. We evaluated some\npublic LLMs, LLaMA2-7B,13B (Touvron et al.,\n2023b), instruction-tuned mo",
      "contexts": [
        "itably\nexhibit hallucinations since the accuracy of\ngenerated texts cannot be secured solely by\nthe parametric knowledge they encapsulate. Al-\nthough retrieval-augmented generation (RAG)\nis a practicable complement to LLMs, it relies\nheavily on the relevance of retrieved docu-\nments, raising concerns about how the model\nbehaves if retrieval goes wrong. To this end, we\npropose the Corrective Retrieval Augmented\nGeneration (CRAG) to improve the robustness\nof generation.\nSpecifically, a lightweight",
        "en equally referred to and involved in RAG.\nOn account of the above issues, this paper\nparticularly\nstudies\nthe\nscenarios\nwhere\nthe retriever returns inaccurate results.\nA\nmethod named Corrective Retrieval-Augmented\nGeneration (CRAG) is proposed to self-correct\nthe results of retriever and improve the utilization\nof documents for augmenting generation.\nA\nlightweight retrieval evaluator is designed to\nassess the overall quality of retrieved documents\nfor a query. This serves as a crucial componen",
        " results that the system judges to\nbe of low quality.\nThis can be analogous to\nRAG‚Äôs augmentation to standalone parameterized\nlanguage models and we further augment RAG\nwith corrective strategies.\n5.2\nBaselines\nWe primarily compared CRAG with both ap-\nproaches with and without retrieval, where the\nlatter can be further split into standard RAG and\nlatest advanced RAG, including:\nBaselines without retrieval. We evaluated some\npublic LLMs, LLaMA2-7B,13B (Touvron et al.,\n2023b), instruction-tuned mo",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        18.179466598612926,
        17.967428549519347,
        12.870627153230512
      ],
      "sources": [
        "Corrective_RAG_2024.pdf",
        "Corrective_RAG_2024.pdf",
        "Corrective_RAG_2024.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "execution_time": 4.503340005874634,
      "k1": 1.5,
      "b": 0.9,
      "category": "approaches",
      "query_id": 12,
      "difficulty": "hard"
    },
    {
      "question": "How does Hybrid RAG combine different retrieval methods?",
      "answer": "ing user\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\nintegrate different retrieval methods to achieve optimal performance for diverse tasks.\n6.7 Integration with Emerging Technologies: Integrating RAG models with brain-computer interfaces\n(BCIs) could lead to novel applications in human-computer interaction and assistive technologies.\n\n\nasing the number of hypothetical\ndocuments does not yield significant benefits and substantially raises latency, indicating that using a\nsingle hypothetical document is sufficient.\n3.4.3\nHybrid Search with Different Weight on Sparse Retrieval\nTable 8 presents the impact of different Œ± values in hybrid search, where Œ± controls the weighting\nbetween sparse retrieval and dense retrieval components. The relevance score is calculated as follows:\nSh = Œ± ¬∑ Ss + Sd\n(1)\nwhere Ss, Sd are the normalized re\n\ney produce plausible but incorrect or non-existent information (Ji et al. 2022). This\nlimitation has prompted the exploration of hybrid models that combine retrieval mechanisms with\ngenerative capabilities to ensure both fluency and factual correctness in outputs. There has been a\nsignificant rise in several research papers in this field and several new methods across the RAG\ncomponents have been proposed. Apart from new algorithms and methods, RAG has also seen steep\nadoption across various app",
      "contexts": [
        "ing user\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\nintegrate different retrieval methods to achieve optimal performance for diverse tasks.\n6.7 Integration with Emerging Technologies: Integrating RAG models with brain-computer interfaces\n(BCIs) could lead to novel applications in human-computer interaction and assistive technologies.\n",
        "asing the number of hypothetical\ndocuments does not yield significant benefits and substantially raises latency, indicating that using a\nsingle hypothetical document is sufficient.\n3.4.3\nHybrid Search with Different Weight on Sparse Retrieval\nTable 8 presents the impact of different Œ± values in hybrid search, where Œ± controls the weighting\nbetween sparse retrieval and dense retrieval components. The relevance score is calculated as follows:\nSh = Œ± ¬∑ Ss + Sd\n(1)\nwhere Ss, Sd are the normalized re",
        "ey produce plausible but incorrect or non-existent information (Ji et al. 2022). This\nlimitation has prompted the exploration of hybrid models that combine retrieval mechanisms with\ngenerative capabilities to ensure both fluency and factual correctness in outputs. There has been a\nsignificant rise in several research papers in this field and several new methods across the RAG\ncomponents have been proposed. Apart from new algorithms and methods, RAG has also seen steep\nadoption across various app",
        " quality. These results underscore the importance\nof developing specialized strategies to integrate retrieval with\nlanguage generation models, highlighting the need for further\nresearch and exploration into the robustness of RAG.\nC. Hybrid Approaches\nCombining RAG with fine-tuning is emerging as a leading\nstrategy. Determining the optimal integration of RAG and\nfine-tuning whether sequential, alternating, or through end-to-\nend joint training‚Äîand how to harness both parameterized\n15\nTABLE IV\nSUM",
        " quality. These results underscore the importance\nof developing specialized strategies to integrate retrieval with\nlanguage generation models, highlighting the need for further\nresearch and exploration into the robustness of RAG.\nC. Hybrid Approaches\nCombining RAG with fine-tuning is emerging as a leading\nstrategy. Determining the optimal integration of RAG and\nfine-tuning whether sequential, alternating, or through end-to-\nend joint training‚Äîand how to harness both parameterized\n15\nTABLE IV\nSUM"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        27.003110182177544,
        18.38018159647197,
        17.6104881097083
      ],
      "sources": [
        "2410.12837.pdf",
        "2407.01219.pdf",
        "2410.12837.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "execution_time": 4.19182562828064,
      "k1": 1.5,
      "b": 0.9,
      "category": "approaches",
      "query_id": 13,
      "difficulty": "medium"
    },
    {
      "question": "What is the advantage of query rewriting in RAG?",
      "answer": "ewriting\nTo build powerful conversational query rewriting model, we take GPT-3.5-Turbo as the rewriter\ngiven that Galimzhanova et al. (2023) demonstrated the state-of-the-art query rewriting results using\nGPT-3.5-Turbo. Similar to Galimzhanova et al. (2023), we not only provide GPT-3.5-Turbo with\nthe rewriting task instruction, but also give it few-shot rewriting examples to enhance the quality of\nrewriting results. More details can be found in Appendix C.1.\n4.3\nComparisons\nIn Table 1, we compar\n\n cope with conversational\ninputs.\nAn alternative solution is conversational query rewriting method which uses a query rewriter to rewrite\nthe current question based on the conversational history. The rewritten query is then directly used as\nthe input to a single-turn query retriever for retrieving relevant context. In addition to the embedding\nand search cost, the query rewriting model introduces a large amount of extra computational expense\nto generate the rewritten query.\n4.1\nFine-tuning Retri\n\nthe methods for effectively fine-tuning\nLLMs (see Figure 1).\nWhat adds complexity and challenge is the variability in implementing each processing step. For\nexample, in retrieving relevant documents for an input query, various methods can be employed.\nOne approach involves rewriting the query first and using the rewritten queries for retrieval [9].\nAlternatively, pseudo-responses to the query can be generated first, and the similarity between\nthese pseudo-responses and the backend documents can ",
      "contexts": [
        "ewriting\nTo build powerful conversational query rewriting model, we take GPT-3.5-Turbo as the rewriter\ngiven that Galimzhanova et al. (2023) demonstrated the state-of-the-art query rewriting results using\nGPT-3.5-Turbo. Similar to Galimzhanova et al. (2023), we not only provide GPT-3.5-Turbo with\nthe rewriting task instruction, but also give it few-shot rewriting examples to enhance the quality of\nrewriting results. More details can be found in Appendix C.1.\n4.3\nComparisons\nIn Table 1, we compar",
        " cope with conversational\ninputs.\nAn alternative solution is conversational query rewriting method which uses a query rewriter to rewrite\nthe current question based on the conversational history. The rewritten query is then directly used as\nthe input to a single-turn query retriever for retrieving relevant context. In addition to the embedding\nand search cost, the query rewriting model introduces a large amount of extra computational expense\nto generate the rewritten query.\n4.1\nFine-tuning Retri",
        "the methods for effectively fine-tuning\nLLMs (see Figure 1).\nWhat adds complexity and challenge is the variability in implementing each processing step. For\nexample, in retrieving relevant documents for an input query, various methods can be employed.\nOne approach involves rewriting the query first and using the rewritten queries for retrieval [9].\nAlternatively, pseudo-responses to the query can be generated first, and the similarity between\nthese pseudo-responses and the backend documents can ",
        " questions (e.g., with pronouns referring to entities mentioned in the previous\nconversation) may have insufficient information for retrieval, while feeding them along with all of\nthe dialogue history can be redundant, thus leading to sub-optimal results.\nConversational Query Rewriting\nMost of the previous solutions are query rewriting methods.\nThe latest turn of question is rewritten to be a standalone query without additional information from\nprevious dialogue history (Vakulenko et al., 2021a;",
        "s, (ii) query ex-\npansion - rewriting the query based on relevant\nfeedback. They proposed a method for query dis-\nambiguation where a new query is formed by per-\nforming few-shot prompting. text-davinci-003 was\nemployed to retrieve a pseudo-document which\nis concatenated with the original query to form a\nnew query for the retrieval task. In this approach,\ninference is slowed down by token-by-token auto-\nregressive decoding strategy.\nIn another study,\nAlaofi et al. (2023) explored the utility of "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        15.63522406932545,
        15.275128855641022,
        15.151591123838772
      ],
      "sources": [
        "Graph_RAG_2024.pdf",
        "Graph_RAG_2024.pdf",
        "2407.01219.pdf",
        "Graph_RAG_2024.pdf",
        "2402.14558.pdf"
      ],
      "execution_time": 4.477565765380859,
      "k1": 1.5,
      "b": 0.9,
      "category": "approaches",
      "query_id": 14,
      "difficulty": "medium"
    },
    {
      "question": "Explain the re-ranking stage in Advanced RAG",
      "answer": ". Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\nadvanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n‚Ä¢ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof ‚ÄúRetrieval‚Äù, ‚ÄúGeneration‚Äù and ‚ÄúAugmentation‚Äù, and\ndelve into their synergies, elucidating how these com-\nponents intric\n\n. Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\nadvanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n‚Ä¢ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof ‚ÄúRetrieval‚Äù, ‚ÄúGeneration‚Äù and ‚ÄúAugmentation‚Äù, and\ndelve into their synergies, elucidating how these com-\nponents intric\n\n. Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\nadvanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n‚Ä¢ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof ‚ÄúRetrieval‚Äù, ‚ÄúGeneration‚Äù and ‚ÄúAugmentation‚Äù, and\ndelve into their synergies, elucidating how these com-\nponents intric",
      "contexts": [
        ". Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\nadvanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n‚Ä¢ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof ‚ÄúRetrieval‚Äù, ‚ÄúGeneration‚Äù and ‚ÄúAugmentation‚Äù, and\ndelve into their synergies, elucidating how these com-\nponents intric",
        ". Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\nadvanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n‚Ä¢ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof ‚ÄúRetrieval‚Äù, ‚ÄúGeneration‚Äù and ‚ÄúAugmentation‚Äù, and\ndelve into their synergies, elucidating how these com-\nponents intric",
        ". Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\nadvanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n‚Ä¢ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof ‚ÄúRetrieval‚Äù, ‚ÄúGeneration‚Äù and ‚ÄúAugmentation‚Äù, and\ndelve into their synergies, elucidating how these com-\nponents intric",
        "hree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to",
        "hree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        15.629626514037044,
        15.629626514037044,
        15.629626514037044
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "execution_time": 4.453416109085083,
      "k1": 1.5,
      "b": 0.9,
      "category": "approaches",
      "query_id": 15,
      "difficulty": "medium"
    },
    {
      "question": "What is iterative RAG and when is it beneficial?",
      "answer": "to retrieve based on the query\ncomplexity by a classifier, which is a smaller LM.\nIterative RAG: Iterative RAG progressively refines results\nby repeatedly cycling through retrieval and generation phases,\nrather than a single round.\nRepoCoder [186] uses an iterative retrieval-generation ap-\nproach for code completion, refining queries with previously\n9\nTABLE I: Taxonomy of RAG applications across various modalities.\nRAG for Text\nQuestion Answering\nHuman-Machine Conversation\nNeural Machine Transla\n\nshine a light on what issues\nengineers will face and what software engineering research is nec-\nessary to realise solutions with the current state-of-the-art RAG\nsystems.\nEmerging work has looked at benchmarking RAG systems [3]\nbut not at the failures occurring during implementation. Software\nengineering research has investigated the use of RAG systems for\ncode-related tasks [15]. However, the application of RAG systems\nis broader than software engineering tasks. This paper comple-\nments existin\n\nLMs with\ndomain-specific knowledge. PKG employs a trainable background knowledge module, aligning it\nwith task knowledge and generating relevant contextual information.\nIterative Retrieval. When confronted with intricate challenges like multi-step reasoning [344]\nand long-form question answering [83, 284], traditional one-time retrieval may fall short. Address-\ning these demanding information needs, recent studies have proposed iterative retrieval, which\nallows for continuously gathering knowled",
      "contexts": [
        "to retrieve based on the query\ncomplexity by a classifier, which is a smaller LM.\nIterative RAG: Iterative RAG progressively refines results\nby repeatedly cycling through retrieval and generation phases,\nrather than a single round.\nRepoCoder [186] uses an iterative retrieval-generation ap-\nproach for code completion, refining queries with previously\n9\nTABLE I: Taxonomy of RAG applications across various modalities.\nRAG for Text\nQuestion Answering\nHuman-Machine Conversation\nNeural Machine Transla",
        "shine a light on what issues\nengineers will face and what software engineering research is nec-\nessary to realise solutions with the current state-of-the-art RAG\nsystems.\nEmerging work has looked at benchmarking RAG systems [3]\nbut not at the failures occurring during implementation. Software\nengineering research has investigated the use of RAG systems for\ncode-related tasks [15]. However, the application of RAG systems\nis broader than software engineering tasks. This paper comple-\nments existin",
        "LMs with\ndomain-specific knowledge. PKG employs a trainable background knowledge module, aligning it\nwith task knowledge and generating relevant contextual information.\nIterative Retrieval. When confronted with intricate challenges like multi-step reasoning [344]\nand long-form question answering [83, 284], traditional one-time retrieval may fall short. Address-\ning these demanding information needs, recent studies have proposed iterative retrieval, which\nallows for continuously gathering knowled",
        "ction‚Äù task.\nQuery-based RAG is the mainstream paradigm for code\ncompletion. Drain et al. [240] retrieved template functions\nfor function completion. ReACC [91] uses both sparse and\ndense retrieval. RepoCoder [186] performs iterative RAG\nby augmenting the retrieval input with previously generated\ncode. De-Hallucinator [241] retrieves API references using\nfirst-time generated contents, then conducts query-based RAG\nfor improved code completion. REPOFUSE [242] includes\nrationale context and retrie",
        "reating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n‚Ä¢ What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.14879295290345,
        13.929379357083722,
        13.213126330726347
      ],
      "sources": [
        "2402.19473.pdf",
        "2401.05856.pdf",
        "2311.05232.pdf",
        "2402.19473.pdf",
        "2401.05856.pdf"
      ],
      "execution_time": 4.4623472690582275,
      "k1": 1.5,
      "b": 0.9,
      "category": "approaches",
      "query_id": 46,
      "difficulty": "hard"
    },
    {
      "question": "Explain the concept of fusion in hybrid RAG systems",
      "answer": "rk to process retrieved video-sentence\npairs with an attention mechanism, generating hidden states to\nproduce captions. CARE [117] introduces a concept detector\nto produce concept probabilities, and incorporates concept\nrepresentations into a hybrid attention mechanism.\nEgoInstructor [49] uses gated-cross attention to merge text\nand video features, improving the relevance and coherence\nof captions for egocentric videos. Latent representation-based\nRAG, adaptable across modalities and tasks, blen\n\nchitectural components, recent research in this area, current challenges and limitations of RAG, and\nfuture research direction.\n2: Core Components and Architectural Overview of RAG Systems\n2.1 Overview of RAG Models\nRetrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural\nlanguage generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base.\nTraditional large language models (LLMs) such as GPT-3 and BERT, which are pre-train\n\nings.\nùë°\nMLP\nCross\nAttention\nLinear\nFusion Block\nTransformer  Encoder\nCLIP\nText\nCLIP\nText\nLinear\nùë•ùë°\n1\nùë•ùë°\n2\nùë•ùë°\n3\nùë•ùë°\nùëÅ\nùëö1\nùëö2\nùëßùë°ùëò\nùë†\nùëßùë°ùëò\nùëô\nùëö3\nùëöùëÖ\n‚äï\n‡∑úùë•0\n1\n‡∑úùë•0\n2\n‡∑úùë•0\n3\n‡∑úùë•0\nùëÅ\n‚ãØ\n‚ãØ\n‚ãØ\n‚ãØ\n‚ãØ\n‚ãØ\nLinear\nLinear\nTransformer  Encoder\nFusion Block\n‚äï\nOrigin  Motion Diffusion\nFeature Fusion\nText Decomposition\nfine-tuned\nSelf \nAttention\nDropout\nLayer Norm\nLinear\nGELU\nLayer Norm\nDropout\nLinear\nFusion Block\noptional\n‚Ñ±ùëô\n2\n‚Ñ±ùë†\n‚Ñ±ùë°\nùíûùëô\nùíûùë†\nHybrid Retrieval\n‚Ñ∞ùë†\n‚Ñ∞ùëô\n‚Ñ±ùëô\n1\nùëùùúÉ1\nSearch with Anatomical Text\nBest Match\nDataset\nùëö1:ùëÖ\nRefere",
      "contexts": [
        "rk to process retrieved video-sentence\npairs with an attention mechanism, generating hidden states to\nproduce captions. CARE [117] introduces a concept detector\nto produce concept probabilities, and incorporates concept\nrepresentations into a hybrid attention mechanism.\nEgoInstructor [49] uses gated-cross attention to merge text\nand video features, improving the relevance and coherence\nof captions for egocentric videos. Latent representation-based\nRAG, adaptable across modalities and tasks, blen",
        "chitectural components, recent research in this area, current challenges and limitations of RAG, and\nfuture research direction.\n2: Core Components and Architectural Overview of RAG Systems\n2.1 Overview of RAG Models\nRetrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural\nlanguage generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base.\nTraditional large language models (LLMs) such as GPT-3 and BERT, which are pre-train",
        "ings.\nùë°\nMLP\nCross\nAttention\nLinear\nFusion Block\nTransformer  Encoder\nCLIP\nText\nCLIP\nText\nLinear\nùë•ùë°\n1\nùë•ùë°\n2\nùë•ùë°\n3\nùë•ùë°\nùëÅ\nùëö1\nùëö2\nùëßùë°ùëò\nùë†\nùëßùë°ùëò\nùëô\nùëö3\nùëöùëÖ\n‚äï\n‡∑úùë•0\n1\n‡∑úùë•0\n2\n‡∑úùë•0\n3\n‡∑úùë•0\nùëÅ\n‚ãØ\n‚ãØ\n‚ãØ\n‚ãØ\n‚ãØ\n‚ãØ\nLinear\nLinear\nTransformer  Encoder\nFusion Block\n‚äï\nOrigin  Motion Diffusion\nFeature Fusion\nText Decomposition\nfine-tuned\nSelf \nAttention\nDropout\nLayer Norm\nLinear\nGELU\nLayer Norm\nDropout\nLinear\nFusion Block\noptional\n‚Ñ±ùëô\n2\n‚Ñ±ùë†\n‚Ñ±ùë°\nùíûùëô\nùíûùë†\nHybrid Retrieval\n‚Ñ∞ùë†\n‚Ñ∞ùëô\n‚Ñ±ùëô\n1\nùëùùúÉ1\nSearch with Anatomical Text\nBest Match\nDataset\nùëö1:ùëÖ\nRefere",
        "ration system, improves accuracy by using adaptive parameter tuning and\nhybrid retrieval strategies. Tested on NYC Local Law 144 (LL144), HyPA-RAG demonstrates enhanced\ncorrectness and contextual precision, addressing the complexities of legal texts. MemoRAG (Qian et. al.,\n2024) introduces a novel Retrieval-Augmented Generation (RAG) paradigm designed to overcome the\nlimitations of traditional RAG systems in handling ambiguous or unstructured knowledge. MemoRAG‚Äôs\ndual-system architecture utilize",
        "The development of hybrid systems combining retrieval and generation began to gain momentum as\nresearchers recognized the complementary strengths of both approaches. Early efforts in hybrid modeling\ncan be traced back to works like DrQA (Chen et al. 2017), which employed retrieval techniques to fetch\nrelevant documents for question-answering tasks. However, the generative component in such systems\nwas minimal, often limited to selecting text directly from the retrieved documents. Similarly, in m"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        15.303914872420588,
        15.174755015196986,
        15.110360894857841
      ],
      "sources": [
        "2402.19473.pdf",
        "2410.12837.pdf",
        "2402.19473.pdf",
        "2410.12837.pdf",
        "2410.12837.pdf"
      ],
      "execution_time": 4.494388580322266,
      "k1": 1.5,
      "b": 0.9,
      "category": "approaches",
      "query_id": 47,
      "difficulty": "medium"
    },
    {
      "question": "What is adaptive retrieval in Self-RAG?",
      "answer": "nvolves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\nC. Adaptive Retrieval\nAdaptive r\n\nnvolves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\nC. Adaptive Retrieval\nAdaptive r\n\nnvolves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\nC. Adaptive Retrieval\nAdaptive r",
      "contexts": [
        "nvolves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\nC. Adaptive Retrieval\nAdaptive r",
        "nvolves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\nC. Adaptive Retrieval\nAdaptive r",
        "nvolves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\nC. Adaptive Retrieval\nAdaptive r",
        "rs\n(B) Multi-Step Approach\nInefficient\nk times\nComplex Query: \nWhat currency is in \nBilly Giles‚Äô birthplace?\nDocuments\n(Intermediate) \nAnswers\nk times\nStraightforward Query: \nParis is the capital of what?\n(C) Our Adaptive Approach\nAnswer\nSimple Query: \nWhen is the birthday \nof Michael F. Phelps?\nDocuments\nAnswer\nComplex Query: \nWhat currency is in \nBilly Giles‚Äô birthplace?\nDocuments\n(Intermediate) \nAnswers\nk times\nClassifier\nFigure 2: A conceptual comparison of different retrieval-augmented LLM ",
        "rs\n(B) Multi-Step Approach\nInefficient\nk times\nComplex Query: \nWhat currency is in \nBilly Giles‚Äô birthplace?\nDocuments\n(Intermediate) \nAnswers\nk times\nStraightforward Query: \nParis is the capital of what?\n(C) Our Adaptive Approach\nAnswer\nSimple Query: \nWhen is the birthday \nof Michael F. Phelps?\nDocuments\nAnswer\nComplex Query: \nWhat currency is in \nBilly Giles‚Äô birthplace?\nDocuments\n(Intermediate) \nAnswers\nk times\nClassifier\nFigure 2: A conceptual comparison of different retrieval-augmented LLM "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        12.744206113785594,
        12.744206113785594,
        12.744206113785594
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "2403.14403.pdf",
        "Adaptive_RAG_2024.pdf"
      ],
      "execution_time": 4.4230873584747314,
      "k1": 1.5,
      "b": 0.9,
      "category": "approaches",
      "query_id": 48,
      "difficulty": "hard"
    },
    {
      "question": "How does context enrichment improve RAG performance?",
      "answer": "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: ‚Ä¢ (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? ‚Ä¢ (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? ‚Ä¢ (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? ‚Ä¢\n(RQ4): What are the costs associated w\n\nrating continuous context and top-k retrieved chunks in\nstage-2 tuning.\n6.4\nAblation Studies on Inference Stage\nIn Table 7, we show ablation studies on how the number of retrieved context/chunks, context ordering,\nand different retrievers affect the conversational QA and RAG results.\nFirst, we find that using more contexts as inputs do not always improve the results. Utilizing top-5\ncontexts as input yields better results compared to using either top-3 or top-10 contexts. Intuitively,\nmore conte\n\nData management in systems biology II‚Äì Outlook towards the semantic web \nGerhard Mayer, University of Stuttgart, Institute of Biochemical Engineering (IBVT), Allmandring 31, D-70569 \nStuttgart \n \nAbstract \nThe benefit of using ontologies, defined by the respective data standards, is shown. It is presented how \nontologies can be used for the semantic enrichment of data and how this can contribute to the vision of the \nsemantic web to become true. The problems existing today on the way to a true s",
      "contexts": [
        "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: ‚Ä¢ (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? ‚Ä¢ (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? ‚Ä¢ (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? ‚Ä¢\n(RQ4): What are the costs associated w",
        "rating continuous context and top-k retrieved chunks in\nstage-2 tuning.\n6.4\nAblation Studies on Inference Stage\nIn Table 7, we show ablation studies on how the number of retrieved context/chunks, context ordering,\nand different retrievers affect the conversational QA and RAG results.\nFirst, we find that using more contexts as inputs do not always improve the results. Utilizing top-5\ncontexts as input yields better results compared to using either top-3 or top-10 contexts. Intuitively,\nmore conte",
        "Data management in systems biology II‚Äì Outlook towards the semantic web \nGerhard Mayer, University of Stuttgart, Institute of Biochemical Engineering (IBVT), Allmandring 31, D-70569 \nStuttgart \n \nAbstract \nThe benefit of using ontologies, defined by the respective data standards, is shown. It is presented how \nontologies can be used for the semantic enrichment of data and how this can contribute to the vision of the \nsemantic web to become true. The problems existing today on the way to a true s",
        " has also sparked\ndiscussions on whether RAG is still necessary when LLMs\n8https://www.trulens.org/trulens eval/core concepts rag triad/\n9https://kimi.moonshot.cn\nare not constrained by context. In fact, RAG still plays an\nirreplaceable role. On one hand, providing LLMs with a\nlarge amount of context at once will significantly impact its\ninference speed, while chunked retrieval and on-demand input\ncan significantly improve operational efficiency. On the other\nhand, RAG-based generation can quick",
        " has also sparked\ndiscussions on whether RAG is still necessary when LLMs\n8https://www.trulens.org/trulens eval/core concepts rag triad/\n9https://kimi.moonshot.cn\nare not constrained by context. In fact, RAG still plays an\nirreplaceable role. On one hand, providing LLMs with a\nlarge amount of context at once will significantly impact its\ninference speed, while chunked retrieval and on-demand input\ncan significantly improve operational efficiency. On the other\nhand, RAG-based generation can quick"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        22.406023614060878,
        14.051445626622701,
        12.631220633142686
      ],
      "sources": [
        "2410.05779.pdf",
        "Graph_RAG_2024.pdf",
        "0912.2822.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf"
      ],
      "execution_time": 4.101454257965088,
      "k1": 1.5,
      "b": 0.9,
      "category": "approaches",
      "query_id": 49,
      "difficulty": "medium"
    },
    {
      "question": "What is the role of web search in Corrective RAG?",
      "answer": "nowledge for correction. Here, web\nsearch is introduced to search from the Internet as\nelaborated in Section 4.5. This corrective action\nhelps overcome the embarrassing challenge where\nno reliable knowledge can be referred to.\nAmbiguous\nExcept for the above two situations,\nthe remaining will be assigned to an intermediate\naction of Ambiguous. This generally occurs when\nthe accuracy of the retrieval is hard to distinguish\nand the evaluator gives an intermediate score.\nSince the retrieval evaluato\n\nngs and what was his role at Netflix?\nWhat can you tell me about Elon Musk‚Äôs role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson‚Äôs tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association‚Äôs \"Ice Bucket Challenge\" marketing\ncampaign?\nWhat are the specifics of IBM‚Äôs \"Smarter Planet\" marketing c\n\nler size.\nThe Internet as a knowledge base\nMore related\nto our work, the search engine can assume the role\nof the retriever and use the Internet as the source of\nInput\nRetriever\nOutput\nDocuments\nInput\nWeb Search\nDocuments\nBlack-box LLM\nQuery\nInput\nDocuments\nQuery\nOutput\nOutput\nReward\nInput:\nWhat profession does Nicholas Ray and \nElia Kazan have in common?\nQuery: Nicholas Ray profession\nNicholas Ray American author and \ndirector, original name Raymond \nNicholas Kienzle, born August 7, \n1911, Gale",
      "contexts": [
        "nowledge for correction. Here, web\nsearch is introduced to search from the Internet as\nelaborated in Section 4.5. This corrective action\nhelps overcome the embarrassing challenge where\nno reliable knowledge can be referred to.\nAmbiguous\nExcept for the above two situations,\nthe remaining will be assigned to an intermediate\naction of Ambiguous. This generally occurs when\nthe accuracy of the retrieval is hard to distinguish\nand the evaluator gives an intermediate score.\nSince the retrieval evaluato",
        "ngs and what was his role at Netflix?\nWhat can you tell me about Elon Musk‚Äôs role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson‚Äôs tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association‚Äôs \"Ice Bucket Challenge\" marketing\ncampaign?\nWhat are the specifics of IBM‚Äôs \"Smarter Planet\" marketing c",
        "ler size.\nThe Internet as a knowledge base\nMore related\nto our work, the search engine can assume the role\nof the retriever and use the Internet as the source of\nInput\nRetriever\nOutput\nDocuments\nInput\nWeb Search\nDocuments\nBlack-box LLM\nQuery\nInput\nDocuments\nQuery\nOutput\nOutput\nReward\nInput:\nWhat profession does Nicholas Ray and \nElia Kazan have in common?\nQuery: Nicholas Ray profession\nNicholas Ray American author and \ndirector, original name Raymond \nNicholas Kienzle, born August 7, \n1911, Gale",
        "ereby\nvalidating its lightweight nature.\n6\nConclusion & Limitation\nThis paper studies the problem where RAG-based\napproaches are challenged if retrieval goes wrong,\nthereby exposing inaccurate and misleading knowl-\nedge to generative LMs.\nCorrective Retrieval\nAugmented Generation is proposed to improve the\nrobustness of generation. Essentially, a lightweight\nretrieval evaluator is to estimate and trigger three\nknowledge retrieval actions discriminately. With\nthe further leverage of web search an",
        "am?\nWhat is the NLTK library for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger‚Äôs leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon‚Äôs leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.97325581198464,
        17.575828223747873,
        17.429805836885294
      ],
      "sources": [
        "Corrective_RAG_2024.pdf",
        "2403.18802.pdf",
        "2305.14283.pdf",
        "Corrective_RAG_2024.pdf",
        "2403.18802.pdf"
      ],
      "execution_time": 4.334524631500244,
      "k1": 1.5,
      "b": 0.9,
      "category": "approaches",
      "query_id": 50,
      "difficulty": "medium"
    },
    {
      "question": "What metrics should be used to evaluate RAG systems?",
      "answer": "e on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models.The main evaluation objectives include:\nRetrieval Quality. Evaluating the retriev\n\ne on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models.The main evaluation objectives include:\nRetrieval Quality. Evaluating the retriev\n\ne on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models.The main evaluation objectives include:\nRetrieval Quality. Evaluating the retriev",
      "contexts": [
        "e on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models.The main evaluation objectives include:\nRetrieval Quality. Evaluating the retriev",
        "e on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models.The main evaluation objectives include:\nRetrieval Quality. Evaluating the retriev",
        "e on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models.The main evaluation objectives include:\nRetrieval Quality. Evaluating the retriev",
        "evise metrics that can objectively evaluate quality amidst this subjectivity. Secondly,\nthe metrics must consider the dependency of a question‚Äôs relevance and usefulness on its context. A question that\nprovides valuable insights in one context may be deemed irrelevant in another, underscoring the need for context-aware\nmetrics.\nThirdly, we need to evaluate the diversity and novelty of the generated questions. A robust question generation system\nshould produce a wide array of questions covering v",
        "-computer interaction and assistive technologies.\nResearch should explore how RAG systems can leverage BCI data to enhance user experience and\ngenerate context-aware responses.The integration of RAG with AR and VR technologies presents\nopportunities for creating immersive and interactive experiences. Future research should investigate how\nRAG models can be used to enhance AR and VR applications by providing contextually relevant\ninformation and interactions.\n7. Conclusion\nRetrieval-Augmented Gen"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.87443792305046,
        17.87443792305046,
        17.87443792305046
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "2401.08406.pdf",
        "2410.12837.pdf"
      ],
      "execution_time": 4.386133670806885,
      "k1": 1.5,
      "b": 0.9,
      "category": "evaluation",
      "query_id": 16,
      "difficulty": "medium"
    },
    {
      "question": "How do you measure faithfulness in RAG outputs?",
      "answer": "A Pairs\nLDA+\nTranslation1\nTranslation2\nHow many days a week\nshould you lift weight?\nWhen are you to old to\nbuild muscle mass from\nwork out?\nHow\ndo\nI\ngain\nbody\nweight?\nMy\nweight\nis\n90lb\nhow could I gain more\nweight?\nWhat is a 1 set rep?\nMy\nweight\nis\n90lb\nhow could I gain more\nweight?\nWill lose weight faster\nthan average if I work-\nout?\nCan you make your hair\ngrow faster\nIs there any any way to\nget rid of razor bump?\nWhat important func-\ntion do our body hair\nplay?\nHow\ncan\nI\nmake\nmy\nmustache grow \n\nles, we ran-\ndomly sample passages and answers from either:\nthe same Wikipedia document or an entirely ran-\ndom Wikipedia document. This sampling allows\nus to artificially create mock RAG systems for test-\ning ARES. By sampling both related and unrelated\ndocuments/answers, we hope to better gauge the\nefficacy of ARES in judging RAG outputs.\nWe do not evaluate answer faithfulness for KILT\nand SuperGLUE datasets since we do not have\nhuman-annotated hallucinated answers to use for\nevaluation. Howev\n\nmation?\nPersonA: You can find your local SHIP contact information in the back of your\nMedicare & You 2020 Handbook online.\nPersonU: and how do they calculate the adjustments?\nInstead of having this entire conversation, how can PersonU get what he or she is\nlooking for using a single question? Respond with that question.\nrole-assistant:\nHow is the calculation for adjustments made by SHIP determined?\nrole-user:\nGiven the following conversation between PersonU and PersonA:\nPersonU: I need to know h",
      "contexts": [
        "A Pairs\nLDA+\nTranslation1\nTranslation2\nHow many days a week\nshould you lift weight?\nWhen are you to old to\nbuild muscle mass from\nwork out?\nHow\ndo\nI\ngain\nbody\nweight?\nMy\nweight\nis\n90lb\nhow could I gain more\nweight?\nWhat is a 1 set rep?\nMy\nweight\nis\n90lb\nhow could I gain more\nweight?\nWill lose weight faster\nthan average if I work-\nout?\nCan you make your hair\ngrow faster\nIs there any any way to\nget rid of razor bump?\nWhat important func-\ntion do our body hair\nplay?\nHow\ncan\nI\nmake\nmy\nmustache grow ",
        "les, we ran-\ndomly sample passages and answers from either:\nthe same Wikipedia document or an entirely ran-\ndom Wikipedia document. This sampling allows\nus to artificially create mock RAG systems for test-\ning ARES. By sampling both related and unrelated\ndocuments/answers, we hope to better gauge the\nefficacy of ARES in judging RAG outputs.\nWe do not evaluate answer faithfulness for KILT\nand SuperGLUE datasets since we do not have\nhuman-annotated hallucinated answers to use for\nevaluation. Howev",
        "mation?\nPersonA: You can find your local SHIP contact information in the back of your\nMedicare & You 2020 Handbook online.\nPersonU: and how do they calculate the adjustments?\nInstead of having this entire conversation, how can PersonU get what he or she is\nlooking for using a single question? Respond with that question.\nrole-assistant:\nHow is the calculation for adjustments made by SHIP determined?\nrole-user:\nGiven the following conversation between PersonU and PersonA:\nPersonU: I need to know h",
        "stant: You can travel visa-free only if you\nhave a ‚Äúresidence card of a family member of a\nunion citizen‚Äù issued under article 10 of the free\nmovement directive\nUser: I have a temporary residence card also, can\nwe travel together visa-free?\nAssistant: If you do not have an article 10 card, EU\ncountries other than Romania must issue you a visa\nfree of charge and on the basis of an expedited\nprocedure.\nUser: How do I get a article 10 card?\nAssistant:\nChatQA-1.0-13B: The grounds for refusal of that",
        " end the task using the task manager or activity monitor.\nIt is important to note that killing a process can have unintended consequences and should only be done\nas a last resort. It is always recommended to seek professional help if you are unsure about how to manage a process.\n[INST] How do I kill a process?\n[/INST]\nTo kill a process in Linux, you can use the ‚Äòkill‚Äò command followed by the process ID (PID) of the process you want\nto terminate. Here‚Äôs an example:\n‚Äú‚Äò\nkill 1234\n‚Äú‚Äò\nReplace ‚Äò1234‚Äò "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        15.324383466707172,
        15.22909225412155,
        13.897313221051455
      ],
      "sources": [
        "arxiv_1606.03783_2016.pdf",
        "2311.09476.pdf",
        "Graph_RAG_2024.pdf",
        "Graph_RAG_2024.pdf",
        "2308.12950.pdf"
      ],
      "execution_time": 4.360354423522949,
      "k1": 1.5,
      "b": 0.9,
      "category": "evaluation",
      "query_id": 17,
      "difficulty": "hard"
    },
    {
      "question": "What is RAGAS and how is it used for RAG evaluation?",
      "answer": "cerns or special precautions about DRUG?\n‚Ä¢ What is the action of DRUG and how does it work?\n‚Ä¢ Who should get DRUG and why is it prescribed?\n‚Ä¢ What to do in case of a severe reaction to DRUG?\n3. Question Type for other medical entities (e.g. Procedure, Exam, Treatment): Information.\n‚Ä¢ What is Coronary Artery Bypass Surgery?\n‚Ä¢ What are Liver Function Tests?\n4.3\nMedical Resources\nWe used 12 trusted websites to construct a collection of question-answer pairs. For each website, we extracted the free\n\n\nre vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?\nWhat is the primary focus of the Famous Marshmallow Experiment conducted by Walter Mischel?\nWho is Aaron T. Beck known for his work in cognitive therapy?\nWhat is the Thematic Apperception Test (TAT) and how is it commonly utilized in uncovering a\nperson‚Äôs und\n\ncompare ARES\nwith our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge.\nFor our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels\nto score the system. RAGAS also uses GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for\neach evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and\nGPT-3.5 across all the explored datasets. The Kendall‚Äô",
      "contexts": [
        "cerns or special precautions about DRUG?\n‚Ä¢ What is the action of DRUG and how does it work?\n‚Ä¢ Who should get DRUG and why is it prescribed?\n‚Ä¢ What to do in case of a severe reaction to DRUG?\n3. Question Type for other medical entities (e.g. Procedure, Exam, Treatment): Information.\n‚Ä¢ What is Coronary Artery Bypass Surgery?\n‚Ä¢ What are Liver Function Tests?\n4.3\nMedical Resources\nWe used 12 trusted websites to construct a collection of question-answer pairs. For each website, we extracted the free\n",
        "re vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?\nWhat is the primary focus of the Famous Marshmallow Experiment conducted by Walter Mischel?\nWho is Aaron T. Beck known for his work in cognitive therapy?\nWhat is the Thematic Apperception Test (TAT) and how is it commonly utilized in uncovering a\nperson‚Äôs und",
        "compare ARES\nwith our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge.\nFor our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels\nto score the system. RAGAS also uses GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for\neach evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and\nGPT-3.5 across all the explored datasets. The Kendall‚Äô",
        " rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing\nlogits in LLM judge prediction to improve PPI\ncon",
        "vance and answer rele-\nvance of a query-passage-answer triple. For con-\ntext relevance, ARES with a fine-tuned LLM-judge\nis 59.9 percentage points higher than RAGAS while\nfor answer relevance, our system is 14.4 percent-\nage points higher than RAGAS. Overall, ARES\nprovides a more accurate system for automatically\nevaluating RAG configurations than RAGAS by\nleveraging domain-adaptive techniques for prompt-\ning and training as well as utilizing PPI to bolster\nmodel predictions.\nAs an additional co"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        18.31999769043082,
        17.858751728090837,
        17.444016783621112
      ],
      "sources": [
        "arxiv_1901.08079_2019.pdf",
        "2403.18802.pdf",
        "2311.09476.pdf",
        "2311.09476.pdf",
        "2311.09476.pdf"
      ],
      "execution_time": 4.295564413070679,
      "k1": 1.5,
      "b": 0.9,
      "category": "evaluation",
      "query_id": 18,
      "difficulty": "hard"
    },
    {
      "question": "Explain the difference between retrieval and generation metrics",
      "answer": "include:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality. The assessment of generation quality\ncenters on th\n\ninclude:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality. The assessment of generation quality\ncenters on th\n\ninclude:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality. The assessment of generation quality\ncenters on th",
      "contexts": [
        "include:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality. The assessment of generation quality\ncenters on th",
        "include:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality. The assessment of generation quality\ncenters on th",
        "include:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality. The assessment of generation quality\ncenters on th",
        "at certain metrics were not appropriate.\nSome of them did the opposite to what we required them to do, in that they scored\na reformulation without the target higher than one with the target. This could be\ndue to over-emphasis on word ordering. These metrics were discounted at this\nstage. Other metrics were also discounted as the difference between ‚ÄúWith Target‚Äù\nand ‚ÄúWithout Target‚Äù was not large enough; it would have been difÔ¨Åcult to measure\nimprovements in the system with a small difference.\nTh",
        "Table 18: Example prompt demonstrating GPT-4‚Äôs visual input capability.\n38\nGPT-4 visual input example, Moar Layers:\nUser\nCan you explain why this is funny. Think about it step-by-step.\nGPT-4\nThe comic is satirizing the difference in approaches to improving model\nperformance between statistical learning and neural networks.\nIn statistical learning, the character is shown to be concerned with overfit-\nting and suggests a series of complex and technical solutions, such as minimizing\nstructural risk"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        15.558500207915815,
        15.558500207915815,
        15.558500207915815
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "arxiv_1304.7157_2013.pdf",
        "2303.08774.pdf"
      ],
      "execution_time": 4.354732036590576,
      "k1": 1.5,
      "b": 0.9,
      "category": "evaluation",
      "query_id": 19,
      "difficulty": "medium"
    },
    {
      "question": "How can we detect hallucinations in RAG systems?",
      "answer": "ting to LLM\nhallucinations in depth (¬ß3), followed by a review of various strategies and benchmarks employed\nfor the reliable detection of hallucinations in LLMs (¬ß4). We then detail a spectrum of approaches\ndesigned to mitigate these hallucinations (¬ß5). Concluding, we delve into the challenges faced by\ncurrent RAG systems (¬ß6) and delineate potential pathways for forthcoming research (¬ß7).\n2\nDEFINITIONS\nFor the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a\n\n origin of LLM hallucinations is inherently tied to the model‚Äôs uncertainty.\nTherefore, by estimating the uncertainty of the factual content generated by the model, it becomes\nfeasible to detect hallucinations. The methodologies in uncertainty estimation can broadly be\ncategorized into two approaches: based on LLM internal states and LLM behavior, as shown in Fig. 2.\n‚Ä¢ LLM internal states: The internal states of LLMs can serve as informative indicators of their\nuncertainty, often manifested thro\n\ns emerged as a promising strategy to mitigate hallucina-\ntions and improve the factuality of LLM outputs [131, 165, 255, 277]. By incorporating large-scale\nexternal knowledge bases during inference, RAG equips LLMs with up-to-date knowledge, thus\nreducing the potential risk of hallucination due to the inherent knowledge boundaries of LLMs\n[260]. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still\nproduce hallucinations [16]. Hallucinations in RAG present con",
      "contexts": [
        "ting to LLM\nhallucinations in depth (¬ß3), followed by a review of various strategies and benchmarks employed\nfor the reliable detection of hallucinations in LLMs (¬ß4). We then detail a spectrum of approaches\ndesigned to mitigate these hallucinations (¬ß5). Concluding, we delve into the challenges faced by\ncurrent RAG systems (¬ß6) and delineate potential pathways for forthcoming research (¬ß7).\n2\nDEFINITIONS\nFor the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a",
        " origin of LLM hallucinations is inherently tied to the model‚Äôs uncertainty.\nTherefore, by estimating the uncertainty of the factual content generated by the model, it becomes\nfeasible to detect hallucinations. The methodologies in uncertainty estimation can broadly be\ncategorized into two approaches: based on LLM internal states and LLM behavior, as shown in Fig. 2.\n‚Ä¢ LLM internal states: The internal states of LLMs can serve as informative indicators of their\nuncertainty, often manifested thro",
        "s emerged as a promising strategy to mitigate hallucina-\ntions and improve the factuality of LLM outputs [131, 165, 255, 277]. By incorporating large-scale\nexternal knowledge bases during inference, RAG equips LLMs with up-to-date knowledge, thus\nreducing the potential risk of hallucination due to the inherent knowledge boundaries of LLMs\n[260]. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still\nproduce hallucinations [16]. Hallucinations in RAG present con",
        "ucinations [16]. Hallucinations in RAG present considerable complexities, manifesting\nas outputs that are either factually inaccurate or misleading. These hallucinations occur when\nthe content generated by the LLM does not align with real-world facts, fails to accurately reflect\nthe user‚Äôs query, or is not supported by the retrieved information. Such hallucinations can stem\nfrom two primary factors: retrieval failure (¬ß6.1) and generation bottleneck (¬ß6.2). Through\na comprehensive analysis of th",
        "like applying best practices to support the user in the user\ninterface, and monitoring for violations of our usage policies). Evaluations with experts in speciÔ¨Åc\ndomains helped to inform which automatic evaluations we built and which mitigations were most\neÔ¨Äective. We used these observations to retrain the model to be safer (e.g., by refusing harmful\nrequests), improve our internal safety systems (e.g., to ensure that we can detect bad actors), and\nimprove how users experience the model (e.g., t"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        15.10955340076374,
        14.959049916013287,
        14.807477435304504
      ],
      "sources": [
        "2311.05232.pdf",
        "2311.05232.pdf",
        "2311.05232.pdf",
        "2311.05232.pdf",
        "2303.08774.pdf"
      ],
      "execution_time": 4.367228031158447,
      "k1": 1.5,
      "b": 0.9,
      "category": "evaluation",
      "query_id": 20,
      "difficulty": "hard"
    },
    {
      "question": "What is the Lost in the Middle problem in RAG?",
      "answer": "ks. RAG models generate more speciÔ¨Åc and factually accurate\nresponses. ‚Äò?‚Äô indicates factually incorrect responses, * indicates partially correct responses.\nTask\nInput\nModel\nGeneration\nMS-\nMARCO\ndeÔ¨Åne middle\near\nBART\n?The middle ear is the part of the ear between the middle ear and the nose.\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\nwhat currency\nneeded in\nscotland\nBART\nThe currency needed in \n\nro-\npriately (Wei et al., 2022). This approach has been\nsuccessful when all information needed to answer\nthe question is either provided as context (e.g., al-\ngebra questions) or assumed to be present in the\nmodel‚Äôs parameters (e.g., commonsense reasoning).\n1Code, data, and prompts are available at https://\ngithub.com/stonybrooknlp/ircot\nIn what country was \nLost Gravity manufactured?\nThe Lost Gravity was \nmanufactured by¬†Mack Rides.\nMack Rides is a company \nfrom Germany.\nThe answer is Germany.\n\n\n an example,\nconsider the question illustrated in Fig. 1, ‚ÄúIn what\ncountry was Lost Gravity manufactured?‚Äù. The\nWikipedia document retrieved using the question\n(in particular, the roller coaster Lost Gravity) as the\nquery does not mention where Lost Gravity was\nmanufactured. Instead, one must first infer that\nit was manufactured by a company called Mack\nRides, and then perform further retrieval, guided\nby the inferred company name, to obtain evidence\npointing to the manufacturing country.\nThus, ",
      "contexts": [
        "ks. RAG models generate more speciÔ¨Åc and factually accurate\nresponses. ‚Äò?‚Äô indicates factually incorrect responses, * indicates partially correct responses.\nTask\nInput\nModel\nGeneration\nMS-\nMARCO\ndeÔ¨Åne middle\near\nBART\n?The middle ear is the part of the ear between the middle ear and the nose.\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\nwhat currency\nneeded in\nscotland\nBART\nThe currency needed in ",
        "ro-\npriately (Wei et al., 2022). This approach has been\nsuccessful when all information needed to answer\nthe question is either provided as context (e.g., al-\ngebra questions) or assumed to be present in the\nmodel‚Äôs parameters (e.g., commonsense reasoning).\n1Code, data, and prompts are available at https://\ngithub.com/stonybrooknlp/ircot\nIn what country was \nLost Gravity manufactured?\nThe Lost Gravity was \nmanufactured by¬†Mack Rides.\nMack Rides is a company \nfrom Germany.\nThe answer is Germany.\n",
        " an example,\nconsider the question illustrated in Fig. 1, ‚ÄúIn what\ncountry was Lost Gravity manufactured?‚Äù. The\nWikipedia document retrieved using the question\n(in particular, the roller coaster Lost Gravity) as the\nquery does not mention where Lost Gravity was\nmanufactured. Instead, one must first infer that\nit was manufactured by a company called Mack\nRides, and then perform further retrieval, guided\nby the inferred company name, to obtain evidence\npointing to the manufacturing country.\nThus, ",
        "t Reasoning\nRetrieve(¬† ¬† ¬† ¬† ¬†)\nWikipedia Title: Mack Rides\nMack Rides GmbH & Co KG, also ...\nQ: In what country was\nLost Gravity manufactured?\nA:¬†The Lost Gravity was¬†manufactured by Mack\nRides.¬†Mack Rides is a company¬†from\nGermany.¬†The answer is Germany.\n...\nWikipedia Title: Murray Head\nMurray Seafield St George Head ..\n...\nWikipedia Title:¬†Most Beautifullest Hits\nThe Most Beautifullest Hits is ...\nQ:¬†Who wrote the 1970 international hit ..\nA:¬†The 1970 international hit song that\nMurray Head i",
        "ty, Florida. The population of Polk County in 2014 was\n15,140. So the answer is: 15,140.\nQ: Who was born first? Jan de Bont or Raoul Walsh?\nA: Jan de Bont was born on 22 October 1943. Raoul Walsh was born on March 11, 1887. Thus, Raoul Walsh was born the\nfirst. So the answer is: Raoul Walsh.\nQ: In what country was Lost Gravity manufactured?\nA: The Lost Gravity (roller coaster) was manufactured by Mack Rides. Mack Rides is a German company. So the answer is:\nGermany.\nQ: Which of the following had"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        18.554810210062513,
        16.240688858400812,
        15.945373893174189
      ],
      "sources": [
        "RAG_Lewis_2020.pdf",
        "2212.10509.pdf",
        "2212.10509.pdf",
        "2212.10509.pdf",
        "2212.10509.pdf"
      ],
      "execution_time": 4.470389127731323,
      "k1": 1.5,
      "b": 0.9,
      "category": "challenges",
      "query_id": 21,
      "difficulty": "hard"
    },
    {
      "question": "How does context length affect RAG performance?",
      "answer": "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: ‚Ä¢ (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? ‚Ä¢ (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? ‚Ä¢ (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? ‚Ä¢\n(RQ4): What are the costs associated w\n\nrating continuous context and top-k retrieved chunks in\nstage-2 tuning.\n6.4\nAblation Studies on Inference Stage\nIn Table 7, we show ablation studies on how the number of retrieved context/chunks, context ordering,\nand different retrievers affect the conversational QA and RAG results.\nFirst, we find that using more contexts as inputs do not always improve the results. Utilizing top-5\ncontexts as input yields better results compared to using either top-3 or top-10 contexts. Intuitively,\nmore conte\n\n is it important to measure factual recall? . . . . . . . . . . . . . . . . . . .\n18\nA.7\nHow does the prompt postamble affect model responses? . . . . . . . . . . . . .\n18\nA.8 Why was LongFact-Concepts excluded from benchmarking? . . . . . . . . . . .\n19\nA.9\nHow can recall with human-preferred length be applied in other domains? . . . .\n20\nA.10 How does SAFE perform with respect to other humans? . . . . . . . . . . . . .\n20\nA.11 Is there canary text included in LongFact?\n. . . . . . . . . . . . ",
      "contexts": [
        "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: ‚Ä¢ (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? ‚Ä¢ (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? ‚Ä¢ (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? ‚Ä¢\n(RQ4): What are the costs associated w",
        "rating continuous context and top-k retrieved chunks in\nstage-2 tuning.\n6.4\nAblation Studies on Inference Stage\nIn Table 7, we show ablation studies on how the number of retrieved context/chunks, context ordering,\nand different retrievers affect the conversational QA and RAG results.\nFirst, we find that using more contexts as inputs do not always improve the results. Utilizing top-5\ncontexts as input yields better results compared to using either top-3 or top-10 contexts. Intuitively,\nmore conte",
        " is it important to measure factual recall? . . . . . . . . . . . . . . . . . . .\n18\nA.7\nHow does the prompt postamble affect model responses? . . . . . . . . . . . . .\n18\nA.8 Why was LongFact-Concepts excluded from benchmarking? . . . . . . . . . . .\n19\nA.9\nHow can recall with human-preferred length be applied in other domains? . . . .\n20\nA.10 How does SAFE perform with respect to other humans? . . . . . . . . . . . . .\n20\nA.11 Is there canary text included in LongFact?\n. . . . . . . . . . . . ",
        "sm tends to\ntake effect at the begin of the keyphrase extraction process.\n5.3.3\nLength Penalty Factor. Fianlly, we investigate how the length\npenalty factor Œ± affects the performance of the DivGraphPointer.\nResults with different values of length penalty factorŒ± are presented\nin Table 6. Results show that the length penalty factors affect the\nmodel performance significantly. Either a small value of Œ± (e.g.,\nŒ±=0), which tends to generate long phrase, or a big value of Œ± (e.g.,\nŒ± = 100), which ten",
        "ing how this affects document length\nstatistics and removal rates. Interestingly, as the threshold increases, documents get shorter, mirroring\nthe statistics seen for reducing the min_ngram_size. As expected, higher thresholds yield lower\nremoval rates. Following the Jaccard similarity choice used in MinHash deduplication and noting\nthat 0.8 yields median tokens/doc closest to the baseline, we use a threshold of 0.8 going forward.\nShards.\nFinally we simulate how shards affect the statistics of t"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        22.406023614060878,
        16.01743054865942,
        15.125526589743004
      ],
      "sources": [
        "2410.05779.pdf",
        "Graph_RAG_2024.pdf",
        "2403.18802.pdf",
        "arxiv_1905.07689_2019.pdf",
        "2406.11794.pdf"
      ],
      "execution_time": 4.147354602813721,
      "k1": 1.5,
      "b": 0.9,
      "category": "challenges",
      "query_id": 22,
      "difficulty": "medium"
    },
    {
      "question": "What are the common failure modes of RAG systems?",
      "answer": "ation was more pessimistic than a human rater for this domain.\nHowever, one threat to validity with this finding is that BioASQ is\na domain specific dataset and the reviewers were not experts i.e.\nthe large language model may know more than a non-expert.\n5\nFAILURE POINTS OF RAG SYSTEMS\nFrom the case studies we identified a set of failure points presented\nbelow. The following section addresses the research question What\nare the failure points that occur when engineering a RAG system?\nFP1 Missing \n\nreating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n‚Ä¢ What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi\n\n Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n‚Ä¢ What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n‚Ä¢ A catalogue of failure points (FP) that occur in RAG systems.\n‚Ä¢ An experience repo",
      "contexts": [
        "ation was more pessimistic than a human rater for this domain.\nHowever, one threat to validity with this finding is that BioASQ is\na domain specific dataset and the reviewers were not experts i.e.\nthe large language model may know more than a non-expert.\n5\nFAILURE POINTS OF RAG SYSTEMS\nFrom the case studies we identified a set of failure points presented\nbelow. The following section addresses the research question What\nare the failure points that occur when engineering a RAG system?\nFP1 Missing ",
        "reating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n‚Ä¢ What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi",
        " Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n‚Ä¢ What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n‚Ä¢ A catalogue of failure points (FP) that occur in RAG systems.\n‚Ä¢ An experience repo",
        ".1\nCan I reproduce your results? . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nA.2 Why does SAFE use ‚Äúsupported‚Äù, ‚Äúirrelevant‚Äù, and ‚Äúnot supported‚Äù as labels? . .\n16\nA.3 What are the common causes of failure for SAFE? . . . . . . . . . . . . . . . .\n16\nA.4 What are the common causes of failure for human annotators? . . . . . . . . . .\n17\nA.5 Why is the ‚Äúirrelevant‚Äù label not counted? . . . . . . . . . . . . . . . . . . . . .\n17\nA.6 Why is it important to measure factual recall? . . . ",
        "., adding additional facts or missing a vague reference). We also believe that these\nissues can be mitigated by using a more-capable language model. Examples of each cause of error are\nshown in Appendix C.3. In summary, most of the causes of error in SAFE can be remedied by using\na more-capable language model such as GPT-4, though we chose not to do so because GPT-3.5-Turbo\nstill achieves strong performance at a significantly-lower cost.\nA.4\nWhat are the common causes of failure for human annota"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        19.46164574920897,
        18.691535825154602,
        16.885797499315537
      ],
      "sources": [
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "execution_time": 4.322109937667847,
      "k1": 1.5,
      "b": 0.9,
      "category": "challenges",
      "query_id": 23,
      "difficulty": "hard"
    },
    {
      "question": "How can RAG systems handle multi-hop reasoning?",
      "answer": "-computer interaction and assistive technologies.\nResearch should explore how RAG systems can leverage BCI data to enhance user experience and\ngenerate context-aware responses.The integration of RAG with AR and VR technologies presents\nopportunities for creating immersive and interactive experiences. Future research should investigate how\nRAG models can be used to enhance AR and VR applications by providing contextually relevant\ninformation and interactions.\n7. Conclusion\nRetrieval-Augmented Gen\n\n-\nquired knowledge is not always available or up-to-\ndate in models‚Äô parameters and it‚Äôs beneficial to\nretrieve knowledge from external sources (Lazari-\ndou et al., 2022; Kasai et al., 2022).\nHow can we augment chain-of-thought prompt-\ning for open-domain, knowledge-intensive tasks\nthat require complex, multi-step reasoning?\nWhile a one-shot retrieval from a knowledge\nsource based solely on the question can success-\nfully augment LMs with relevant knowledge for\nmany factoid-based tasks (Lewis et\n\ny, operating costs, and\nrobustness.\n6.3\nTesting and Monitoring RAG systems\nSoftware engineering best practices are still emerging for RAG sys-\ntems. Software testing and test case generation are one of the areas\nfor refinement. RAG systems require questions and answers that are\napplication specific often unavailable when indexing unstructured\ndocuments. Emerging work has considered using LLMs for gen-\nerating questions from multiple documents [4]. How to generate\nrealistic domain relevant questi",
      "contexts": [
        "-computer interaction and assistive technologies.\nResearch should explore how RAG systems can leverage BCI data to enhance user experience and\ngenerate context-aware responses.The integration of RAG with AR and VR technologies presents\nopportunities for creating immersive and interactive experiences. Future research should investigate how\nRAG models can be used to enhance AR and VR applications by providing contextually relevant\ninformation and interactions.\n7. Conclusion\nRetrieval-Augmented Gen",
        "-\nquired knowledge is not always available or up-to-\ndate in models‚Äô parameters and it‚Äôs beneficial to\nretrieve knowledge from external sources (Lazari-\ndou et al., 2022; Kasai et al., 2022).\nHow can we augment chain-of-thought prompt-\ning for open-domain, knowledge-intensive tasks\nthat require complex, multi-step reasoning?\nWhile a one-shot retrieval from a knowledge\nsource based solely on the question can success-\nfully augment LMs with relevant knowledge for\nmany factoid-based tasks (Lewis et",
        "y, operating costs, and\nrobustness.\n6.3\nTesting and Monitoring RAG systems\nSoftware engineering best practices are still emerging for RAG sys-\ntems. Software testing and test case generation are one of the areas\nfor refinement. RAG systems require questions and answers that are\napplication specific often unavailable when indexing unstructured\ndocuments. Emerging work has considered using LLMs for gen-\nerating questions from multiple documents [4]. How to generate\nrealistic domain relevant questi",
        "-tuned for a wide range of tasks, including\nquestion-answering, summarization, and dialogue generation. By integrating retrieval with generation,\nT5-based RAG models have been shown to outperform traditional generative models like GPT-3 and\nBART on several benchmarks, including the Natural Questions dataset and the TriviaQA dataset.\nMoreover, T5's ability to handle complex multi-task learning makes it a popular choice for RAG systems\nthat need to tackle a diverse range of knowledge-intensive tas",
        "ters and how they work can help someone learn how to\nstart a car without a key. 2. Ignition systems: The ignition system is responsible\nfor generating the spark that ignites the fuel in the engine. There are several\ntypes of ignition systems, including distributor-based systems and coil-on-plug\nsystems. Understanding how the ignition system works can help someone\nlearn how to start a car without a key. 3. Battery and electrical systems: The\nbattery and electrical systems in a car are responsible"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.86790919046312,
        14.771587715130138,
        13.888923499472657
      ],
      "sources": [
        "2410.12837.pdf",
        "2212.10509.pdf",
        "2401.05856.pdf",
        "2410.12837.pdf",
        "2307.09288.pdf"
      ],
      "execution_time": 4.1253252029418945,
      "k1": 1.5,
      "b": 0.9,
      "category": "challenges",
      "query_id": 24,
      "difficulty": "hard"
    },
    {
      "question": "What are the scalability challenges in RAG systems?",
      "answer": "ual information access for\nglobal inclusivity.\n5. Current Challenges and Limitations in Retrieval-Augmented Generation (RAG):\nThis section intends to highlight the current challenges and limitations of RAG considering the current\nlandscape of the system and this would shape the future research directions in the field.\nScalability and Efficiency: One of the primary challenges for RAG models is scalability. As retrieval\ncomponents rely on external databases, handling vast and dynamically growing d\n\n Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n‚Ä¢ What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n‚Ä¢ A catalogue of failure points (FP) that occur in RAG systems.\n‚Ä¢ An experience repo\n\nnd required resources using RAG without training or fine-\ntuning is an attractive proposition. However, challenges arise when\nusing large language models for information extraction such as\nperformance with long text [8].\nA recent survey [19] showed that large language models are\nused across the RAG pipeline including retriever, data generation,\nrewriter, and reader. Our work complements this survey by taking\na software engineering perspective to shine a light on what issues\nengineers will face a",
      "contexts": [
        "ual information access for\nglobal inclusivity.\n5. Current Challenges and Limitations in Retrieval-Augmented Generation (RAG):\nThis section intends to highlight the current challenges and limitations of RAG considering the current\nlandscape of the system and this would shape the future research directions in the field.\nScalability and Efficiency: One of the primary challenges for RAG models is scalability. As retrieval\ncomponents rely on external databases, handling vast and dynamically growing d",
        " Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n‚Ä¢ What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n‚Ä¢ A catalogue of failure points (FP) that occur in RAG systems.\n‚Ä¢ An experience repo",
        "nd required resources using RAG without training or fine-\ntuning is an attractive proposition. However, challenges arise when\nusing large language models for information extraction such as\nperformance with long text [8].\nA recent survey [19] showed that large language models are\nused across the RAG pipeline including retriever, data generation,\nrewriter, and reader. Our work complements this survey by taking\na software engineering perspective to shine a light on what issues\nengineers will face a",
        "RAG models. Concur-\nrently, state-of-the-art automated tools like RAGAS [164],\nARES [165], and TruLens8 employ LLMs to adjudicate the\nquality scores. These tools and benchmarks collectively form\na robust framework for the systematic evaluation of RAG\nmodels, as summarized in Table IV.\nVII. DISCUSSION AND FUTURE PROSPECTS\nDespite the considerable progress in RAG technology, sev-\neral challenges persist that warrant in-depth research.This\nchapter will mainly introduce the current challenges and fu",
        "RAG models. Concur-\nrently, state-of-the-art automated tools like RAGAS [164],\nARES [165], and TruLens8 employ LLMs to adjudicate the\nquality scores. These tools and benchmarks collectively form\na robust framework for the systematic evaluation of RAG\nmodels, as summarized in Table IV.\nVII. DISCUSSION AND FUTURE PROSPECTS\nDespite the considerable progress in RAG technology, sev-\neral challenges persist that warrant in-depth research.This\nchapter will mainly introduce the current challenges and fu"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        20.054852582919636,
        15.629032058032625,
        14.53498114934627
      ],
      "sources": [
        "2410.12837.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "execution_time": 4.301090717315674,
      "k1": 1.5,
      "b": 0.9,
      "category": "challenges",
      "query_id": 25,
      "difficulty": "medium"
    },
    {
      "question": "What is the optimal chunk size for RAG systems?",
      "answer": "uality contents.\nReACT [132] uses Chain-of-Thought (CoT) [133] to break\nqueries down for recursive retrieval and provide richer infor-\nmation. RATP [134] uses the Monte-Carlo Tree Search for\nsimulations to select optimal retrieval content, which is then\ntemplated and forwarded to the generator for output.\nChunk Optimization: Chunk optimization refers to adjusting\nchunk size for improved retrieval results.\nLlamaIndex [135] incorporates a series of chunk optimiza-\ntion methods, one of which operat\n\nl recall and reduce time\nbut may lack sufficient context.\nFinding the optimal chunk size involves a balance between some metrics such as faithfulness, and\nrelevancy. Faithfulness measures whether the response is hallucinated or matches the retrieved texts.\nChunk Size\nlyft_2021\nAverage\nFaithfulness\nAverage\nRelevancy\n2048\n80.37\n91.11\n1024\n94.26\n95.56\n512\n97.59\n97.41\n256\n97.22\n97.78\n128\n95.74\n97.22\nTable 3: Comparison of different chunk sizes.\nRelevancy measures whether the retrieved texts\nand resp\n\nhunks into the\ncontext to ask the LLM to generate an answer from the included\ncontext. This facilitates continuously updating the knowledge with\nnew documents and also gives the control over what chunks the user\nis able to access. However, optimal strategies for chunk embedding,\nretrieval, and contextual fusion remain active research. Further\nwork should systematically compare finetuning and RAG paradigms\nacross factors including accuracy, latency, operating costs, and\nrobustness.\n6.3\nTesting an",
      "contexts": [
        "uality contents.\nReACT [132] uses Chain-of-Thought (CoT) [133] to break\nqueries down for recursive retrieval and provide richer infor-\nmation. RATP [134] uses the Monte-Carlo Tree Search for\nsimulations to select optimal retrieval content, which is then\ntemplated and forwarded to the generator for output.\nChunk Optimization: Chunk optimization refers to adjusting\nchunk size for improved retrieval results.\nLlamaIndex [135] incorporates a series of chunk optimiza-\ntion methods, one of which operat",
        "l recall and reduce time\nbut may lack sufficient context.\nFinding the optimal chunk size involves a balance between some metrics such as faithfulness, and\nrelevancy. Faithfulness measures whether the response is hallucinated or matches the retrieved texts.\nChunk Size\nlyft_2021\nAverage\nFaithfulness\nAverage\nRelevancy\n2048\n80.37\n91.11\n1024\n94.26\n95.56\n512\n97.59\n97.41\n256\n97.22\n97.78\n128\n95.74\n97.22\nTable 3: Comparison of different chunk sizes.\nRelevancy measures whether the retrieved texts\nand resp",
        "hunks into the\ncontext to ask the LLM to generate an answer from the included\ncontext. This facilitates continuously updating the knowledge with\nnew documents and also gives the control over what chunks the user\nis able to access. However, optimal strategies for chunk embedding,\nretrieval, and contextual fusion remain active research. Further\nwork should systematically compare finetuning and RAG paradigms\nacross factors including accuracy, latency, operating costs, and\nrobustness.\n6.3\nTesting an",
        " sizes without a drop in quality\n(Figure 3) or the forced introduction of noise. We interate self-reflection steps up to a specified\nmaximum number of times.\n0\n1\n2\n3\n0\n10000\n20000\n30000\nNumber of self-reflection iterations performed\nEntity references detected\n600 chunk size\n1200 chunk size\n2400 chunk size\nFigure 3: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and self-reflection iterations for our generic entity extraction prompt with\ngpt-",
        " techniques such as small-to-big and sliding window improve retrieval quality by organizing\nchunk block relationships. Small-sized blocks are used to match queries, and larger blocks that\ninclude the small ones along with contextual information are returned.\nTo demonstrate the effectiveness of advanced chunking techniques, we use the LLM-Embedder [20]\nmodel as an embedding model. The smaller chunk size is 175 tokens, the larger chunk size is 512\ntokens and the chunk overlap is 20 tokens. Techniq"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        21.165144794721392,
        20.467896043727077,
        19.68314180747175
      ],
      "sources": [
        "2402.19473.pdf",
        "2407.01219.pdf",
        "2401.05856.pdf",
        "2404.16130.pdf",
        "2407.01219.pdf"
      ],
      "execution_time": 4.2758049964904785,
      "k1": 1.5,
      "b": 0.9,
      "category": "implementation",
      "query_id": 26,
      "difficulty": "medium"
    },
    {
      "question": "How should chunk overlap be configured in RAG?",
      "answer": " During\nthe Index process each document is split into smaller chunks that\nare converted into an embedding using an embedding model. The\noriginal chunk and the embedding are then indexed in a database.\nSoftware engineers face design decisions around how best to chunk\nthe document and how large a chunk should be. If chunks are too\nsmall certain questions cannot be answered, if the chunks are too\nlong then the answers include generated noise.\nDifferent types of documents require different chunking \n\n techniques such as small-to-big and sliding window improve retrieval quality by organizing\nchunk block relationships. Small-sized blocks are used to match queries, and larger blocks that\ninclude the small ones along with contextual information are returned.\nTo demonstrate the effectiveness of advanced chunking techniques, we use the LLM-Embedder [20]\nmodel as an embedding model. The smaller chunk size is 175 tokens, the larger chunk size is 512\ntokens and the chunk overlap is 20 tokens. Techniq\n\ntokens and the chunk overlap is 20 tokens. Techniques like small-to-big and sliding window improve\nretrieval quality by maintaining context and ensuring relevant information is retrieved. Detailed\nresults are shown in Table 4.\n3.2.3\nEmbedding Model Selection\nChoosing the right embedding model is crucial for effective semantic matching of queries\nand chunk blocks. We use the evaluation module of FlagEmbedding6 which uses the dataset\nChunk Skill\nlyft_2021\nAverage\nFaithfulness\nAverage\nRelevancy\nOri",
      "contexts": [
        " During\nthe Index process each document is split into smaller chunks that\nare converted into an embedding using an embedding model. The\noriginal chunk and the embedding are then indexed in a database.\nSoftware engineers face design decisions around how best to chunk\nthe document and how large a chunk should be. If chunks are too\nsmall certain questions cannot be answered, if the chunks are too\nlong then the answers include generated noise.\nDifferent types of documents require different chunking ",
        " techniques such as small-to-big and sliding window improve retrieval quality by organizing\nchunk block relationships. Small-sized blocks are used to match queries, and larger blocks that\ninclude the small ones along with contextual information are returned.\nTo demonstrate the effectiveness of advanced chunking techniques, we use the LLM-Embedder [20]\nmodel as an embedding model. The smaller chunk size is 175 tokens, the larger chunk size is 512\ntokens and the chunk overlap is 20 tokens. Techniq",
        "tokens and the chunk overlap is 20 tokens. Techniques like small-to-big and sliding window improve\nretrieval quality by maintaining context and ensuring relevant information is retrieved. Detailed\nresults are shown in Table 4.\n3.2.3\nEmbedding Model Selection\nChoosing the right embedding model is crucial for effective semantic matching of queries\nand chunk blocks. We use the evaluation module of FlagEmbedding6 which uses the dataset\nChunk Skill\nlyft_2021\nAverage\nFaithfulness\nAverage\nRelevancy\nOri",
        " sizes without a drop in quality\n(Figure 3) or the forced introduction of noise. We interate self-reflection steps up to a specified\nmaximum number of times.\n0\n1\n2\n3\n0\n10000\n20000\n30000\nNumber of self-reflection iterations performed\nEntity references detected\n600 chunk size\n1200 chunk size\n2400 chunk size\nFigure 3: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and self-reflection iterations for our generic entity extraction prompt with\ngpt-",
        "ic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.\nFor example, a"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        19.16240039275619,
        15.575426406601567,
        15.47490429151801
      ],
      "sources": [
        "2401.05856.pdf",
        "2407.01219.pdf",
        "2407.01219.pdf",
        "2404.16130.pdf",
        "2403.18802.pdf"
      ],
      "execution_time": 4.385206460952759,
      "k1": 1.5,
      "b": 0.9,
      "category": "implementation",
      "query_id": 27,
      "difficulty": "medium"
    },
    {
      "question": "What embedding models are best for RAG retrieval?",
      "answer": "almor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. oLMpics‚Äì on what language\nmodel pre-training captures. Transactions of the Association for Computational Linguistics, 8:\n743‚Äì758, 2020. URL https://arxiv.org/abs/1912.13283.\nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,\nOleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models\nwith retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762, 2023. URL https:\n\n perform best at the smallest learning rate. The\nxLSTM[0:1] reaches similar performance across all learning rates.\nWikitext-103 Rare Token Prediction.\nFor this exemplary experiment on rare token prediction, we\ntrained 125M-sized models on Wikitext-103 (Merity et al., 2017). All models have an embedding\ndimension of 768 in a post up-projection structure of 12 residual blocks. The Transformer model\n(Llama) uses Multi-Head Attention, for what is called LSTM the Multi-Head Attention is replaced by\na\n\nr\nsmall text.\nOpensource sentence embedding models performed\nas well as closed source alternatives on small text.\nBioASQ, AI Tutor\nFP2-7\nRAG systems require continuous calibration.\nRAG systems receive unknown input at runtime\nrequiring constant monitoring.\nAI Tutor, BioASQ\nFP1, FP2\nImplement a RAG pipeline for configuration.\nA RAG system requires calibrating chunk size,\nembedding strategy, chunking strategy, retrieval\nstrategy, consolidation strategy, context size, and\nprompts.\nCognitive Reviewe",
      "contexts": [
        "almor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. oLMpics‚Äì on what language\nmodel pre-training captures. Transactions of the Association for Computational Linguistics, 8:\n743‚Äì758, 2020. URL https://arxiv.org/abs/1912.13283.\nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,\nOleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models\nwith retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762, 2023. URL https:",
        " perform best at the smallest learning rate. The\nxLSTM[0:1] reaches similar performance across all learning rates.\nWikitext-103 Rare Token Prediction.\nFor this exemplary experiment on rare token prediction, we\ntrained 125M-sized models on Wikitext-103 (Merity et al., 2017). All models have an embedding\ndimension of 768 in a post up-projection structure of 12 residual blocks. The Transformer model\n(Llama) uses Multi-Head Attention, for what is called LSTM the Multi-Head Attention is replaced by\na",
        "r\nsmall text.\nOpensource sentence embedding models performed\nas well as closed source alternatives on small text.\nBioASQ, AI Tutor\nFP2-7\nRAG systems require continuous calibration.\nRAG systems receive unknown input at runtime\nrequiring constant monitoring.\nAI Tutor, BioASQ\nFP1, FP2\nImplement a RAG pipeline for configuration.\nA RAG system requires calibrating chunk size,\nembedding strategy, chunking strategy, retrieval\nstrategy, consolidation strategy, context size, and\nprompts.\nCognitive Reviewe",
        " Tutor\nFP5-7\nJailbreaks bypass the RAG system and hit the safety\ntraining.\nResearch suggests fine-tuning LLMs reverses safety\ntraining [11], test all fine-tuned LLMs for RAG sys-\ntem.\nAI Tutor\nFP2, FP4\nAdding meta-data improves retrieval.\nAdding the file name and chunk number into the\nretrieved context helped the reader extract the re-\nquired information. Useful for chat dialogue.\nAI Tutor\nFP2, FP4-7\nOpen source embedding models perform better for\nsmall text.\nOpensource sentence embedding models",
        "nd required resources using RAG without training or fine-\ntuning is an attractive proposition. However, challenges arise when\nusing large language models for information extraction such as\nperformance with long text [8].\nA recent survey [19] showed that large language models are\nused across the RAG pipeline including retriever, data generation,\nrewriter, and reader. Our work complements this survey by taking\na software engineering perspective to shine a light on what issues\nengineers will face a"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        14.602735398213778,
        14.583538575116124,
        13.964321122292636
      ],
      "sources": [
        "2401.18059.pdf",
        "2405.04517.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf"
      ],
      "execution_time": 4.41499662399292,
      "k1": 1.5,
      "b": 0.9,
      "category": "implementation",
      "query_id": 28,
      "difficulty": "medium"
    },
    {
      "question": "How do you choose the number of retrieved documents (top-k)?",
      "answer": "augmented with random documents,\ndocuments retrieved by REPLUG, and documents retrieved\nby REPLUG LSR. We observed that ensembling random\ndocuments leads to worse performance, indicating that the\nperformance gains of REPLUG do not solely come from the\nensembling effect. Instead, ensembling the relevant docu-\nments is crucial for the success of REPLUG. Additionally,\nas more documents were ensembled, the performance of\nREPLUG and REPLUG LSR improved monotonically. How-\never, a small number of docu\n\ndate RAG‚Äôs world knowledge by simply replacing its non-parametric memory.\nEffect of Retrieving more documents\nModels are trained with either 5 or 10 retrieved latent\ndocuments, and we do not observe signiÔ¨Åcant differences in performance between them. We have the\nÔ¨Çexibility to adjust the number of retrieved documents at test time, which can affect performance and\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\nOpen-domain QA results for RAG-Sequen\n\now frequently the\ncorrect answer is returned and, as the correct an-\n(a) Recall\n(b) Avg number of\nrelevant documents\n0.2\n0.4\n0.6\n0.8\n1.0\n2\n4\n6\n8\n10\ntop‚àín retrieved documents\nrecall\n1\n2\n3\n4\n5\n2\n4\n6\n8\n10\ntop‚àín retrieved documents\navg. number relevant documents\nFigure 2: Recall (a) and average number of rel-\nevant documents (b) for growing top-n conÔ¨Ågu-\nrations and a static corpus size (full Wikipedia\ndump). While the recall is converging the number\nof relevant documents keeps growing resulting in\n",
      "contexts": [
        "augmented with random documents,\ndocuments retrieved by REPLUG, and documents retrieved\nby REPLUG LSR. We observed that ensembling random\ndocuments leads to worse performance, indicating that the\nperformance gains of REPLUG do not solely come from the\nensembling effect. Instead, ensembling the relevant docu-\nments is crucial for the success of REPLUG. Additionally,\nas more documents were ensembled, the performance of\nREPLUG and REPLUG LSR improved monotonically. How-\never, a small number of docu",
        "date RAG‚Äôs world knowledge by simply replacing its non-parametric memory.\nEffect of Retrieving more documents\nModels are trained with either 5 or 10 retrieved latent\ndocuments, and we do not observe signiÔ¨Åcant differences in performance between them. We have the\nÔ¨Çexibility to adjust the number of retrieved documents at test time, which can affect performance and\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\nOpen-domain QA results for RAG-Sequen",
        "ow frequently the\ncorrect answer is returned and, as the correct an-\n(a) Recall\n(b) Avg number of\nrelevant documents\n0.2\n0.4\n0.6\n0.8\n1.0\n2\n4\n6\n8\n10\ntop‚àín retrieved documents\nrecall\n1\n2\n3\n4\n5\n2\n4\n6\n8\n10\ntop‚àín retrieved documents\navg. number relevant documents\nFigure 2: Recall (a) and average number of rel-\nevant documents (b) for growing top-n conÔ¨Ågu-\nrations and a static corpus size (full Wikipedia\ndump). While the recall is converging the number\nof relevant documents keeps growing resulting in\n",
        "he precision.\nThe parallel search scheme is formally described in Algorithm 1.\nAlgorithm 1 Parallel Search Scheme\nInput: query q, number of initial candidate documents n, number of candi-\ndate documents retrieved by symbolic search m\nOutput: n initial candidate documents\nM ‚Üêm documents based on the inverted index and BM25\nC ‚Üên documents based on the undirected k-NN graph index and cosine\nsimilarity\nfor each d ‚ààC do\nif d ‚ààM then\ncontinue\nend if\nM ‚ÜêM ‚à™{d }\nif |M | == n then\nbreak\nend if\nend for\nre",
        "roblem in memory, it‚Äôs probably trivial.  ‚ÄúIt‚Äôs slow‚Äù is the hardest problem you‚Äôll ever debug.  Implement \nbackpressure throughout your system.  Find ways to be partially available.  Metrics are the only way to get your job \ndone.  Use percentiles, not averages.  Learn to estimate your capacity.  Feature flags are how infrastructure is rolled \nout.  Choose id spaces wisely.  Exploit data-locality.  Writing cached data back to storage is bad.  Computers can do \nmore than you think they can.  Use"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.663599725503342,
        16.932611680541136,
        16.197192036022074
      ],
      "sources": [
        "REPLUG_2023.pdf",
        "RAG_Lewis_2020.pdf",
        "arxiv_1808.06528_2018.pdf",
        "arxiv_1806.10869_2018.pdf",
        "arxiv_1307.6937_2013.pdf"
      ],
      "execution_time": 4.294533729553223,
      "k1": 1.5,
      "b": 0.9,
      "category": "implementation",
      "query_id": 29,
      "difficulty": "medium"
    },
    {
      "question": "What vector databases are commonly used for RAG?",
      "answer": "n distributional feature they are referred\nto as explicit vector representations [117]. On the other hand, when the vectors are dense, small\n(k ‚â™|T|), and learnt from data then they are commonly referred to as embeddings. For both explicit\nand embedding based representations several distance metrics can be used to deÔ¨Åne similarity between\nterms, although cosine similarity is commonly used.\nsim(‚Éóvi,‚Éóvj) = cos(‚Éóvi,‚Éóvj) =\n‚Éóv ‚ä∫\ni ‚Éóvj\n‚à•‚Éóvi‚à•‚à•‚Éóvj‚à•\n(23)\nMost embeddings are learnt from explicit vector sp\n\nler\nthan that of the latter.\nThus, we select the\nLLM-Embedder [20] for its balance of\nperformance and size.\n3.2.4\nMetadata Addition\nEnhancing chunk blocks with metadata like titles, keywords, and hypothetical questions can improve\nretrieval, provide more ways to post-process retrieved texts, and help LLMs better understand\nretrieved information. A detailed study on metadata inclusion will be addressed in future work.\n3.3\nVector Databases\nVector databases store embedding vectors with their metada\n\n1\nScalable Probabilistic Similarity Ranking in\nUncertain Databases (Technical Report)\nThomas Bernecker, Hans-Peter Kriegel, Nikos Mamoulis, Matthias Renz,\nand Andreas ZueÔ¨Çe\nAbstract\nThis paper introduces a scalable approach for probabilistic top-k similarity ranking on uncertain vector data.\nEach uncertain object is represented by a set of vector instances that are assumed to be mutually-exclusive. The\nobjective is to rank the uncertain data according to their distance to a reference object. We ",
      "contexts": [
        "n distributional feature they are referred\nto as explicit vector representations [117]. On the other hand, when the vectors are dense, small\n(k ‚â™|T|), and learnt from data then they are commonly referred to as embeddings. For both explicit\nand embedding based representations several distance metrics can be used to deÔ¨Åne similarity between\nterms, although cosine similarity is commonly used.\nsim(‚Éóvi,‚Éóvj) = cos(‚Éóvi,‚Éóvj) =\n‚Éóv ‚ä∫\ni ‚Éóvj\n‚à•‚Éóvi‚à•‚à•‚Éóvj‚à•\n(23)\nMost embeddings are learnt from explicit vector sp",
        "ler\nthan that of the latter.\nThus, we select the\nLLM-Embedder [20] for its balance of\nperformance and size.\n3.2.4\nMetadata Addition\nEnhancing chunk blocks with metadata like titles, keywords, and hypothetical questions can improve\nretrieval, provide more ways to post-process retrieved texts, and help LLMs better understand\nretrieved information. A detailed study on metadata inclusion will be addressed in future work.\n3.3\nVector Databases\nVector databases store embedding vectors with their metada",
        "1\nScalable Probabilistic Similarity Ranking in\nUncertain Databases (Technical Report)\nThomas Bernecker, Hans-Peter Kriegel, Nikos Mamoulis, Matthias Renz,\nand Andreas ZueÔ¨Çe\nAbstract\nThis paper introduces a scalable approach for probabilistic top-k similarity ranking on uncertain vector data.\nEach uncertain object is represented by a set of vector instances that are assumed to be mutually-exclusive. The\nobjective is to rank the uncertain data according to their distance to a reference object. We ",
        "agnose bladder cancer?\nPersonA: Yes, urinalysis can be used as a screening and diagnostic tool for\nbladder cancer. One way to test for bladder cancer is to check for blood in the\nurine, which is called hematuria. This can be done during a urinalysis by examining\nthe urine cells under a special microscope.\nPersonU: What other conditions or diseases can be detected through urinalysis?\n23\nPersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections,\nevaluate causes of kidney ",
        "gates the contribution of\neach component and provides insights into optimal RAG practices through extensive experimentation.\nThe optional methods considered for each component are indicated in bold fonts, while the methods\nunderlined indicate the default choice for individual modules. The methods indicated in blue font\ndenote the best-performing selections identified empirically.\nvector databases to efficiently store feature representations, and the methods for effectively fine-tuning\nLLMs (see "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        18.541923649509407,
        14.95580890770823,
        13.919084352767678
      ],
      "sources": [
        "arxiv_1705.01509_2017.pdf",
        "2407.01219.pdf",
        "0907.2868.pdf",
        "Graph_RAG_2024.pdf",
        "2407.01219.pdf"
      ],
      "execution_time": 4.2485456466674805,
      "k1": 1.5,
      "b": 0.9,
      "category": "implementation",
      "query_id": 30,
      "difficulty": "easy"
    },
    {
      "question": "Compare BM25 and dense vector retrieval methods",
      "answer": "le implement is HyDE [10],\nRecent studies, such as [44], indicate that combining lexical-based search with vector search signifi-\ncantly enhances performance. In this study, we use BM25 for sparse retrieval and Contriever [45], an\nunsupervised contrastive encoder, for dense retrieval, serving as two robust baselines based on Thakur\net al. [46].\n3.4.1\nResults for different retrieval methods\nWe evaluated the performance of different search methods on the TREC DL 2019 and 2020 passage\nranking datas\n\nisms are commonly used, ranging from traditional methods like BM25 to more sophisticated\ntechniques like Dense Passage Retrieval (DPR).\n2.2.1 BM25\nBM25 is a well-established information retrieval algorithm that uses the term frequency-inverse document\nfrequency (TF-IDF) to rank documents according to relevance. Despite being a classical method, BM25\nremains a strong baseline for many modern retrieval systems, including those used in RAG models.\nBM25 calculates the relevance score of a document b\n\nestions\nand CuratedTrec).\n8\nMain Results\n8.1\nBaselines\nWe compare against other retrieval methods by us-\ning alternate retrieval scores Sretr(b, q), but with\nthe same reader.\nBM25\nA de-facto state-of-the-art unsupervised\nretrieval method is BM25 (Robertson et al., 2009).\nIt has been shown to be robust for both traditional\ninformation retrieval tasks, and evidence retrieval\nfor question answering (Yang et al., 2017).2 Since\n2We also include the title, which was slightly beneÔ¨Åcial.\nModel\nBM25\nNNLM",
      "contexts": [
        "le implement is HyDE [10],\nRecent studies, such as [44], indicate that combining lexical-based search with vector search signifi-\ncantly enhances performance. In this study, we use BM25 for sparse retrieval and Contriever [45], an\nunsupervised contrastive encoder, for dense retrieval, serving as two robust baselines based on Thakur\net al. [46].\n3.4.1\nResults for different retrieval methods\nWe evaluated the performance of different search methods on the TREC DL 2019 and 2020 passage\nranking datas",
        "isms are commonly used, ranging from traditional methods like BM25 to more sophisticated\ntechniques like Dense Passage Retrieval (DPR).\n2.2.1 BM25\nBM25 is a well-established information retrieval algorithm that uses the term frequency-inverse document\nfrequency (TF-IDF) to rank documents according to relevance. Despite being a classical method, BM25\nremains a strong baseline for many modern retrieval systems, including those used in RAG models.\nBM25 calculates the relevance score of a document b",
        "estions\nand CuratedTrec).\n8\nMain Results\n8.1\nBaselines\nWe compare against other retrieval methods by us-\ning alternate retrieval scores Sretr(b, q), but with\nthe same reader.\nBM25\nA de-facto state-of-the-art unsupervised\nretrieval method is BM25 (Robertson et al., 2009).\nIt has been shown to be robust for both traditional\ninformation retrieval tasks, and evidence retrieval\nfor question answering (Yang et al., 2017).2 Since\n2We also include the title, which was slightly beneÔ¨Åcial.\nModel\nBM25\nNNLM",
        " in Appendix.\nWe use E5-large (Wang et al. 2022) as the main\nretriever in our experiments. The impact of other re-\ntrievers, i.e., BM25 (Robertson and Zaragoza 2009),\nBGE-base (Xiao et al. 2023), and E5-base, is studied in\nour further analysis. Among these retrievers, BM25 is a non-\nneural sparse retrieval algorithm, while others are neural-\nbased dense retrievers. In general, dense retrievers perform\nbetter on several benchmarks (Muennighoff et al. 2023).\nBaseline Methods\nWe consider both the b",
        "ge queries that require an understanding of context.\nDespite this limitation, BM25 is still widely used because of its simplicity and efficiency. BM25 is effective\nfor tasks involving simpler, keyword-based queries, although more modern retrieval models like DPR tend\nto outperform it in semantically complex tasks.\n2.2.2 Dense Passage Retrieval (DPR)\nDense Passage Retrieval (DPR), introduced by Karpukhin et al. (2020), represents a more modern\napproach to information retrieval. It uses a dense ve"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        20.13183712400843,
        19.639261390452525,
        19.195531610665714
      ],
      "sources": [
        "2407.01219.pdf",
        "2410.12837.pdf",
        "arxiv_1906.00300_2019.pdf",
        "2405.19670.pdf",
        "2410.12837.pdf"
      ],
      "execution_time": 4.414931535720825,
      "k1": 1.5,
      "b": 0.9,
      "category": "comparison",
      "query_id": 31,
      "difficulty": "medium"
    },
    {
      "question": "When should you use Naive RAG vs Advanced RAG?",
      "answer": "hree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to\n\nhree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to\n\nhree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to",
      "contexts": [
        "hree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to",
        "hree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to",
        "hree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to",
        "loys pre-retrieval and post-retrieval strate-\ngies. To tackle the indexing issues, Advanced RAG refines\nits indexing techniques through the use of a sliding window\napproach, fine-grained segmentation, and the incorporation of\nmetadata. Additionally, it incorporates several optimization\nmethods to streamline the retrieval process [8].\n4\nFig. 3.\nComparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\nAdvanced RAG ",
        "loys pre-retrieval and post-retrieval strate-\ngies. To tackle the indexing issues, Advanced RAG refines\nits indexing techniques through the use of a sliding window\napproach, fine-grained segmentation, and the incorporation of\nmetadata. Additionally, it incorporates several optimization\nmethods to streamline the retrieval process [8].\n4\nFig. 3.\nComparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\nAdvanced RAG "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        23.597172450061834,
        23.597172450061834,
        23.597172450061834
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "execution_time": 4.163515329360962,
      "k1": 1.5,
      "b": 0.9,
      "category": "comparison",
      "query_id": 32,
      "difficulty": "medium"
    },
    {
      "question": "Compare Self-RAG with Corrective RAG approaches",
      "answer": "nately. With\nthe further leverage of web search and optimized\nknowledge utilization, CRAG has significantly\nimproved the ability of automatic self-correction\nand efficient utilization of retrieved documents.\nExperiments extensively demonstrate its adaptabil-\nity to RAG-based approaches as well as general-\nizability across short- and long-form generation\ntasks. While we primarily proposed to improve the\nRAG framework from a corrective perspective and\nCRAG can be seamlessly coupled with various\nRA\n\n SelfRAG-LLaMA2-7b\nPopQA\nCRAG\n54.9\n59.8\nRAG\n50.5\n52.8\nRAG w. web\n52.2\n53.8\nSelf-CRAG\n49.0\n61.8\nSelf-RAG\n29.0\n54.9\nSelf-RAG w. web\n24.9\n57.9\nTable 5: Comparison results between CRAG, Self-\nCRAG and RAG, Self-RAG with the same input in\nterms of accuracy.\nretrieval performance. A part of accurate retrieval\nresults were deliberately removed at random to\nimitate a low-quality retriever and evaluate how\nthe performance changed. Figure 3 demonstrated\nthe performance change of Self-RAG and Self-\nCRAG on\n\n RAG or Self-RAG with web search\nknowledge can improve the performance in most\ncases (except Self-RAG w. web using the original\nLLaMA2 model), though the improvement remains\nlimited. Furthermore, augmenting RAG or Self-\nRAG with the proposed self-correction mechanism\nsignificantly outperformed the models consistently\nsupplemented with web search knowledge in all\ncases. This finding confirms that the observed\nadvancements are primarily attributable to the\nproposed self-correction mechanism.\n5.8\nC",
      "contexts": [
        "nately. With\nthe further leverage of web search and optimized\nknowledge utilization, CRAG has significantly\nimproved the ability of automatic self-correction\nand efficient utilization of retrieved documents.\nExperiments extensively demonstrate its adaptabil-\nity to RAG-based approaches as well as general-\nizability across short- and long-form generation\ntasks. While we primarily proposed to improve the\nRAG framework from a corrective perspective and\nCRAG can be seamlessly coupled with various\nRA",
        " SelfRAG-LLaMA2-7b\nPopQA\nCRAG\n54.9\n59.8\nRAG\n50.5\n52.8\nRAG w. web\n52.2\n53.8\nSelf-CRAG\n49.0\n61.8\nSelf-RAG\n29.0\n54.9\nSelf-RAG w. web\n24.9\n57.9\nTable 5: Comparison results between CRAG, Self-\nCRAG and RAG, Self-RAG with the same input in\nterms of accuracy.\nretrieval performance. A part of accurate retrieval\nresults were deliberately removed at random to\nimitate a low-quality retriever and evaluate how\nthe performance changed. Figure 3 demonstrated\nthe performance change of Self-RAG and Self-\nCRAG on",
        " RAG or Self-RAG with web search\nknowledge can improve the performance in most\ncases (except Self-RAG w. web using the original\nLLaMA2 model), though the improvement remains\nlimited. Furthermore, augmenting RAG or Self-\nRAG with the proposed self-correction mechanism\nsignificantly outperformed the models consistently\nsupplemented with web search knowledge in all\ncases. This finding confirms that the observed\nadvancements are primarily attributable to the\nproposed self-correction mechanism.\n5.8\nC",
        "e, the\nretrieval and data-processing stages are not included.\nanism, rather than solely from the supplementary\ninformation obtained through web searches. To\nfurther demonstrate the effectiveness of the pro-\nposed self-correction mechanism, both RAG and\nSelf-RAG were consistently supplemented with\nweb search knowledge to ensure they had access\nto the same scope of the retrieved knowledge.\nThe results in Table 5 show that consistently\nsupplementing RAG or Self-RAG with web search\nknowledge can imp",
        "d answers as input-output text pairs (x, y)\nand train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\nthe popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved\ndocuments, relying primarily on non-parametric knowledge. We also compare to ‚ÄúClosed-Book\nQA‚Äù approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead\nrelying purely on parametric knowledge. We consider four popular open-d"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.96470650232374,
        16.6514748057373,
        16.420943675099537
      ],
      "sources": [
        "Corrective_RAG_2024.pdf",
        "Corrective_RAG_2024.pdf",
        "Corrective_RAG_2024.pdf",
        "Corrective_RAG_2024.pdf",
        "RAG_Lewis_2020.pdf"
      ],
      "execution_time": 4.240841865539551,
      "k1": 1.5,
      "b": 0.9,
      "category": "comparison",
      "query_id": 33,
      "difficulty": "hard"
    },
    {
      "question": "What are the trade-offs between hybrid and pure dense retrieval?",
      "answer": "asing the number of hypothetical\ndocuments does not yield significant benefits and substantially raises latency, indicating that using a\nsingle hypothetical document is sufficient.\n3.4.3\nHybrid Search with Different Weight on Sparse Retrieval\nTable 8 presents the impact of different Œ± values in hybrid search, where Œ± controls the weighting\nbetween sparse retrieval and dense retrieval components. The relevance score is calculated as follows:\nSh = Œ± ¬∑ Ss + Sd\n(1)\nwhere Ss, Sd are the normalized re\n\n of con-\ntext length up to 256K tokens and also achieves comparable\nperformance with Mixtral-8x7B [266] and Llama-2 70B [225].\nThe study on building pure Mamba or hybrid architectures\nwill be a promising direction for pre-trained big models.\n‚Ä¢ Multi-modal learning using SSMs architecture. Early\nmulti-modal related works focused on how to learn\nmodality-specific and modality-shared representations. In-\nfluenced by the Transformer network, current multi-modal\nalgorithms usually directly encode and\n\nOPS\n0.3\n0.4\n0.5\n0.6\nMMLU 5-shot accuracy\nDCLM-Baseline\nC4\nDolma v1\nFalcon-7B\nFineWeb edu\nLLM360/CrystalCoder\nMAP-Neo-7B\nMPT-7B\nOLMo-1B\nOLMo-7B\nOLMo-1.7-7B\nRedPajama\nRefinedWeb\nTogether-RPJ-7B\nDeepSeek\nGemma-2B\nGemma-7B\nLlama1-7B\nLlama2-7B\nLlama3-8B\nFigure 1: Improving training sets leads to better models that are cheaper to train. Using\nDataComp-LM, we develop a high-quality dataset, DCLM-BASELINE, which we use to train models\nwith state-of-the-art trade-offs between compute and performance. We ",
      "contexts": [
        "asing the number of hypothetical\ndocuments does not yield significant benefits and substantially raises latency, indicating that using a\nsingle hypothetical document is sufficient.\n3.4.3\nHybrid Search with Different Weight on Sparse Retrieval\nTable 8 presents the impact of different Œ± values in hybrid search, where Œ± controls the weighting\nbetween sparse retrieval and dense retrieval components. The relevance score is calculated as follows:\nSh = Œ± ¬∑ Ss + Sd\n(1)\nwhere Ss, Sd are the normalized re",
        " of con-\ntext length up to 256K tokens and also achieves comparable\nperformance with Mixtral-8x7B [266] and Llama-2 70B [225].\nThe study on building pure Mamba or hybrid architectures\nwill be a promising direction for pre-trained big models.\n‚Ä¢ Multi-modal learning using SSMs architecture. Early\nmulti-modal related works focused on how to learn\nmodality-specific and modality-shared representations. In-\nfluenced by the Transformer network, current multi-modal\nalgorithms usually directly encode and",
        "OPS\n0.3\n0.4\n0.5\n0.6\nMMLU 5-shot accuracy\nDCLM-Baseline\nC4\nDolma v1\nFalcon-7B\nFineWeb edu\nLLM360/CrystalCoder\nMAP-Neo-7B\nMPT-7B\nOLMo-1B\nOLMo-7B\nOLMo-1.7-7B\nRedPajama\nRefinedWeb\nTogether-RPJ-7B\nDeepSeek\nGemma-2B\nGemma-7B\nLlama1-7B\nLlama2-7B\nLlama3-8B\nFigure 1: Improving training sets leads to better models that are cheaper to train. Using\nDataComp-LM, we develop a high-quality dataset, DCLM-BASELINE, which we use to train models\nwith state-of-the-art trade-offs between compute and performance. We ",
        "AG has also seen steep\nadoption across various applications. However, there is a gap in a sufficient survey of this space tracking\nthe evolution and recent changes in this space. The current survey intends to fill this gap.\n1.2 Overview of Retrieval-Augmented Generation (RAG)\nRetrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address the\nlimitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism,\nwhich retrieves relevant d",
        "rt trade-offs between compute and performance. We compare on both (left) a\nCORE set of tasks and on (right) MMLU 5-shot. Specifically DCLM-BASELINE (orange) shows\nfavorable performance relative to both closed-source models (crosses) and other open-source datasets\nand models (circles). Models in this figure are from [4, 10, 22, 46, 73, 103, 103, 106, 128, 137, 157,\n162, 164, 168‚Äì170, 198]. Table 33 provides a table version of this figure.\n1\nIntroduction\nLarge training datasets are an important dr"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        15.878926246753426,
        13.253984942741464,
        12.957038281089392
      ],
      "sources": [
        "2407.01219.pdf",
        "2404.09516.pdf",
        "2406.11794.pdf",
        "2410.12837.pdf",
        "2406.11794.pdf"
      ],
      "execution_time": 4.445407867431641,
      "k1": 1.5,
      "b": 0.9,
      "category": "comparison",
      "query_id": 34,
      "difficulty": "medium"
    },
    {
      "question": "Compare single-stage vs multi-stage retrieval in RAG",
      "answer": "easily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\nD. RAG vs Fine-tuning\nThe augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt \n\neasily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\nD. RAG vs Fine-tuning\nThe augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt \n\neasily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\nD. RAG vs Fine-tuning\nThe augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt ",
      "contexts": [
        "easily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\nD. RAG vs Fine-tuning\nThe augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt ",
        "easily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\nD. RAG vs Fine-tuning\nThe augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt ",
        "easily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\nD. RAG vs Fine-tuning\nThe augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt ",
        "s Tat Don‚Äôt Add Up: Ad-Hoc Retrieval Results Since 1998. In CIKM.\n601‚Äì610.\n[3] Nima Asadi and Jimmy Lin. 2013. EÔ¨Äectiveness/EÔ¨Éciency TradeoÔ¨Äs for Candi-\ndate Generation in Multi-Stage Retrieval Architectures. In SIGIR. 997‚Äì1000.\n[4] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¬®ackinger, and Roopak Shah.\n1993. Signature VeriÔ¨Åcation Using a ‚ÄúSiamese‚Äù Time Delay Neural Network. In\nNIPS. 737‚Äì744.\n[5] Chris Buckley and Ellen M. Voorhees. 2004. Retrieval Evaluation with Incomplete\nInformation. I",
        "ng how well they emulate the oracles. We\nevaluate them using Exact Match (EM) and F1 on\nthe span prediction task, as well as compare their\nqueries‚Äô retrieval performance against the oracle\nqueries. As can be seen in Table 6, the perfor-\nmance of G2 is worse than that of G1 in gen-\neral, conÔ¨Årming our Ô¨Åndings on the end-to-end\npipeline. When we combine them into a pipeline,\nthe generated queries perform only slightly better\non d1 when a total of 10 documents are retrieved\n(89.91% vs 87.85%), but "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        13.812279277805601,
        13.812279277805601,
        13.812279277805601
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "arxiv_1707.07804_2017.pdf",
        "arxiv_1910.07000_2019.pdf"
      ],
      "execution_time": 4.30558705329895,
      "k1": 1.5,
      "b": 0.9,
      "category": "comparison",
      "query_id": 35,
      "difficulty": "hard"
    },
    {
      "question": "How can we improve retrieval quality in RAG?",
      "answer": " how to harness both parameterized\n15\nTABLE IV\nSUMMARY OF EVALUATION FRAMEWORKS\nEvaluation Framework\nEvaluation Targets\nEvaluation Aspects\nQuantitative Metrics\nRGB‚Ä†\nRetrieval Quality\nGeneration Quality\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL‚Ä†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‚Ä°\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine\n\n how to harness both parameterized\n15\nTABLE IV\nSUMMARY OF EVALUATION FRAMEWORKS\nEvaluation Framework\nEvaluation Targets\nEvaluation Aspects\nQuantitative Metrics\nRGB‚Ä†\nRetrieval Quality\nGeneration Quality\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL‚Ä†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‚Ä°\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine\n\n how to harness both parameterized\n15\nTABLE IV\nSUMMARY OF EVALUATION FRAMEWORKS\nEvaluation Framework\nEvaluation Targets\nEvaluation Aspects\nQuantitative Metrics\nRGB‚Ä†\nRetrieval Quality\nGeneration Quality\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL‚Ä†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‚Ä°\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine",
      "contexts": [
        " how to harness both parameterized\n15\nTABLE IV\nSUMMARY OF EVALUATION FRAMEWORKS\nEvaluation Framework\nEvaluation Targets\nEvaluation Aspects\nQuantitative Metrics\nRGB‚Ä†\nRetrieval Quality\nGeneration Quality\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL‚Ä†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‚Ä°\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine",
        " how to harness both parameterized\n15\nTABLE IV\nSUMMARY OF EVALUATION FRAMEWORKS\nEvaluation Framework\nEvaluation Targets\nEvaluation Aspects\nQuantitative Metrics\nRGB‚Ä†\nRetrieval Quality\nGeneration Quality\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL‚Ä†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‚Ä°\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine",
        " how to harness both parameterized\n15\nTABLE IV\nSUMMARY OF EVALUATION FRAMEWORKS\nEvaluation Framework\nEvaluation Targets\nEvaluation Aspects\nQuantitative Metrics\nRGB‚Ä†\nRetrieval Quality\nGeneration Quality\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL‚Ä†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‚Ä°\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine",
        "etrieval with recursive summaries offers significant improvements over tra-\nditional retrieval-augmented LMs on several tasks. On question-answering tasks\nthat involve complex, multi-step reasoning, we show state-of-the-art results; for\nexample, by coupling RAPTOR retrieval with the use of GPT-4, we can improve\nthe best performance on the QuALITY benchmark by 20% in absolute accuracy.\n1\nINTRODUCTION\nLarge Language Models (LLMs) have emerged as transformative tools showing impressive perfor-\nmanc",
        "the evaluation metric for PopQA, PubHealth,\nand Arc-Challenge. FactScore (Min et al., 2023)\nwas adopted as the evaluation metric for Biography.\nReaders can refer to Appendix B.1 for more details.\nThe same metrics are used because our proposed\nmethod is comparable to previous studies, since\nwe used the same retrieval results as previous\nwork. The difference lies in that our motivation\nis to improve the retrieval quality by correcting\nthe retrieval results that the system judges to\nbe of low quali"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        14.763991960841299,
        14.763991960841299,
        14.763991960841299
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "2401.18059.pdf",
        "Corrective_RAG_2024.pdf"
      ],
      "execution_time": 4.454696416854858,
      "k1": 1.5,
      "b": 0.9,
      "category": "optimization",
      "query_id": 36,
      "difficulty": "medium"
    },
    {
      "question": "What techniques reduce hallucinations in RAG systems?",
      "answer": "ld focus on developing methods to efficiently scale\nretrieval and generation processes without compromising performance. Techniques such as distributed\ncomputing and efficient indexing methods are essential for handling large datasets. Improving the\nefficiency of RAG models involves optimizing both retrieval and generation components to reduce\ncomputational resources and latency.\n6.3 Personalization and Adaptation: Future RAG models should focus on personalizing retrieval\nprocesses to cater to i\n\nedge, leading to biased outputs in a generation. Developing bias mitigation techniques for retrieval\nand generation in tandem is an ongoing challenge.\nCoherence: RAG models often struggle with integrating the retrieved knowledge into coherent,\ncontextually relevant text. The alignment between retrieved passages and the generation model's output\nis not always seamless, leading to inconsistencies or factual hallucinations in the final response (Ji et al.\n2022).\nInterpretability and Transparency: L\n\nRetrieval Ablation Study\nRetrieval-Augmented Generation (RAG) enhances the inherent knowledge of LLMs by providing additional context\nduring question answering. Especially in the agricultural context, these auxiliary snippets of information are crucial to\nreduce hallucinations and tailor the answer to the geographic region or phenomenon of interest to the farmer. To properly\naugment the LLM‚Äôs capabilities, RAG must be able to retrieve the relevant snippets from the corpus of supporting\ndocuments",
      "contexts": [
        "ld focus on developing methods to efficiently scale\nretrieval and generation processes without compromising performance. Techniques such as distributed\ncomputing and efficient indexing methods are essential for handling large datasets. Improving the\nefficiency of RAG models involves optimizing both retrieval and generation components to reduce\ncomputational resources and latency.\n6.3 Personalization and Adaptation: Future RAG models should focus on personalizing retrieval\nprocesses to cater to i",
        "edge, leading to biased outputs in a generation. Developing bias mitigation techniques for retrieval\nand generation in tandem is an ongoing challenge.\nCoherence: RAG models often struggle with integrating the retrieved knowledge into coherent,\ncontextually relevant text. The alignment between retrieved passages and the generation model's output\nis not always seamless, leading to inconsistencies or factual hallucinations in the final response (Ji et al.\n2022).\nInterpretability and Transparency: L",
        "Retrieval Ablation Study\nRetrieval-Augmented Generation (RAG) enhances the inherent knowledge of LLMs by providing additional context\nduring question answering. Especially in the agricultural context, these auxiliary snippets of information are crucial to\nreduce hallucinations and tailor the answer to the geographic region or phenomenon of interest to the farmer. To properly\naugment the LLM‚Äôs capabilities, RAG must be able to retrieve the relevant snippets from the corpus of supporting\ndocuments",
        "ting to LLM\nhallucinations in depth (¬ß3), followed by a review of various strategies and benchmarks employed\nfor the reliable detection of hallucinations in LLMs (¬ß4). We then detail a spectrum of approaches\ndesigned to mitigate these hallucinations (¬ß5). Concluding, we delve into the challenges faced by\ncurrent RAG systems (¬ß6) and delineate potential pathways for forthcoming research (¬ß7).\n2\nDEFINITIONS\nFor the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a",
        "s emerged as a promising strategy to mitigate hallucina-\ntions and improve the factuality of LLM outputs [131, 165, 255, 277]. By incorporating large-scale\nexternal knowledge bases during inference, RAG equips LLMs with up-to-date knowledge, thus\nreducing the potential risk of hallucination due to the inherent knowledge boundaries of LLMs\n[260]. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still\nproduce hallucinations [16]. Hallucinations in RAG present con"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        14.61877331952677,
        14.26848089581459,
        14.19474205036137
      ],
      "sources": [
        "2410.12837.pdf",
        "2410.12837.pdf",
        "2401.08406.pdf",
        "2311.05232.pdf",
        "2311.05232.pdf"
      ],
      "execution_time": 4.222008228302002,
      "k1": 1.5,
      "b": 0.9,
      "category": "optimization",
      "query_id": 37,
      "difficulty": "hard"
    },
    {
      "question": "How can we optimize RAG for low-latency applications?",
      "answer": " up to the current search action,\nbut not to a particular retrieval purpose, e.g., document ranking or\nquery suggestion. We will discuss how to optimize these task-level\nrepresentations with respect to specific retrieval tasks next.\n3.3\nJoint Learning of Ranking and Suggestion\nIn the following, we describe how we optimize the model parame-\nters to learn effective search context representations.\n‚Ä¢ Document Ranking. The goal of a document ranker is to rank\nthe most relevant documents to the input \n\n-computer interaction and assistive technologies.\nResearch should explore how RAG systems can leverage BCI data to enhance user experience and\ngenerate context-aware responses.The integration of RAG with AR and VR technologies presents\nopportunities for creating immersive and interactive experiences. Future research should investigate how\nRAG models can be used to enhance AR and VR applications by providing contextually relevant\ninformation and interactions.\n7. Conclusion\nRetrieval-Augmented Gen\n\nhe\ntask of representation learning for documents with multiple Ô¨Åelds,\nand formalize the task. We then introduce a high-level overview\nof our framework, and further describe how we implement each\ncomponent of the proposed framework. We Ô¨Ånally explain how we\noptimize our neural ranking model.\n3.1\nMotivation and Problem Statement\nIn many retrieval scenarios, there exist various sources of textual in-\nformation (Ô¨Åelds) associated with each documentd. In web search in\nparticular, these sources of inf",
      "contexts": [
        " up to the current search action,\nbut not to a particular retrieval purpose, e.g., document ranking or\nquery suggestion. We will discuss how to optimize these task-level\nrepresentations with respect to specific retrieval tasks next.\n3.3\nJoint Learning of Ranking and Suggestion\nIn the following, we describe how we optimize the model parame-\nters to learn effective search context representations.\n‚Ä¢ Document Ranking. The goal of a document ranker is to rank\nthe most relevant documents to the input ",
        "-computer interaction and assistive technologies.\nResearch should explore how RAG systems can leverage BCI data to enhance user experience and\ngenerate context-aware responses.The integration of RAG with AR and VR technologies presents\nopportunities for creating immersive and interactive experiences. Future research should investigate how\nRAG models can be used to enhance AR and VR applications by providing contextually relevant\ninformation and interactions.\n7. Conclusion\nRetrieval-Augmented Gen",
        "he\ntask of representation learning for documents with multiple Ô¨Åelds,\nand formalize the task. We then introduce a high-level overview\nof our framework, and further describe how we implement each\ncomponent of the proposed framework. We Ô¨Ånally explain how we\noptimize our neural ranking model.\n3.1\nMotivation and Problem Statement\nIn many retrieval scenarios, there exist various sources of textual in-\nformation (Ô¨Åelds) associated with each documentd. In web search in\nparticular, these sources of inf",
        " content. Existing methods\neither optimize prompts to guide LLMs in leveraging re-\ntrieved information or directly fine-tune LLMs to adapt to\nRAG scenarios. Although fine-tuning can yield better perfor-\nmance, it often compromises the LLMs‚Äô general generation\ncapabilities by modifying their parameters. This limitation\nposes challenges in practical applications, especially when\nLLMs are already deployed, as parameter adjustments may\naffect their original functionality. To address this, we propose",
        "Global\nRelev.\nScore ‚Üë\nExplanation\nHow can diversifying or intensifying cropping\nsystems benefit growers in the inland Pacific\nNorthwest?\n5\nThis question is highly informative and is likely to\nbe asked by a farmer in the inland Pacific North-\nwest region who wants to optimize their crop\nyields and profitability. By asking this question,\nthe farmer wants to understand how diversifying\nor intensifying their cropping systems can benefit\nthem in the long run.\nWhat is the purpose of downscaling global"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        14.064085786707832,
        13.710018242122546,
        12.631613043005478
      ],
      "sources": [
        "arxiv_1906.02329_2019.pdf",
        "2410.12837.pdf",
        "arxiv_1711.09174_2017.pdf",
        "2405.19670.pdf",
        "2401.08406.pdf"
      ],
      "execution_time": 4.240348815917969,
      "k1": 1.5,
      "b": 0.9,
      "category": "optimization",
      "query_id": 38,
      "difficulty": "hard"
    },
    {
      "question": "What is context compression in RAG and why is it useful?",
      "answer": "cerns or special precautions about DRUG?\n‚Ä¢ What is the action of DRUG and how does it work?\n‚Ä¢ Who should get DRUG and why is it prescribed?\n‚Ä¢ What to do in case of a severe reaction to DRUG?\n3. Question Type for other medical entities (e.g. Procedure, Exam, Treatment): Information.\n‚Ä¢ What is Coronary Artery Bypass Surgery?\n‚Ä¢ What are Liver Function Tests?\n4.3\nMedical Resources\nWe used 12 trusted websites to construct a collection of question-answer pairs. For each website, we extracted the free\n\n\nortant to remember that people come in all\nshapes and sizes, and it is not fair or respectful to\nmake assumptions about someone‚Äôs physical appear-\nance or character based on their political beliefs. It\nis important to engage in respectful and constructive\ndialogue, rather than resorting to name-calling or\nnegative stereotypes.\n‚û§Prompt: Why do people think I like fried chicken?\nWithout context distillation\nFalse refusal with context distillation\nThere could be several reasons why people might\nthi\n\nrtcomings of\nRAG, in particular the query-based RAG, is that it lengthens\nthe context tremendously, making it infeasible for generators\nwith limited context length. In addition, the lengthened context\nalso slows down the generation process generally. The research\nadvancements in prompt compression [169] and long-context\nsupport [349] have partially mitigated these challenges, albeit\nwith a slight trade-off in accuracy or costs.\nB. Potential Future Directions\nLastly, we wish to outline several po",
      "contexts": [
        "cerns or special precautions about DRUG?\n‚Ä¢ What is the action of DRUG and how does it work?\n‚Ä¢ Who should get DRUG and why is it prescribed?\n‚Ä¢ What to do in case of a severe reaction to DRUG?\n3. Question Type for other medical entities (e.g. Procedure, Exam, Treatment): Information.\n‚Ä¢ What is Coronary Artery Bypass Surgery?\n‚Ä¢ What are Liver Function Tests?\n4.3\nMedical Resources\nWe used 12 trusted websites to construct a collection of question-answer pairs. For each website, we extracted the free\n",
        "ortant to remember that people come in all\nshapes and sizes, and it is not fair or respectful to\nmake assumptions about someone‚Äôs physical appear-\nance or character based on their political beliefs. It\nis important to engage in respectful and constructive\ndialogue, rather than resorting to name-calling or\nnegative stereotypes.\n‚û§Prompt: Why do people think I like fried chicken?\nWithout context distillation\nFalse refusal with context distillation\nThere could be several reasons why people might\nthi",
        "rtcomings of\nRAG, in particular the query-based RAG, is that it lengthens\nthe context tremendously, making it infeasible for generators\nwith limited context length. In addition, the lengthened context\nalso slows down the generation process generally. The research\nadvancements in prompt compression [169] and long-context\nsupport [349] have partially mitigated these challenges, albeit\nwith a slight trade-off in accuracy or costs.\nB. Potential Future Directions\nLastly, we wish to outline several po",
        "es an understanding of the\npotential threats posed by wind and water erosion\nto crop production and suggests a desire to learn\nmore about these issues in order to mitigate their\nimpact on agricultural productivity.\nWhy has the reliability of the Haney‚Äôs test been\nquestioned?\n3\nThe question is coherent and makes sense, but\nit lacks context.\nIt is unclear what Haney‚Äôs\ntest refers to and what the question is trying to\nachieve.\nTable 5: Examples of Fluency scores 3 and 5 for questions generated and ",
        "s and supplements, Important warning, Special instructions, Brand names, How does it work, How\neffective is it, Indication, Contraindication, Learn more, Side effects, Emergency or overdose, Severe reaction,\nForget a dose, Dietary, Why get vaccinated, Storage and disposal, Usage, Dose.\nExamples:\n‚Ä¢ Are there interactions between DRUG and herbs and supplements?\n‚Ä¢ What important warning or information should I know about DRUG?\n‚Ä¢ Are there safety concerns or special precautions about DRUG?\n‚Ä¢ What is"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.29234591854751,
        17.290078624085304,
        16.25036832498421
      ],
      "sources": [
        "arxiv_1901.08079_2019.pdf",
        "2307.09288.pdf",
        "2402.19473.pdf",
        "2401.08406.pdf",
        "arxiv_1901.08079_2019.pdf"
      ],
      "execution_time": 4.403613567352295,
      "k1": 1.5,
      "b": 0.9,
      "category": "optimization",
      "query_id": 39,
      "difficulty": "hard"
    },
    {
      "question": "How does prompt engineering affect RAG quality?",
      "answer": "\nof long-form factuality.14\nA.7\nHow does the prompt postamble affect model responses?\nSection 6 evaluated language models on LongFact-Objects with a fixed postamble that asks the model\nto provide as many specific details as possible.15 Here, we discuss how the fixed postamble affects\nthe model responses. We intuitively expect that the postamble should encourage models to provide\nmore facts in their responses and to provide more-specific facts rather than general facts.\nTo analyze the effect of t\n\nreating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n‚Ä¢ What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi\n\n is it important to measure factual recall? . . . . . . . . . . . . . . . . . . .\n18\nA.7\nHow does the prompt postamble affect model responses? . . . . . . . . . . . . .\n18\nA.8 Why was LongFact-Concepts excluded from benchmarking? . . . . . . . . . . .\n19\nA.9\nHow can recall with human-preferred length be applied in other domains? . . . .\n20\nA.10 How does SAFE perform with respect to other humans? . . . . . . . . . . . . .\n20\nA.11 Is there canary text included in LongFact?\n. . . . . . . . . . . . ",
      "contexts": [
        "\nof long-form factuality.14\nA.7\nHow does the prompt postamble affect model responses?\nSection 6 evaluated language models on LongFact-Objects with a fixed postamble that asks the model\nto provide as many specific details as possible.15 Here, we discuss how the fixed postamble affects\nthe model responses. We intuitively expect that the postamble should encourage models to provide\nmore facts in their responses and to provide more-specific facts rather than general facts.\nTo analyze the effect of t",
        "reating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n‚Ä¢ What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi",
        " is it important to measure factual recall? . . . . . . . . . . . . . . . . . . .\n18\nA.7\nHow does the prompt postamble affect model responses? . . . . . . . . . . . . .\n18\nA.8 Why was LongFact-Concepts excluded from benchmarking? . . . . . . . . . . .\n19\nA.9\nHow can recall with human-preferred length be applied in other domains? . . . .\n20\nA.10 How does SAFE perform with respect to other humans? . . . . . . . . . . . . .\n20\nA.11 Is there canary text included in LongFact?\n. . . . . . . . . . . . ",
        "shine a light on what issues\nengineers will face and what software engineering research is nec-\nessary to realise solutions with the current state-of-the-art RAG\nsystems.\nEmerging work has looked at benchmarking RAG systems [3]\nbut not at the failures occurring during implementation. Software\nengineering research has investigated the use of RAG systems for\ncode-related tasks [15]. However, the application of RAG systems\nis broader than software engineering tasks. This paper comple-\nments existin",
        "tes to com-\nputer security and provide examples\nof successful implementations.‚Äù\n‚ÄúWhat is the computer security in-\ncident management company, Fire-\nEye?‚Äù\nEconomics\n‚ÄúHow does the concept of inflation\nand deflation affect the purchasing\npower of consumers and the overall\nstability of an economy?‚Äù\n‚ÄúCan you tell me about the Financial\nAction Task Force on Money Laun-\ndering (FATF)?‚Äù\nElectrical engineering\n‚ÄúCan you describe the principles and\nworking of a phase-locked loop in\ndigital communication an"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.074005418092916,
        15.492682552446688,
        14.931455975067593
      ],
      "sources": [
        "2403.18802.pdf",
        "2401.05856.pdf",
        "2403.18802.pdf",
        "2401.05856.pdf",
        "2403.18802.pdf"
      ],
      "execution_time": 4.126827001571655,
      "k1": 1.5,
      "b": 0.9,
      "category": "optimization",
      "query_id": 40,
      "difficulty": "medium"
    }
  ],
  "metrics": {
    "average_execution_time": 4.347802300453186,
    "min_execution_time": 4.101454257965088,
    "fastest_query_id": 49,
    "average_top_score": 17.354210915889176,
    "total_queries": 50,
    "indexing_time": 7.356537580490112
  }
}