{
  "system_name": "BM25 RAG",
  "total_chunks": 21430,
  "chunk_size": 500,
  "chunk_overlap": 50,
  "k1": 2.0,
  "b": 0.9,
  "llm_model": "fallback (без LLM)",
  "queries": [
    {
      "question": "What is Retrieval-Augmented Generation (RAG)?",
      "answer": "h.\n4.4\nOpen-domain QA Evaluation on\nRetrieval-Augmented Language Models\nAnother aspect of the choice of granularity lies\nin what units should be used in the prompt for\nretrieval-augmented language models. For large\nlanguage models, retrieval-augmented generation\nis achieved by prepending retrieved units to user in-\nstruction and taking them as the input for language\nmodels. We aim to understand the implications of\nusing retrieved units of different granularity within\nthe same computational budge\n\nSPRING is adaptable to different retrievers and various\nnumbers of retrieved results. Consequently, there is no need\nto retrain SPRING with each update to the retrieval system,\nenhancing its practicality and efficiency.\nRelated Work\nRetrieval-Augmented Generation\nCompared to standard\ntext generation, retrieval-augmented generation (RAG) in-\ncorporates a retrieval module that accesses external knowl-\nedge to enhance generation quality (Lewis et al. 2020; Guu\net al. 2020; Zhu et al. 2023; Jin et a\n\n a few words.\nQuestion: when was the last time anyone was on the moon\nThe answer is\nFigure 9: Prompt for retrieval-augmented generation of open-domain QA for the LLaMA-2-7B model.\n19\n",
      "contexts": [
        "h.\n4.4\nOpen-domain QA Evaluation on\nRetrieval-Augmented Language Models\nAnother aspect of the choice of granularity lies\nin what units should be used in the prompt for\nretrieval-augmented language models. For large\nlanguage models, retrieval-augmented generation\nis achieved by prepending retrieved units to user in-\nstruction and taking them as the input for language\nmodels. We aim to understand the implications of\nusing retrieved units of different granularity within\nthe same computational budge",
        "SPRING is adaptable to different retrievers and various\nnumbers of retrieved results. Consequently, there is no need\nto retrain SPRING with each update to the retrieval system,\nenhancing its practicality and efficiency.\nRelated Work\nRetrieval-Augmented Generation\nCompared to standard\ntext generation, retrieval-augmented generation (RAG) in-\ncorporates a retrieval module that accesses external knowl-\nedge to enhance generation quality (Lewis et al. 2020; Guu\net al. 2020; Zhu et al. 2023; Jin et a",
        " a few words.\nQuestion: when was the last time anyone was on the moon\nThe answer is\nFigure 9: Prompt for retrieval-augmented generation of open-domain QA for the LLaMA-2-7B model.\n19\n",
        "et al.\n2022).\nInterpretability and Transparency: Like many AI systems, RAG models are often treated as black\nboxes, with limited transparency in how retrieval influences generation. Improving the interpretability of\nthese models is crucial to fostering trust, especially in critical applications (Roller et al. 2020).\n6. Future Research Directions for Retrieval-Augmented Generation (RAG)\nRetrieval-augmented generation (RAG) represents a significant advancement in natural language\nprocessing and re",
        "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating\nretrieval-augmented\ngeneration\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        15.03761285803035,
        12.964745698079625,
        12.228621927359656
      ],
      "sources": [
        "2312.06648.pdf",
        "2405.19670.pdf",
        "2312.06648.pdf",
        "2410.12837.pdf",
        "2311.09476.pdf"
      ],
      "execution_time": 4.353467702865601,
      "k1": 2.0,
      "b": 0.9,
      "category": "definition",
      "query_id": 1,
      "difficulty": "easy"
    },
    {
      "question": "What are the main components of a RAG system?",
      "answer": " Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n• What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n• A catalogue of failure points (FP) that occur in RAG systems.\n• An experience repo\n\n the three case studies are shown in Table 2.\nWe present our findings for the research question: What are the\nkey considerations when engineering a RAG system? Based on our\ntakeaways we identified multiple potential research areas linked to\nRAG as follows:\n6.1\nChunking and Embeddings\nChunking documents sounds trivial. However, the quality of chunk-\ning affects the retrieval process in many ways and in particular\non the embeddings of the chunk then affects the similarity and\nmatching of chunks to\n\nreating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi",
      "contexts": [
        " Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n• What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n• A catalogue of failure points (FP) that occur in RAG systems.\n• An experience repo",
        " the three case studies are shown in Table 2.\nWe present our findings for the research question: What are the\nkey considerations when engineering a RAG system? Based on our\ntakeaways we identified multiple potential research areas linked to\nRAG as follows:\n6.1\nChunking and Embeddings\nChunking documents sounds trivial. However, the quality of chunk-\ning affects the retrieval process in many ways and in particular\non the embeddings of the chunk then affects the similarity and\nmatching of chunks to",
        "reating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi",
        "ation was more pessimistic than a human rater for this domain.\nHowever, one threat to validity with this finding is that BioASQ is\na domain specific dataset and the reviewers were not experts i.e.\nthe large language model may know more than a non-expert.\n5\nFAILURE POINTS OF RAG SYSTEMS\nFrom the case studies we identified a set of failure points presented\nbelow. The following section addresses the research question What\nare the failure points that occur when engineering a RAG system?\nFP1 Missing ",
        "me about the Economic Cooperation Organization?\nE.6.13\nElectrical Engineering\nCan you provide details about the Eaton Cutler-Hammer C30CNE Lighting Contactor in an electrical\ncircuit system?\nCan you explain what the Keysight 33500B Series waveform generators are used for?\nWhat are the capabilities of the Texas Instruments LM741 Operational Amplifier in signal processing?\nCan you explain the use of the National Instruments CompactRIO controller in control systems and\ndata acquisition?\nCan you des"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        20.701703801449163,
        19.671090063065655,
        19.656349818475114
      ],
      "sources": [
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2403.18802.pdf"
      ],
      "execution_time": 4.510069131851196,
      "k1": 2.0,
      "b": 0.9,
      "category": "definition",
      "query_id": 2,
      "difficulty": "easy"
    },
    {
      "question": "How does RAG differ from traditional LLMs?",
      "answer": "of how recommendation\nsystems can be evaluated. Answer 1, while detailed, is more focused on traditional metrics and lacks the\ninclusion of engagement metrics which are crucial for understanding user interaction.\"\nDiversity: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"Answer 2 not only covers a wide variety of metrics but also includes nuanced explanations\nof how some metrics interrelate and differ from one another, like the inclusion of both RMSE and MSE, as\nwell as the details behind AUC.\n\nept of Universal\nJurisdiction function in international\nlaw and how has it been applied in\nvarious cases across the globe?”\n“Can you share some information\nabout the Convention on the Elimi-\nnation of All Forms of Discrimina-\ntion Against Women (CEDAW)?”\nJurisprudence\n“What is legal positivism and how\ndoes it differ from other legal the-\nories? Can you provide a detailed\nanalysis of its historical evolution,\nkey proponents, and significant con-\ntributions to the field of jurispru-\ndence?”\n“Can y\n\ner will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Infor-\nmation Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of",
      "contexts": [
        "of how recommendation\nsystems can be evaluated. Answer 1, while detailed, is more focused on traditional metrics and lacks the\ninclusion of engagement metrics which are crucial for understanding user interaction.\"\nDiversity: \"Winner\": \"Answer 2 (LightRAG)\",\n\"Explanation\": \"Answer 2 not only covers a wide variety of metrics but also includes nuanced explanations\nof how some metrics interrelate and differ from one another, like the inclusion of both RMSE and MSE, as\nwell as the details behind AUC.",
        "ept of Universal\nJurisdiction function in international\nlaw and how has it been applied in\nvarious cases across the globe?”\n“Can you share some information\nabout the Convention on the Elimi-\nnation of All Forms of Discrimina-\ntion Against Women (CEDAW)?”\nJurisprudence\n“What is legal positivism and how\ndoes it differ from other legal the-\nories? Can you provide a detailed\nanalysis of its historical evolution,\nkey proponents, and significant con-\ntributions to the field of jurispru-\ndence?”\n“Can y",
        "er will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Infor-\nmation Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of",
        "er will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Infor-\nmation Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of",
        "er will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Infor-\nmation Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.8943462919901,
        15.496579506676058,
        13.815316060821619
      ],
      "sources": [
        "2410.05779.pdf",
        "2403.18802.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf"
      ],
      "execution_time": 4.201245069503784,
      "k1": 2.0,
      "b": 0.9,
      "category": "definition",
      "query_id": 3,
      "difficulty": "medium"
    },
    {
      "question": "What is the purpose of the retrieval component in RAG?",
      "answer": "f each retrieval granularity fails to provide the correct\nanswer. The underlined text is the correct answer. The gray text is the context of propositions, but it is for illustration\npurpose only and not provided to the retrievers and downstream QA models.\nPassage Retrieval\nSentence Retrieval\nProposition Retrieval\nQ3: What is the function of the pericardial sac?\nTitle: Pericardium\n✓\nThe pericardium, also called pericardial sac\n... It separates the heart from interference of\nother structures, prot\n\nnd medicine questions:\n1. What does the EB-5 Immigrant Investor Program under US Immigration Law entail?\n2. Can you provide some information about the UN Convention on the Law of the Sea?\n3. What is the purpose of the I-765 Application for Employment Authorization in US Immigration\nLaw?\n4. What is the Orphan Drug Act?\n43\n5. What is the role of the protein Fibroblast Growth Factor 21 in regulating metabolism?\n6. What are the primary functions of the neuromodulatory system in the human brainstem?\n\n\nnltk/nltk/blob/develop/LICENSE.txt\n18https://github.com/Mimino666/langdetect/blob/master/LICENSE\n80\nU\nDatasheet\nU.1\nMotivation\nQ1 For what purpose was the dataset created? Was there a specific task in mind? Was there\na specific gap that needed to be filled? Please provide a description.\n• The purpose of DCLM and the associated DCLM-POOL and DCLM-BASELINE\ndatasets are to enable the study of what makes a strong pretraining dataset for large\nlanguage models. These models are transformative to socie",
      "contexts": [
        "f each retrieval granularity fails to provide the correct\nanswer. The underlined text is the correct answer. The gray text is the context of propositions, but it is for illustration\npurpose only and not provided to the retrievers and downstream QA models.\nPassage Retrieval\nSentence Retrieval\nProposition Retrieval\nQ3: What is the function of the pericardial sac?\nTitle: Pericardium\n✓\nThe pericardium, also called pericardial sac\n... It separates the heart from interference of\nother structures, prot",
        "nd medicine questions:\n1. What does the EB-5 Immigrant Investor Program under US Immigration Law entail?\n2. Can you provide some information about the UN Convention on the Law of the Sea?\n3. What is the purpose of the I-765 Application for Employment Authorization in US Immigration\nLaw?\n4. What is the Orphan Drug Act?\n43\n5. What is the role of the protein Fibroblast Growth Factor 21 in regulating metabolism?\n6. What are the primary functions of the neuromodulatory system in the human brainstem?\n",
        "nltk/nltk/blob/develop/LICENSE.txt\n18https://github.com/Mimino666/langdetect/blob/master/LICENSE\n80\nU\nDatasheet\nU.1\nMotivation\nQ1 For what purpose was the dataset created? Was there a specific task in mind? Was there\na specific gap that needed to be filled? Please provide a description.\n• The purpose of DCLM and the associated DCLM-POOL and DCLM-BASELINE\ndatasets are to enable the study of what makes a strong pretraining dataset for large\nlanguage models. These models are transformative to socie",
        " where 5 means the answer has perfect relevance. An example is\nprovided in Table 7.\n• Groundedness: The metric defines weather the answer follows logically from the information contained in\nthe context or not and provides and integer score to determine how grounded the answer is. An example is\nprovided in Table 8.\n17\nQuestion\nRelevance\nScore ↑\nAnswer\nWhat is the purpose of collecting soil compaction\ndata?\n5\nThe purpose of collecting soil compaction data\nis to determine the extent of soil compact",
        "h a collection of pertinent doc-\numents sourced from an external knowledge database, incorporating two essential elements: the\nRetrieval Component and the Generation Component. 1) The retrieval component is responsible\nfor fetching relevant documents or information from the external knowledge database. It identifies and\nretrieves the most pertinent data based on the input query. 2) After the retrieval process, the generation\ncomponent takes the retrieved information and generates coherent, conte"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.29554677416137,
        16.902037812044554,
        16.545905944973267
      ],
      "sources": [
        "2312.06648.pdf",
        "2403.18802.pdf",
        "2406.11794.pdf",
        "2401.08406.pdf",
        "2410.05779.pdf"
      ],
      "execution_time": 4.487224578857422,
      "k1": 2.0,
      "b": 0.9,
      "category": "definition",
      "query_id": 4,
      "difficulty": "easy"
    },
    {
      "question": "Explain the concept of grounding in RAG systems",
      "answer": "g misinformation or providing incorrect advice (Ji et al. 2022).\nRAG models directly address these concerns by grounding their generative process in external,\nup-to-date knowledge sources. This grounding improves the factual accuracy of the output and enhances\nthe relevance of responses by incorporating real-world data that is directly tied to the query. Additionally,\nRAG models are less likely to propagate biases present in static training data, as they can retrieve more\ndiverse and balanced in\n\nme about the Economic Cooperation Organization?\nE.6.13\nElectrical Engineering\nCan you provide details about the Eaton Cutler-Hammer C30CNE Lighting Contactor in an electrical\ncircuit system?\nCan you explain what the Keysight 33500B Series waveform generators are used for?\nWhat are the capabilities of the Texas Instruments LM741 Operational Amplifier in signal processing?\nCan you explain the use of the National Instruments CompactRIO controller in control systems and\ndata acquisition?\nCan you des\n\necoder\nas a condition. Grounding Decoder receives both the 3D\npatch features and the obtained location token embeddings\nas inputs and predicts the 3D visual grounding results.\nC.2. Grounding Decoder Details\nHere illustrate more architectural and training objective de-\ntails about the grounding decoder. Our grounding decoder\nconsists of L = 4 decoder layers, as illustrated in Fig. 6.\nFor query initialization, we employ farthest point sampling\nto select N = 512 instance queries from the 3D patches",
      "contexts": [
        "g misinformation or providing incorrect advice (Ji et al. 2022).\nRAG models directly address these concerns by grounding their generative process in external,\nup-to-date knowledge sources. This grounding improves the factual accuracy of the output and enhances\nthe relevance of responses by incorporating real-world data that is directly tied to the query. Additionally,\nRAG models are less likely to propagate biases present in static training data, as they can retrieve more\ndiverse and balanced in",
        "me about the Economic Cooperation Organization?\nE.6.13\nElectrical Engineering\nCan you provide details about the Eaton Cutler-Hammer C30CNE Lighting Contactor in an electrical\ncircuit system?\nCan you explain what the Keysight 33500B Series waveform generators are used for?\nWhat are the capabilities of the Texas Instruments LM741 Operational Amplifier in signal processing?\nCan you explain the use of the National Instruments CompactRIO controller in control systems and\ndata acquisition?\nCan you des",
        "ecoder\nas a condition. Grounding Decoder receives both the 3D\npatch features and the obtained location token embeddings\nas inputs and predicts the 3D visual grounding results.\nC.2. Grounding Decoder Details\nHere illustrate more architectural and training objective de-\ntails about the grounding decoder. Our grounding decoder\nconsists of L = 4 decoder layers, as illustrated in Fig. 6.\nFor query initialization, we employ farthest point sampling\nto select N = 512 instance queries from the 3D patches",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        15.757557637198014,
        14.142159466450167,
        14.103831107928514
      ],
      "sources": [
        "2410.12837.pdf",
        "2403.18802.pdf",
        "2409.18125.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "execution_time": 4.479430675506592,
      "k1": 2.0,
      "b": 0.9,
      "category": "definition",
      "query_id": 5,
      "difficulty": "medium"
    },
    {
      "question": "How does the retrieval mechanism work in RAG systems?",
      "answer": "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: • (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? • (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? • (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? •\n(RQ4): What are the costs associated w\n\ne the RAG system and their use\ncases, which guide the generation of corpus-specific sensemaking queries.\n2.4\nRAG evaluation criteria\nOur evaluation relies on the LLM to evaluate how well the RAG system answers the generated ques-\ntions. Prior work has shown LLMs to be good evaluators of natural language generation, includ-\ning work where LLMs evaluations were competitive with human evaluations (Wang et al., 2023a;\nZheng et al., 2024). Some prior work proposes criteria for having LLMs quantify th\n\nreating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi",
      "contexts": [
        "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: • (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? • (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? • (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? •\n(RQ4): What are the costs associated w",
        "e the RAG system and their use\ncases, which guide the generation of corpus-specific sensemaking queries.\n2.4\nRAG evaluation criteria\nOur evaluation relies on the LLM to evaluate how well the RAG system answers the generated ques-\ntions. Prior work has shown LLMs to be good evaluators of natural language generation, includ-\ning work where LLMs evaluations were competitive with human evaluations (Wang et al., 2023a;\nZheng et al., 2024). Some prior work proposes criteria for having LLMs quantify th",
        "reating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi",
        " chemist Paul L. Modrich.\nWho is Gertrude B. Elion and how has her groundbreaking work on the synthesis of medical drugs\nfrom regular compounds influenced pharmaceutical chemistry?\nWho is Glenn T. Seaborg and how did his discovery of plutonium and other transuranic elements\nchange the study of heavy elements in chemistry?\nWho is Richard J. Roberts and how did his discovery of introns in eukaryotic DNA and the mechanism\nof gene-splicing revolutionize molecular biology and chemistry?\nWho is Kary M",
        "ing user\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\nintegrate different retrieval methods to achieve optimal performance for diverse tasks.\n6.7 Integration with Emerging Technologies: Integrating RAG models with brain-computer interfaces\n(BCIs) could lead to novel applications in human-computer interaction and assistive technologies.\n"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.989288763053285,
        15.011273783288562,
        14.540273210238272
      ],
      "sources": [
        "2410.05779.pdf",
        "2404.16130.pdf",
        "2401.05856.pdf",
        "2403.18802.pdf",
        "2410.12837.pdf"
      ],
      "execution_time": 4.479373931884766,
      "k1": 2.0,
      "b": 0.9,
      "category": "technical",
      "query_id": 6,
      "difficulty": "medium"
    },
    {
      "question": "What is the role of embeddings in RAG?",
      "answer": "ngs and what was his role at Netflix?\nWhat can you tell me about Elon Musk’s role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson’s tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association’s \"Ice Bucket Challenge\" marketing\ncampaign?\nWhat are the specifics of IBM’s \"Smarter Planet\" marketing c\n\nam?\nWhat is the NLTK library for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you\n\nstein Veblen?\nWho is Vilfredo Pareto?\nE.6.35\nSports\nWhat is the Stanley Cup?\nWhat is the Green Jacket in the Masters Golf Tournament?\nWhat is the Ryder Cup?\nWho is Serena Williams?\nWhat is the America’s Cup?\nE.6.36\nUS Foreign Policy\nWho is John Bolton?\nWho is Zalmay Khalilzad?\n64\nHow is the United States related to the East Asia Summit (EAS)?\nWho is Richard N. Haass?\nWhat was the United States’ role in the Kyoto Protocol?\nE.6.37\nVirology\nWho is Thomas Wellems?\nWhat is the Avian influenza A(H7N9)",
      "contexts": [
        "ngs and what was his role at Netflix?\nWhat can you tell me about Elon Musk’s role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson’s tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association’s \"Ice Bucket Challenge\" marketing\ncampaign?\nWhat are the specifics of IBM’s \"Smarter Planet\" marketing c",
        "am?\nWhat is the NLTK library for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you",
        "stein Veblen?\nWho is Vilfredo Pareto?\nE.6.35\nSports\nWhat is the Stanley Cup?\nWhat is the Green Jacket in the Masters Golf Tournament?\nWhat is the Ryder Cup?\nWho is Serena Williams?\nWhat is the America’s Cup?\nE.6.36\nUS Foreign Policy\nWho is John Bolton?\nWho is Zalmay Khalilzad?\n64\nHow is the United States related to the East Asia Summit (EAS)?\nWho is Richard N. Haass?\nWhat was the United States’ role in the Kyoto Protocol?\nE.6.37\nVirology\nWho is Thomas Wellems?\nWhat is the Avian influenza A(H7N9)",
        "nd medicine questions:\n1. What does the EB-5 Immigrant Investor Program under US Immigration Law entail?\n2. Can you provide some information about the UN Convention on the Law of the Sea?\n3. What is the purpose of the I-765 Application for Employment Authorization in US Immigration\nLaw?\n4. What is the Orphan Drug Act?\n43\n5. What is the role of the protein Fibroblast Growth Factor 21 in regulating metabolism?\n6. What are the primary functions of the neuromodulatory system in the human brainstem?\n",
        "uring the Samsung’s Galaxy Note 7 crisis?\nWhat role did the public relations firm, Bell Pottinger, play in the South African Gupta family\nscandal?\nWhat do you know about FleishmanHillard International Communications?\nWhat actions did PepsiCo’s public relations team take in response to the backlash for the Kendall\nJenner ad in 2017?\nE.6.34\nSociology\nWho is Peter L. Berger?\nWho is Anthony Giddens?\nWho is Norbert Elias?\nWho is Max Weber?\nWho is Thorstein Veblen?\nWho is Vilfredo Pareto?\nE.6.35\nSport"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        19.267821834998582,
        18.207120451375918,
        15.676903217276074
      ],
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "execution_time": 4.4939963817596436,
      "k1": 2.0,
      "b": 0.9,
      "category": "technical",
      "query_id": 7,
      "difficulty": "medium"
    },
    {
      "question": "Explain how dense retrieval works in RAG",
      "answer": "ing user\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\nintegrate different retrieval methods to achieve optimal performance for diverse tasks.\n6.7 Integration with Emerging Technologies: Integrating RAG models with brain-computer interfaces\n(BCIs) could lead to novel applications in human-computer interaction and assistive technologies.\n\n\n In addition to sparse retrieval and dense re-\ntrieval, there are alternative methods for retrieving relevant\nobjects [77], [78]. Instead of calculating representations, some\nresearch works directly use the edit distance between natural\nlanguage texts [79] or abstract syntax trees (AST) of code snip-\npets [80], [81]. In knowledge graphs, entities are connected\nby relations, serving as a pre-built index for retrieval. Thus,\nRAG methods utilizing knowledge graphs can employ k-hop\nneighbor searches\n\n2021; Gao and Callan, 2022; Liu\nand Shao, 2022) have been proposed to improve the\neffectiveness of supervised dense retrieval models.\nOn the other hand, zero-shot dense retrieval still\nremains difﬁcult. Many recent works consider the\nalternative transfer learning setup, where the dense\nretrievers are trained on a high-resource dataset and\nthen evaluated on queries from new tasks. The MS-\nMARCO collection (Bajaj et al., 2016), a massive\njudged dataset with a large number of judged query-\ndocument",
      "contexts": [
        "ing user\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\nintegrate different retrieval methods to achieve optimal performance for diverse tasks.\n6.7 Integration with Emerging Technologies: Integrating RAG models with brain-computer interfaces\n(BCIs) could lead to novel applications in human-computer interaction and assistive technologies.\n",
        " In addition to sparse retrieval and dense re-\ntrieval, there are alternative methods for retrieving relevant\nobjects [77], [78]. Instead of calculating representations, some\nresearch works directly use the edit distance between natural\nlanguage texts [79] or abstract syntax trees (AST) of code snip-\npets [80], [81]. In knowledge graphs, entities are connected\nby relations, serving as a pre-built index for retrieval. Thus,\nRAG methods utilizing knowledge graphs can employ k-hop\nneighbor searches",
        "2021; Gao and Callan, 2022; Liu\nand Shao, 2022) have been proposed to improve the\neffectiveness of supervised dense retrieval models.\nOn the other hand, zero-shot dense retrieval still\nremains difﬁcult. Many recent works consider the\nalternative transfer learning setup, where the dense\nretrievers are trained on a high-resource dataset and\nthen evaluated on queries from new tasks. The MS-\nMARCO collection (Bajaj et al., 2016), a massive\njudged dataset with a large number of judged query-\ndocument",
        "ng Long-tail and Real-time Knowledge:\nWhile a key motivation of RAG is to harness real-time and\nlong-tail knowledge, few studies have explored the pipeline\nfor knowledge updating and expansion. Many existing works\nuse merely the generators’ training data as retrieval sources,\nneglecting the dynamic and flexible information that retrieval\ncould offer. As a consequence, there is a growing research on\ndesigning RAG systems with continuously updated knowledge\nand flexible sources. We also expect RAG",
        "he\ntask of representation learning for documents with multiple ﬁelds,\nand formalize the task. We then introduce a high-level overview\nof our framework, and further describe how we implement each\ncomponent of the proposed framework. We ﬁnally explain how we\noptimize our neural ranking model.\n3.1\nMotivation and Problem Statement\nIn many retrieval scenarios, there exist various sources of textual in-\nformation (ﬁelds) associated with each documentd. In web search in\nparticular, these sources of inf"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        18.038267699431962,
        15.107520930146555,
        15.021197468173874
      ],
      "sources": [
        "2410.12837.pdf",
        "2402.19473.pdf",
        "Atlas_2022.pdf",
        "2402.19473.pdf",
        "arxiv_1711.09174_2017.pdf"
      ],
      "execution_time": 4.4182093143463135,
      "k1": 2.0,
      "b": 0.9,
      "category": "technical",
      "query_id": 8,
      "difficulty": "hard"
    },
    {
      "question": "What is the difference between sparse and dense retrieval?",
      "answer": "n should I use an em-dash, an en-dash, and a hyphen?\n4\nWhen to use -, – and —?\n6\nWhat is the difference between `-` and `--`\n5\n55\nDoes “cost-benefit ratio” use a hyphen or an en-dash?\n0\nWhat kind of dash character should I use at the end of a famous saying to mark of the author?\n-1\ndash non-restrictive element in the middle of a sentence\n1\nem dash followed by a comma\n0\nwhat's the difference between a hyphen, a dash and a minus sign?\n0\nUsing comma to delimit the name of a group and its constituen\n\nasing the number of hypothetical\ndocuments does not yield significant benefits and substantially raises latency, indicating that using a\nsingle hypothetical document is sufficient.\n3.4.3\nHybrid Search with Different Weight on Sparse Retrieval\nTable 8 presents the impact of different α values in hybrid search, where α controls the weighting\nbetween sparse retrieval and dense retrieval components. The relevance score is calculated as follows:\nSh = α · Ss + Sd\n(1)\nwhere Ss, Sd are the normalized re\n\nls are better suited for particular use cases.\n1) Mix/hybrid Retrieval : Sparse and dense embedding\napproaches capture different relevance features and can ben-\nefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n6https://github.com/aurelio-labs/semantic-router\n7https://huggingface.co/spaces/mteb/leaderboard\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)",
      "contexts": [
        "n should I use an em-dash, an en-dash, and a hyphen?\n4\nWhen to use -, – and —?\n6\nWhat is the difference between `-` and `--`\n5\n55\nDoes “cost-benefit ratio” use a hyphen or an en-dash?\n0\nWhat kind of dash character should I use at the end of a famous saying to mark of the author?\n-1\ndash non-restrictive element in the middle of a sentence\n1\nem dash followed by a comma\n0\nwhat's the difference between a hyphen, a dash and a minus sign?\n0\nUsing comma to delimit the name of a group and its constituen",
        "asing the number of hypothetical\ndocuments does not yield significant benefits and substantially raises latency, indicating that using a\nsingle hypothetical document is sufficient.\n3.4.3\nHybrid Search with Different Weight on Sparse Retrieval\nTable 8 presents the impact of different α values in hybrid search, where α controls the weighting\nbetween sparse retrieval and dense retrieval components. The relevance score is calculated as follows:\nSh = α · Ss + Sd\n(1)\nwhere Ss, Sd are the normalized re",
        "ls are better suited for particular use cases.\n1) Mix/hybrid Retrieval : Sparse and dense embedding\napproaches capture different relevance features and can ben-\nefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n6https://github.com/aurelio-labs/semantic-router\n7https://huggingface.co/spaces/mteb/leaderboard\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)",
        "ls are better suited for particular use cases.\n1) Mix/hybrid Retrieval : Sparse and dense embedding\napproaches capture different relevance features and can ben-\nefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n6https://github.com/aurelio-labs/semantic-router\n7https://huggingface.co/spaces/mteb/leaderboard\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)",
        "ls are better suited for particular use cases.\n1) Mix/hybrid Retrieval : Sparse and dense embedding\napproaches capture different relevance features and can ben-\nefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n6https://github.com/aurelio-labs/semantic-router\n7https://huggingface.co/spaces/mteb/leaderboard\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.554777518660543,
        15.57745461039904,
        15.065629894198949
      ],
      "sources": [
        "2406.11794.pdf",
        "2407.01219.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf"
      ],
      "execution_time": 4.515317916870117,
      "k1": 2.0,
      "b": 0.9,
      "category": "technical",
      "query_id": 9,
      "difficulty": "medium"
    },
    {
      "question": "How does chunking strategy affect RAG performance?",
      "answer": "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: • (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? • (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? • (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? •\n(RQ4): What are the costs associated w\n\ns, even\nsentences. However, inappropriate retrieval granularity can compromise the semantic integrity\nand affect the relevance of retrieved information [224], thereby affecting the performance of\nLLMs. Fixed-size chunking, which typically breaks down the documents into chunks of a specified\nlength such as 100-word paragraphs, serves as the most crude and prevalent strategy of chunking,\nwhich is widely used in RAG systems [24, 109, 165]. Considering fixed-size chunking falls short in\ncapture stru\n\n, 20, 21]. Post-processing retrieved documents is another method to enhance generator\noutput, with techniques like hierarchical prompt summarization [22] and using abstractive and\nextractive compressors [23] to reduce context length and remove redundancy [24].\n2.2\nRetriever Enhancement Strategy\nDocument chunking and embedding methods significantly impact retrieval performance. Common\nchunking strategies divide documents into chunks, but determining optimal chunk length can be\nchallenging. Small ",
      "contexts": [
        "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: • (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? • (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? • (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? •\n(RQ4): What are the costs associated w",
        "s, even\nsentences. However, inappropriate retrieval granularity can compromise the semantic integrity\nand affect the relevance of retrieved information [224], thereby affecting the performance of\nLLMs. Fixed-size chunking, which typically breaks down the documents into chunks of a specified\nlength such as 100-word paragraphs, serves as the most crude and prevalent strategy of chunking,\nwhich is widely used in RAG systems [24, 109, 165]. Considering fixed-size chunking falls short in\ncapture stru",
        ", 20, 21]. Post-processing retrieved documents is another method to enhance generator\noutput, with techniques like hierarchical prompt summarization [22] and using abstractive and\nextractive compressors [23] to reduce context length and remove redundancy [24].\n2.2\nRetriever Enhancement Strategy\nDocument chunking and embedding methods significantly impact retrieval performance. Common\nchunking strategies divide documents into chunks, but determining optimal chunk length can be\nchallenging. Small ",
        "by-side\ncomparisons.\nExpertise QA\nIn Expertise QA, we engage with in-house experts that have formal training and\nexperience in various domains (e.g., history, literature, psychology). They produced hard, sometimes\ncomplex questions (e.g., As films began using sound for dialogue, how did the changing use of visual\nmetaphor affect the ways audiences expected narratives to develop in movies? or How does Vygotsky’s\ntheory of internal speech apply across multilingual language acquisition?).\nThe same ",
        " the three case studies are shown in Table 2.\nWe present our findings for the research question: What are the\nkey considerations when engineering a RAG system? Based on our\ntakeaways we identified multiple potential research areas linked to\nRAG as follows:\n6.1\nChunking and Embeddings\nChunking documents sounds trivial. However, the quality of chunk-\ning affects the retrieval process in many ways and in particular\non the embeddings of the chunk then affects the similarity and\nmatching of chunks to"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        22.987506382727926,
        20.663444901631316,
        15.362781218284024
      ],
      "sources": [
        "2410.05779.pdf",
        "2311.05232.pdf",
        "2407.01219.pdf",
        "2403.05530.pdf",
        "2401.05856.pdf"
      ],
      "execution_time": 4.1076531410217285,
      "k1": 2.0,
      "b": 0.9,
      "category": "technical",
      "query_id": 10,
      "difficulty": "hard"
    },
    {
      "question": "What is the role of the retriever in RAG architecture?",
      "answer": "ngs and what was his role at Netflix?\nWhat can you tell me about Elon Musk’s role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson’s tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association’s \"Ice Bucket Challenge\" marketing\ncampaign?\nWhat are the specifics of IBM’s \"Smarter Planet\" marketing c\n\nam?\nWhat is the NLTK library for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you\n\nler size.\nThe Internet as a knowledge base\nMore related\nto our work, the search engine can assume the role\nof the retriever and use the Internet as the source of\nInput\nRetriever\nOutput\nDocuments\nInput\nWeb Search\nDocuments\nBlack-box LLM\nQuery\nInput\nDocuments\nQuery\nOutput\nOutput\nReward\nInput:\nWhat profession does Nicholas Ray and \nElia Kazan have in common?\nQuery: Nicholas Ray profession\nNicholas Ray American author and \ndirector, original name Raymond \nNicholas Kienzle, born August 7, \n1911, Gale",
      "contexts": [
        "ngs and what was his role at Netflix?\nWhat can you tell me about Elon Musk’s role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson’s tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association’s \"Ice Bucket Challenge\" marketing\ncampaign?\nWhat are the specifics of IBM’s \"Smarter Planet\" marketing c",
        "am?\nWhat is the NLTK library for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you",
        "ler size.\nThe Internet as a knowledge base\nMore related\nto our work, the search engine can assume the role\nof the retriever and use the Internet as the source of\nInput\nRetriever\nOutput\nDocuments\nInput\nWeb Search\nDocuments\nBlack-box LLM\nQuery\nInput\nDocuments\nQuery\nOutput\nOutput\nReward\nInput:\nWhat profession does Nicholas Ray and \nElia Kazan have in common?\nQuery: Nicholas Ray profession\nNicholas Ray American author and \ndirector, original name Raymond \nNicholas Kienzle, born August 7, \n1911, Gale",
        "stein Veblen?\nWho is Vilfredo Pareto?\nE.6.35\nSports\nWhat is the Stanley Cup?\nWhat is the Green Jacket in the Masters Golf Tournament?\nWhat is the Ryder Cup?\nWho is Serena Williams?\nWhat is the America’s Cup?\nE.6.36\nUS Foreign Policy\nWho is John Bolton?\nWho is Zalmay Khalilzad?\n64\nHow is the United States related to the East Asia Summit (EAS)?\nWho is Richard N. Haass?\nWhat was the United States’ role in the Kyoto Protocol?\nE.6.37\nVirology\nWho is Thomas Wellems?\nWhat is the Avian influenza A(H7N9)",
        "nd medicine questions:\n1. What does the EB-5 Immigrant Investor Program under US Immigration Law entail?\n2. Can you provide some information about the UN Convention on the Law of the Sea?\n3. What is the purpose of the I-765 Application for Employment Authorization in US Immigration\nLaw?\n4. What is the Orphan Drug Act?\n43\n5. What is the role of the protein Fibroblast Growth Factor 21 in regulating metabolism?\n6. What are the primary functions of the neuromodulatory system in the human brainstem?\n"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        19.55865265699029,
        18.627319114400027,
        16.371581563428283
      ],
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2305.14283.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "execution_time": 4.481529474258423,
      "k1": 2.0,
      "b": 0.9,
      "category": "technical",
      "query_id": 41,
      "difficulty": "easy"
    },
    {
      "question": "Explain the concept of semantic search in RAG",
      "answer": "ds.” In: WikiWorkshop@ICWSM. 2016.\n[9]\nJason V. Davis et al. “Information-theoretic metric learning.” In: ICML. 2007.\n[10]\nScott Deerwester et al. “Indexing by latent semantic analysis.” In: Journal of the\nAmerican Society for Information Science 41.6 (1990).\n[11]\nManaal Faruqui et al. “Retroﬁtting Word Vectors to Semantic Lexicons.” In: CoRR\n(2014).\n[12]\nLev Finkelstein et al. “Placing Search in Context: the Concept Revisited.” In:\nWWW. 2001.\n[13]\nEvgeniy Gabrilovich and Shaul Markovitch. “Comp\n\non - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept\n\non - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept",
      "contexts": [
        "ds.” In: WikiWorkshop@ICWSM. 2016.\n[9]\nJason V. Davis et al. “Information-theoretic metric learning.” In: ICML. 2007.\n[10]\nScott Deerwester et al. “Indexing by latent semantic analysis.” In: Journal of the\nAmerican Society for Information Science 41.6 (1990).\n[11]\nManaal Faruqui et al. “Retroﬁtting Word Vectors to Semantic Lexicons.” In: CoRR\n(2014).\n[12]\nLev Finkelstein et al. “Placing Search in Context: the Concept Revisited.” In:\nWWW. 2001.\n[13]\nEvgeniy Gabrilovich and Shaul Markovitch. “Comp",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept",
        "r a versatile RAG\nsystem designed to accommodate diverse scenarios.\nMetadata Router/ Filter. The first step involves extracting\nkeywords (entity) from the query, followed by filtering based\non the keywords and metadata within the chunks to narrow\ndown the search scope.\nSemantic Router is another method of routing involves\nleveraging the semantic information of the query. Specific\napprach see Semantic Router 6. Certainly, a hybrid routing\napproach can also be employed, combining both semantic and"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        14.503584558659448,
        13.820909768594646,
        13.820909768594646
      ],
      "sources": [
        "arxiv_1705.07425_2017.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "2312.10997.pdf"
      ],
      "execution_time": 4.475604057312012,
      "k1": 2.0,
      "b": 0.9,
      "category": "technical",
      "query_id": 42,
      "difficulty": "medium"
    },
    {
      "question": "How do attention mechanisms work in RAG systems?",
      "answer": "tion (Sec. 3.4) and IO-Aware Attention (Sec. 3.5).\n3.1\nLocal Attention\nThe traditional attention mechanism is characterized by its global and full attention nature, wherein every token is\nexpected to attend to every other token, resulting in quadratic time and space complexities. Considering the significance\nof local context in certain applications [246], various approaches have been introduced to implement local attention\nmechanisms in recent years. These mechanisms restrict each token’s attent\n\nhter).\n3.2\nHierarchical Attention\nThe global token techniques [6, 17, 78] and the inter-block attention [198] mentioned above are essentially introducing\nhierarchical features to self-attention to compensate with more global information from the higher-level attention\nwhile keeping the low computation cost from the low-level local attention at the same time. From this view, more work\nhas explored various hierarchical mechanisms that introduce a structured hierarchy into self-attention, leveragin\n\nl proceedings. Furthermore,\nstudies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks,\nincluding document summarization and, knowledge-grounded dialogues\n2.2 Retriever Mechanisms in RAG Systems\nThe retriever in RAG systems is essential for fetching relevant documents from an external corpus.\nEffective retrieval ensures that the model's output is grounded in accurate information. Several retrieval\nmechanisms are commonly used, ranging from traditional m",
      "contexts": [
        "tion (Sec. 3.4) and IO-Aware Attention (Sec. 3.5).\n3.1\nLocal Attention\nThe traditional attention mechanism is characterized by its global and full attention nature, wherein every token is\nexpected to attend to every other token, resulting in quadratic time and space complexities. Considering the significance\nof local context in certain applications [246], various approaches have been introduced to implement local attention\nmechanisms in recent years. These mechanisms restrict each token’s attent",
        "hter).\n3.2\nHierarchical Attention\nThe global token techniques [6, 17, 78] and the inter-block attention [198] mentioned above are essentially introducing\nhierarchical features to self-attention to compensate with more global information from the higher-level attention\nwhile keeping the low computation cost from the low-level local attention at the same time. From this view, more work\nhas explored various hierarchical mechanisms that introduce a structured hierarchy into self-attention, leveragin",
        "l proceedings. Furthermore,\nstudies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks,\nincluding document summarization and, knowledge-grounded dialogues\n2.2 Retriever Mechanisms in RAG Systems\nThe retriever in RAG systems is essential for fetching relevant documents from an external corpus.\nEffective retrieval ensures that the model's output is grounded in accurate information. Several retrieval\nmechanisms are commonly used, ranging from traditional m",
        "t completely exclusive\nmodels. It is also shown that linear self-attention can be\nachieved in theory and practice by learning gated RNNs\nwith multiplicative interactions, bridging the gap between\nthese two architectures. Ali et al. [143] explore the learning\nmechanisms of Mamba models, in particular how depen-\ndencies are captured and their similarity to other established\nlayers, such as RNN, CNN, or attention mechanisms. An\nimportant relationship between the Mamba and the self-\nattention layer ",
        "e the RAG system and their use\ncases, which guide the generation of corpus-specific sensemaking queries.\n2.4\nRAG evaluation criteria\nOur evaluation relies on the LLM to evaluate how well the RAG system answers the generated ques-\ntions. Prior work has shown LLMs to be good evaluators of natural language generation, includ-\ning work where LLMs evaluations were competitive with human evaluations (Wang et al., 2023a;\nZheng et al., 2024). Some prior work proposes criteria for having LLMs quantify th"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.838786194326524,
        16.637876918386482,
        15.460209884089167
      ],
      "sources": [
        "2311.12351.pdf",
        "2311.12351.pdf",
        "2410.12837.pdf",
        "2404.09516.pdf",
        "2404.16130.pdf"
      ],
      "execution_time": 4.405184984207153,
      "k1": 2.0,
      "b": 0.9,
      "category": "technical",
      "query_id": 43,
      "difficulty": "hard"
    },
    {
      "question": "What is the purpose of document encoding in RAG?",
      "answer": "nd medicine questions:\n1. What does the EB-5 Immigrant Investor Program under US Immigration Law entail?\n2. Can you provide some information about the UN Convention on the Law of the Sea?\n3. What is the purpose of the I-765 Application for Employment Authorization in US Immigration\nLaw?\n4. What is the Orphan Drug Act?\n43\n5. What is the role of the protein Fibroblast Growth Factor 21 in regulating metabolism?\n6. What are the primary functions of the neuromodulatory system in the human brainstem?\n\n\nnltk/nltk/blob/develop/LICENSE.txt\n18https://github.com/Mimino666/langdetect/blob/master/LICENSE\n80\nU\nDatasheet\nU.1\nMotivation\nQ1 For what purpose was the dataset created? Was there a specific task in mind? Was there\na specific gap that needed to be filled? Please provide a description.\n• The purpose of DCLM and the associated DCLM-POOL and DCLM-BASELINE\ndatasets are to enable the study of what makes a strong pretraining dataset for large\nlanguage models. These models are transformative to socie\n\n position embedding\ncharacterizes the position of a word in a sequence, while the ordinal\nembedding in SetRank characterizes the rank of a document w.r.t.\na ranking algorithm being applied to a document set. Therefore,\neach word inputted to Transformer has only one position embed-\nding while each document in SetRank may have multiple ordinal\nembeddings.\n4.3\nDocument Encoding with (Induced)\nMulti-head Self Attention Block\nThe key of SetRank is the encoding component which takes the\ndocument repre",
      "contexts": [
        "nd medicine questions:\n1. What does the EB-5 Immigrant Investor Program under US Immigration Law entail?\n2. Can you provide some information about the UN Convention on the Law of the Sea?\n3. What is the purpose of the I-765 Application for Employment Authorization in US Immigration\nLaw?\n4. What is the Orphan Drug Act?\n43\n5. What is the role of the protein Fibroblast Growth Factor 21 in regulating metabolism?\n6. What are the primary functions of the neuromodulatory system in the human brainstem?\n",
        "nltk/nltk/blob/develop/LICENSE.txt\n18https://github.com/Mimino666/langdetect/blob/master/LICENSE\n80\nU\nDatasheet\nU.1\nMotivation\nQ1 For what purpose was the dataset created? Was there a specific task in mind? Was there\na specific gap that needed to be filled? Please provide a description.\n• The purpose of DCLM and the associated DCLM-POOL and DCLM-BASELINE\ndatasets are to enable the study of what makes a strong pretraining dataset for large\nlanguage models. These models are transformative to socie",
        " position embedding\ncharacterizes the position of a word in a sequence, while the ordinal\nembedding in SetRank characterizes the rank of a document w.r.t.\na ranking algorithm being applied to a document set. Therefore,\neach word inputted to Transformer has only one position embed-\nding while each document in SetRank may have multiple ordinal\nembeddings.\n4.3\nDocument Encoding with (Induced)\nMulti-head Self Attention Block\nThe key of SetRank is the encoding component which takes the\ndocument repre",
        "hat can be deﬁned as a document aware query\nFig. 3. Multi-Resolution n-Gram Attention\nFig. 4. Document Aware Query Attention\nencoding of i-th position of a query. The ﬁnal output represen-\ntations of encoder fe(·, ·) is a document aware query encoding\nattention vector qe that is evaluated as:\nqe = [qe1, qe2, · · · , qehq] ∈Rhq\nIn other words, if the document includes more positions of\nmradj that are very much alike to the position of mraqi\nin the query, the document aware query attention compone",
        " where 5 means the answer has perfect relevance. An example is\nprovided in Table 7.\n• Groundedness: The metric defines weather the answer follows logically from the information contained in\nthe context or not and provides and integer score to determine how grounded the answer is. An example is\nprovided in Table 8.\n17\nQuestion\nRelevance\nScore ↑\nAnswer\nWhat is the purpose of collecting soil compaction\ndata?\n5\nThe purpose of collecting soil compaction data\nis to determine the extent of soil compact"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.38403362163592,
        16.1252711851784,
        16.100444289643747
      ],
      "sources": [
        "2403.18802.pdf",
        "2406.11794.pdf",
        "arxiv_1912.05891_2019.pdf",
        "arxiv_1911.00964_2019.pdf",
        "2401.08406.pdf"
      ],
      "execution_time": 4.554481744766235,
      "k1": 2.0,
      "b": 0.9,
      "category": "technical",
      "query_id": 44,
      "difficulty": "medium"
    },
    {
      "question": "How does RAG handle out-of-domain queries?",
      "answer": "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: • (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? • (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? • (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? •\n(RQ4): What are the costs associated w\n\nchanisms\nwith generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs.\nThe study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated\nto handle knowledge-intensive tasks. A detailed review of the significant technological advancements in\nRAG is provided, including key innovations in retrieval-augmented language models and applications\nacross various domains such as question-answering, summarization, and knowledg\n\n against all of the\ndocuments. Cognitive Reviewer is currently used by PhD students\nfrom Deakin University to support their literature reviews. The\nCognitive Reviewer does the Index process at run time and relies\n5https://figshare.com/s/fbf7805b5f20d7f7e356\non a robust data processing pipeline to handle uploaded documents\ni.e. no quality control possible at development time. This system\nalso uses a ranking algorithm to sort the uploaded documents.\n4.2\nAI Tutor\nThe AI Tutor is a RAG system where ",
      "contexts": [
        "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: • (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? • (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? • (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? •\n(RQ4): What are the costs associated w",
        "chanisms\nwith generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs.\nThe study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated\nto handle knowledge-intensive tasks. A detailed review of the significant technological advancements in\nRAG is provided, including key innovations in retrieval-augmented language models and applications\nacross various domains such as question-answering, summarization, and knowledg",
        " against all of the\ndocuments. Cognitive Reviewer is currently used by PhD students\nfrom Deakin University to support their literature reviews. The\nCognitive Reviewer does the Index process at run time and relies\n5https://figshare.com/s/fbf7805b5f20d7f7e356\non a robust data processing pipeline to handle uploaded documents\ni.e. no quality control possible at development time. This system\nalso uses a ranking algorithm to sort the uploaded documents.\n4.2\nAI Tutor\nThe AI Tutor is a RAG system where ",
        " able to gather ﬂexible size of n-gram\nrepresentations that are needed for better understanding\nof the text that depends on context, syntax, and semantics.\n• In order to handle the extraction of ﬂexible size n-\ngram representations, one can employ the kernels with\nvarious window sizes, but another issue is ﬁred up with\nsuch settings: What would be the right architecture of\nusing different kernel sizes? In other terms, how much\nexpanding does the network require for different kernel\nsizes to prod",
        "-computer interaction and assistive technologies.\nResearch should explore how RAG systems can leverage BCI data to enhance user experience and\ngenerate context-aware responses.The integration of RAG with AR and VR technologies presents\nopportunities for creating immersive and interactive experiences. Future research should investigate how\nRAG models can be used to enhance AR and VR applications by providing contextually relevant\ninformation and interactions.\n7. Conclusion\nRetrieval-Augmented Gen"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        13.554281411108605,
        12.564242607276075,
        12.086566173027354
      ],
      "sources": [
        "2410.05779.pdf",
        "2410.12837.pdf",
        "2401.05856.pdf",
        "arxiv_1911.00964_2019.pdf",
        "2410.12837.pdf"
      ],
      "execution_time": 4.11660099029541,
      "k1": 2.0,
      "b": 0.9,
      "category": "technical",
      "query_id": 45,
      "difficulty": "hard"
    },
    {
      "question": "What is Self-RAG and how does it differ from standard RAG?",
      "answer": "re vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?\nWhat is the primary focus of the Famous Marshmallow Experiment conducted by Walter Mischel?\nWho is Aaron T. Beck known for his work in cognitive therapy?\nWhat is the Thematic Apperception Test (TAT) and how is it commonly utilized in uncovering a\nperson’s und\n\nept of Universal\nJurisdiction function in international\nlaw and how has it been applied in\nvarious cases across the globe?”\n“Can you share some information\nabout the Convention on the Elimi-\nnation of All Forms of Discrimina-\ntion Against Women (CEDAW)?”\nJurisprudence\n“What is legal positivism and how\ndoes it differ from other legal the-\nories? Can you provide a detailed\nanalysis of its historical evolution,\nkey proponents, and significant con-\ntributions to the field of jurispru-\ndence?”\n“Can y\n\ncerns or special precautions about DRUG?\n• What is the action of DRUG and how does it work?\n• Who should get DRUG and why is it prescribed?\n• What to do in case of a severe reaction to DRUG?\n3. Question Type for other medical entities (e.g. Procedure, Exam, Treatment): Information.\n• What is Coronary Artery Bypass Surgery?\n• What are Liver Function Tests?\n4.3\nMedical Resources\nWe used 12 trusted websites to construct a collection of question-answer pairs. For each website, we extracted the free\n",
      "contexts": [
        "re vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?\nWhat is the primary focus of the Famous Marshmallow Experiment conducted by Walter Mischel?\nWho is Aaron T. Beck known for his work in cognitive therapy?\nWhat is the Thematic Apperception Test (TAT) and how is it commonly utilized in uncovering a\nperson’s und",
        "ept of Universal\nJurisdiction function in international\nlaw and how has it been applied in\nvarious cases across the globe?”\n“Can you share some information\nabout the Convention on the Elimi-\nnation of All Forms of Discrimina-\ntion Against Women (CEDAW)?”\nJurisprudence\n“What is legal positivism and how\ndoes it differ from other legal the-\nories? Can you provide a detailed\nanalysis of its historical evolution,\nkey proponents, and significant con-\ntributions to the field of jurispru-\ndence?”\n“Can y",
        "cerns or special precautions about DRUG?\n• What is the action of DRUG and how does it work?\n• Who should get DRUG and why is it prescribed?\n• What to do in case of a severe reaction to DRUG?\n3. Question Type for other medical entities (e.g. Procedure, Exam, Treatment): Information.\n• What is Coronary Artery Bypass Surgery?\n• What are Liver Function Tests?\n4.3\nMedical Resources\nWe used 12 trusted websites to construct a collection of question-answer pairs. For each website, we extracted the free\n",
        "s and supplements, Important warning, Special instructions, Brand names, How does it work, How\neffective is it, Indication, Contraindication, Learn more, Side effects, Emergency or overdose, Severe reaction,\nForget a dose, Dietary, Why get vaccinated, Storage and disposal, Usage, Dose.\nExamples:\n• Are there interactions between DRUG and herbs and supplements?\n• What important warning or information should I know about DRUG?\n• Are there safety concerns or special precautions about DRUG?\n• What is",
        " to delimit the name of a group and its constituents?\n1\nI tend to overuse the hyphen as a pause, and would appreciate some feedback on this\nHot Network Questions\nHow could we fake a moon landing now?\nUsing audio cues to encourage good posture\nChinese Seal on silk painting - what does it mean?\nHow does the math work when buying airline miles?\nWhat causes the direction of lightning flashes?\nMaximum summed subsequences with non-adjacent items\nMost bit efficient text communication method?\nSignifican"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        20.150638461795324,
        19.582883368797187,
        17.738776230167
      ],
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "arxiv_1901.08079_2019.pdf",
        "arxiv_1901.08079_2019.pdf",
        "2406.11794.pdf"
      ],
      "execution_time": 4.416114568710327,
      "k1": 2.0,
      "b": 0.9,
      "category": "approaches",
      "query_id": 11,
      "difficulty": "hard"
    },
    {
      "question": "Explain the concept of Corrective RAG (CRAG)",
      "answer": "itably\nexhibit hallucinations since the accuracy of\ngenerated texts cannot be secured solely by\nthe parametric knowledge they encapsulate. Al-\nthough retrieval-augmented generation (RAG)\nis a practicable complement to LLMs, it relies\nheavily on the relevance of retrieved docu-\nments, raising concerns about how the model\nbehaves if retrieval goes wrong. To this end, we\npropose the Corrective Retrieval Augmented\nGeneration (CRAG) to improve the robustness\nof generation.\nSpecifically, a lightweight\n\nen equally referred to and involved in RAG.\nOn account of the above issues, this paper\nparticularly\nstudies\nthe\nscenarios\nwhere\nthe retriever returns inaccurate results.\nA\nmethod named Corrective Retrieval-Augmented\nGeneration (CRAG) is proposed to self-correct\nthe results of retriever and improve the utilization\nof documents for augmenting generation.\nA\nlightweight retrieval evaluator is designed to\nassess the overall quality of retrieved documents\nfor a query. This serves as a crucial componen\n\non - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept",
      "contexts": [
        "itably\nexhibit hallucinations since the accuracy of\ngenerated texts cannot be secured solely by\nthe parametric knowledge they encapsulate. Al-\nthough retrieval-augmented generation (RAG)\nis a practicable complement to LLMs, it relies\nheavily on the relevance of retrieved docu-\nments, raising concerns about how the model\nbehaves if retrieval goes wrong. To this end, we\npropose the Corrective Retrieval Augmented\nGeneration (CRAG) to improve the robustness\nof generation.\nSpecifically, a lightweight",
        "en equally referred to and involved in RAG.\nOn account of the above issues, this paper\nparticularly\nstudies\nthe\nscenarios\nwhere\nthe retriever returns inaccurate results.\nA\nmethod named Corrective Retrieval-Augmented\nGeneration (CRAG) is proposed to self-correct\nthe results of retriever and improve the utilization\nof documents for augmenting generation.\nA\nlightweight retrieval evaluator is designed to\nassess the overall quality of retrieved documents\nfor a query. This serves as a crucial componen",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept",
        "on - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        18.37566252479702,
        18.14501650391511,
        13.366118956990485
      ],
      "sources": [
        "Corrective_RAG_2024.pdf",
        "Corrective_RAG_2024.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf"
      ],
      "execution_time": 4.487623453140259,
      "k1": 2.0,
      "b": 0.9,
      "category": "approaches",
      "query_id": 12,
      "difficulty": "hard"
    },
    {
      "question": "How does Hybrid RAG combine different retrieval methods?",
      "answer": "ing user\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\nintegrate different retrieval methods to achieve optimal performance for diverse tasks.\n6.7 Integration with Emerging Technologies: Integrating RAG models with brain-computer interfaces\n(BCIs) could lead to novel applications in human-computer interaction and assistive technologies.\n\n\nasing the number of hypothetical\ndocuments does not yield significant benefits and substantially raises latency, indicating that using a\nsingle hypothetical document is sufficient.\n3.4.3\nHybrid Search with Different Weight on Sparse Retrieval\nTable 8 presents the impact of different α values in hybrid search, where α controls the weighting\nbetween sparse retrieval and dense retrieval components. The relevance score is calculated as follows:\nSh = α · Ss + Sd\n(1)\nwhere Ss, Sd are the normalized re\n\ney produce plausible but incorrect or non-existent information (Ji et al. 2022). This\nlimitation has prompted the exploration of hybrid models that combine retrieval mechanisms with\ngenerative capabilities to ensure both fluency and factual correctness in outputs. There has been a\nsignificant rise in several research papers in this field and several new methods across the RAG\ncomponents have been proposed. Apart from new algorithms and methods, RAG has also seen steep\nadoption across various app",
      "contexts": [
        "ing user\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\nintegrate different retrieval methods to achieve optimal performance for diverse tasks.\n6.7 Integration with Emerging Technologies: Integrating RAG models with brain-computer interfaces\n(BCIs) could lead to novel applications in human-computer interaction and assistive technologies.\n",
        "asing the number of hypothetical\ndocuments does not yield significant benefits and substantially raises latency, indicating that using a\nsingle hypothetical document is sufficient.\n3.4.3\nHybrid Search with Different Weight on Sparse Retrieval\nTable 8 presents the impact of different α values in hybrid search, where α controls the weighting\nbetween sparse retrieval and dense retrieval components. The relevance score is calculated as follows:\nSh = α · Ss + Sd\n(1)\nwhere Ss, Sd are the normalized re",
        "ey produce plausible but incorrect or non-existent information (Ji et al. 2022). This\nlimitation has prompted the exploration of hybrid models that combine retrieval mechanisms with\ngenerative capabilities to ensure both fluency and factual correctness in outputs. There has been a\nsignificant rise in several research papers in this field and several new methods across the RAG\ncomponents have been proposed. Apart from new algorithms and methods, RAG has also seen steep\nadoption across various app",
        " quality. These results underscore the importance\nof developing specialized strategies to integrate retrieval with\nlanguage generation models, highlighting the need for further\nresearch and exploration into the robustness of RAG.\nC. Hybrid Approaches\nCombining RAG with fine-tuning is emerging as a leading\nstrategy. Determining the optimal integration of RAG and\nfine-tuning whether sequential, alternating, or through end-to-\nend joint training—and how to harness both parameterized\n15\nTABLE IV\nSUM",
        " quality. These results underscore the importance\nof developing specialized strategies to integrate retrieval with\nlanguage generation models, highlighting the need for further\nresearch and exploration into the robustness of RAG.\nC. Hybrid Approaches\nCombining RAG with fine-tuning is emerging as a leading\nstrategy. Determining the optimal integration of RAG and\nfine-tuning whether sequential, alternating, or through end-to-\nend joint training—and how to harness both parameterized\n15\nTABLE IV\nSUM"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        27.90629853920705,
        19.22566344089442,
        17.874228976674086
      ],
      "sources": [
        "2410.12837.pdf",
        "2407.01219.pdf",
        "2410.12837.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "execution_time": 4.217241525650024,
      "k1": 2.0,
      "b": 0.9,
      "category": "approaches",
      "query_id": 13,
      "difficulty": "medium"
    },
    {
      "question": "What is the advantage of query rewriting in RAG?",
      "answer": "ewriting\nTo build powerful conversational query rewriting model, we take GPT-3.5-Turbo as the rewriter\ngiven that Galimzhanova et al. (2023) demonstrated the state-of-the-art query rewriting results using\nGPT-3.5-Turbo. Similar to Galimzhanova et al. (2023), we not only provide GPT-3.5-Turbo with\nthe rewriting task instruction, but also give it few-shot rewriting examples to enhance the quality of\nrewriting results. More details can be found in Appendix C.1.\n4.3\nComparisons\nIn Table 1, we compar\n\n cope with conversational\ninputs.\nAn alternative solution is conversational query rewriting method which uses a query rewriter to rewrite\nthe current question based on the conversational history. The rewritten query is then directly used as\nthe input to a single-turn query retriever for retrieving relevant context. In addition to the embedding\nand search cost, the query rewriting model introduces a large amount of extra computational expense\nto generate the rewritten query.\n4.1\nFine-tuning Retri\n\n questions (e.g., with pronouns referring to entities mentioned in the previous\nconversation) may have insufficient information for retrieval, while feeding them along with all of\nthe dialogue history can be redundant, thus leading to sub-optimal results.\nConversational Query Rewriting\nMost of the previous solutions are query rewriting methods.\nThe latest turn of question is rewritten to be a standalone query without additional information from\nprevious dialogue history (Vakulenko et al., 2021a;",
      "contexts": [
        "ewriting\nTo build powerful conversational query rewriting model, we take GPT-3.5-Turbo as the rewriter\ngiven that Galimzhanova et al. (2023) demonstrated the state-of-the-art query rewriting results using\nGPT-3.5-Turbo. Similar to Galimzhanova et al. (2023), we not only provide GPT-3.5-Turbo with\nthe rewriting task instruction, but also give it few-shot rewriting examples to enhance the quality of\nrewriting results. More details can be found in Appendix C.1.\n4.3\nComparisons\nIn Table 1, we compar",
        " cope with conversational\ninputs.\nAn alternative solution is conversational query rewriting method which uses a query rewriter to rewrite\nthe current question based on the conversational history. The rewritten query is then directly used as\nthe input to a single-turn query retriever for retrieving relevant context. In addition to the embedding\nand search cost, the query rewriting model introduces a large amount of extra computational expense\nto generate the rewritten query.\n4.1\nFine-tuning Retri",
        " questions (e.g., with pronouns referring to entities mentioned in the previous\nconversation) may have insufficient information for retrieval, while feeding them along with all of\nthe dialogue history can be redundant, thus leading to sub-optimal results.\nConversational Query Rewriting\nMost of the previous solutions are query rewriting methods.\nThe latest turn of question is rewritten to be a standalone query without additional information from\nprevious dialogue history (Vakulenko et al., 2021a;",
        "the methods for effectively fine-tuning\nLLMs (see Figure 1).\nWhat adds complexity and challenge is the variability in implementing each processing step. For\nexample, in retrieving relevant documents for an input query, various methods can be employed.\nOne approach involves rewriting the query first and using the rewritten queries for retrieval [9].\nAlternatively, pseudo-responses to the query can be generated first, and the similarity between\nthese pseudo-responses and the backend documents can ",
        "s, (ii) query ex-\npansion - rewriting the query based on relevant\nfeedback. They proposed a method for query dis-\nambiguation where a new query is formed by per-\nforming few-shot prompting. text-davinci-003 was\nemployed to retrieve a pseudo-document which\nis concatenated with the original query to form a\nnew query for the retrieval task. In this approach,\ninference is slowed down by token-by-token auto-\nregressive decoding strategy.\nIn another study,\nAlaofi et al. (2023) explored the utility of "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.16920907014761,
        16.406868153766897,
        15.541725295318528
      ],
      "sources": [
        "Graph_RAG_2024.pdf",
        "Graph_RAG_2024.pdf",
        "Graph_RAG_2024.pdf",
        "2407.01219.pdf",
        "2402.14558.pdf"
      ],
      "execution_time": 4.53835391998291,
      "k1": 2.0,
      "b": 0.9,
      "category": "approaches",
      "query_id": 14,
      "difficulty": "medium"
    },
    {
      "question": "Explain the re-ranking stage in Advanced RAG",
      "answer": ". Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\nadvanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n• We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof “Retrieval”, “Generation” and “Augmentation”, and\ndelve into their synergies, elucidating how these com-\nponents intric\n\n. Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\nadvanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n• We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof “Retrieval”, “Generation” and “Augmentation”, and\ndelve into their synergies, elucidating how these com-\nponents intric\n\n. Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\nadvanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n• We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof “Retrieval”, “Generation” and “Augmentation”, and\ndelve into their synergies, elucidating how these com-\nponents intric",
      "contexts": [
        ". Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\nadvanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n• We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof “Retrieval”, “Generation” and “Augmentation”, and\ndelve into their synergies, elucidating how these com-\nponents intric",
        ". Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\nadvanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n• We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof “Retrieval”, “Generation” and “Augmentation”, and\ndelve into their synergies, elucidating how these com-\nponents intric",
        ". Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\nadvanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n• We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof “Retrieval”, “Generation” and “Augmentation”, and\ndelve into their synergies, elucidating how these com-\nponents intric",
        "hree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to",
        "hree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        15.99887250951981,
        15.99887250951981,
        15.99887250951981
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "execution_time": 4.481347322463989,
      "k1": 2.0,
      "b": 0.9,
      "category": "approaches",
      "query_id": 15,
      "difficulty": "medium"
    },
    {
      "question": "What is iterative RAG and when is it beneficial?",
      "answer": "to retrieve based on the query\ncomplexity by a classifier, which is a smaller LM.\nIterative RAG: Iterative RAG progressively refines results\nby repeatedly cycling through retrieval and generation phases,\nrather than a single round.\nRepoCoder [186] uses an iterative retrieval-generation ap-\nproach for code completion, refining queries with previously\n9\nTABLE I: Taxonomy of RAG applications across various modalities.\nRAG for Text\nQuestion Answering\nHuman-Machine Conversation\nNeural Machine Transla\n\nshine a light on what issues\nengineers will face and what software engineering research is nec-\nessary to realise solutions with the current state-of-the-art RAG\nsystems.\nEmerging work has looked at benchmarking RAG systems [3]\nbut not at the failures occurring during implementation. Software\nengineering research has investigated the use of RAG systems for\ncode-related tasks [15]. However, the application of RAG systems\nis broader than software engineering tasks. This paper comple-\nments existin\n\nre vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?\nWhat is the primary focus of the Famous Marshmallow Experiment conducted by Walter Mischel?\nWho is Aaron T. Beck known for his work in cognitive therapy?\nWhat is the Thematic Apperception Test (TAT) and how is it commonly utilized in uncovering a\nperson’s und",
      "contexts": [
        "to retrieve based on the query\ncomplexity by a classifier, which is a smaller LM.\nIterative RAG: Iterative RAG progressively refines results\nby repeatedly cycling through retrieval and generation phases,\nrather than a single round.\nRepoCoder [186] uses an iterative retrieval-generation ap-\nproach for code completion, refining queries with previously\n9\nTABLE I: Taxonomy of RAG applications across various modalities.\nRAG for Text\nQuestion Answering\nHuman-Machine Conversation\nNeural Machine Transla",
        "shine a light on what issues\nengineers will face and what software engineering research is nec-\nessary to realise solutions with the current state-of-the-art RAG\nsystems.\nEmerging work has looked at benchmarking RAG systems [3]\nbut not at the failures occurring during implementation. Software\nengineering research has investigated the use of RAG systems for\ncode-related tasks [15]. However, the application of RAG systems\nis broader than software engineering tasks. This paper comple-\nments existin",
        "re vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?\nWhat is the primary focus of the Famous Marshmallow Experiment conducted by Walter Mischel?\nWho is Aaron T. Beck known for his work in cognitive therapy?\nWhat is the Thematic Apperception Test (TAT) and how is it commonly utilized in uncovering a\nperson’s und",
        "LMs with\ndomain-specific knowledge. PKG employs a trainable background knowledge module, aligning it\nwith task knowledge and generating relevant contextual information.\nIterative Retrieval. When confronted with intricate challenges like multi-step reasoning [344]\nand long-form question answering [83, 284], traditional one-time retrieval may fall short. Address-\ning these demanding information needs, recent studies have proposed iterative retrieval, which\nallows for continuously gathering knowled",
        "ction” task.\nQuery-based RAG is the mainstream paradigm for code\ncompletion. Drain et al. [240] retrieved template functions\nfor function completion. ReACC [91] uses both sparse and\ndense retrieval. RepoCoder [186] performs iterative RAG\nby augmenting the retrieval input with previously generated\ncode. De-Hallucinator [241] retrieves API references using\nfirst-time generated contents, then conducts query-based RAG\nfor improved code completion. REPOFUSE [242] includes\nrationale context and retrie"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.391675169100665,
        14.961265661074043,
        14.112748991735916
      ],
      "sources": [
        "2402.19473.pdf",
        "2401.05856.pdf",
        "2403.18802.pdf",
        "2311.05232.pdf",
        "2402.19473.pdf"
      ],
      "execution_time": 4.4697511196136475,
      "k1": 2.0,
      "b": 0.9,
      "category": "approaches",
      "query_id": 46,
      "difficulty": "hard"
    },
    {
      "question": "Explain the concept of fusion in hybrid RAG systems",
      "answer": "rk to process retrieved video-sentence\npairs with an attention mechanism, generating hidden states to\nproduce captions. CARE [117] introduces a concept detector\nto produce concept probabilities, and incorporates concept\nrepresentations into a hybrid attention mechanism.\nEgoInstructor [49] uses gated-cross attention to merge text\nand video features, improving the relevance and coherence\nof captions for egocentric videos. Latent representation-based\nRAG, adaptable across modalities and tasks, blen\n\nings.\n𝑡\nMLP\nCross\nAttention\nLinear\nFusion Block\nTransformer  Encoder\nCLIP\nText\nCLIP\nText\nLinear\n𝑥𝑡\n1\n𝑥𝑡\n2\n𝑥𝑡\n3\n𝑥𝑡\n𝑁\n𝑚1\n𝑚2\n𝑧𝑡𝑘\n𝑠\n𝑧𝑡𝑘\n𝑙\n𝑚3\n𝑚𝑅\n⊕\nො𝑥0\n1\nො𝑥0\n2\nො𝑥0\n3\nො𝑥0\n𝑁\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\nLinear\nLinear\nTransformer  Encoder\nFusion Block\n⊕\nOrigin  Motion Diffusion\nFeature Fusion\nText Decomposition\nfine-tuned\nSelf \nAttention\nDropout\nLayer Norm\nLinear\nGELU\nLayer Norm\nDropout\nLinear\nFusion Block\noptional\nℱ𝑙\n2\nℱ𝑠\nℱ𝑡\n𝒞𝑙\n𝒞𝑠\nHybrid Retrieval\nℰ𝑠\nℰ𝑙\nℱ𝑙\n1\n𝑝𝜃1\nSearch with Anatomical Text\nBest Match\nDataset\n𝑚1:𝑅\nRefere\n\nchitectural components, recent research in this area, current challenges and limitations of RAG, and\nfuture research direction.\n2: Core Components and Architectural Overview of RAG Systems\n2.1 Overview of RAG Models\nRetrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural\nlanguage generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base.\nTraditional large language models (LLMs) such as GPT-3 and BERT, which are pre-train",
      "contexts": [
        "rk to process retrieved video-sentence\npairs with an attention mechanism, generating hidden states to\nproduce captions. CARE [117] introduces a concept detector\nto produce concept probabilities, and incorporates concept\nrepresentations into a hybrid attention mechanism.\nEgoInstructor [49] uses gated-cross attention to merge text\nand video features, improving the relevance and coherence\nof captions for egocentric videos. Latent representation-based\nRAG, adaptable across modalities and tasks, blen",
        "ings.\n𝑡\nMLP\nCross\nAttention\nLinear\nFusion Block\nTransformer  Encoder\nCLIP\nText\nCLIP\nText\nLinear\n𝑥𝑡\n1\n𝑥𝑡\n2\n𝑥𝑡\n3\n𝑥𝑡\n𝑁\n𝑚1\n𝑚2\n𝑧𝑡𝑘\n𝑠\n𝑧𝑡𝑘\n𝑙\n𝑚3\n𝑚𝑅\n⊕\nො𝑥0\n1\nො𝑥0\n2\nො𝑥0\n3\nො𝑥0\n𝑁\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\nLinear\nLinear\nTransformer  Encoder\nFusion Block\n⊕\nOrigin  Motion Diffusion\nFeature Fusion\nText Decomposition\nfine-tuned\nSelf \nAttention\nDropout\nLayer Norm\nLinear\nGELU\nLayer Norm\nDropout\nLinear\nFusion Block\noptional\nℱ𝑙\n2\nℱ𝑠\nℱ𝑡\n𝒞𝑙\n𝒞𝑠\nHybrid Retrieval\nℰ𝑠\nℰ𝑙\nℱ𝑙\n1\n𝑝𝜃1\nSearch with Anatomical Text\nBest Match\nDataset\n𝑚1:𝑅\nRefere",
        "chitectural components, recent research in this area, current challenges and limitations of RAG, and\nfuture research direction.\n2: Core Components and Architectural Overview of RAG Systems\n2.1 Overview of RAG Models\nRetrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural\nlanguage generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base.\nTraditional large language models (LLMs) such as GPT-3 and BERT, which are pre-train",
        "The development of hybrid systems combining retrieval and generation began to gain momentum as\nresearchers recognized the complementary strengths of both approaches. Early efforts in hybrid modeling\ncan be traced back to works like DrQA (Chen et al. 2017), which employed retrieval techniques to fetch\nrelevant documents for question-answering tasks. However, the generative component in such systems\nwas minimal, often limited to selecting text directly from the retrieved documents. Similarly, in m",
        "ration system, improves accuracy by using adaptive parameter tuning and\nhybrid retrieval strategies. Tested on NYC Local Law 144 (LL144), HyPA-RAG demonstrates enhanced\ncorrectness and contextual precision, addressing the complexities of legal texts. MemoRAG (Qian et. al.,\n2024) introduces a novel Retrieval-Augmented Generation (RAG) paradigm designed to overcome the\nlimitations of traditional RAG systems in handling ambiguous or unstructured knowledge. MemoRAG’s\ndual-system architecture utilize"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.20992399556019,
        15.967760439419232,
        15.573520367187735
      ],
      "sources": [
        "2402.19473.pdf",
        "2402.19473.pdf",
        "2410.12837.pdf",
        "2410.12837.pdf",
        "2410.12837.pdf"
      ],
      "execution_time": 4.499094486236572,
      "k1": 2.0,
      "b": 0.9,
      "category": "approaches",
      "query_id": 47,
      "difficulty": "medium"
    },
    {
      "question": "What is adaptive retrieval in Self-RAG?",
      "answer": "nvolves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\nC. Adaptive Retrieval\nAdaptive r\n\nnvolves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\nC. Adaptive Retrieval\nAdaptive r\n\nnvolves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\nC. Adaptive Retrieval\nAdaptive r",
      "contexts": [
        "nvolves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\nC. Adaptive Retrieval\nAdaptive r",
        "nvolves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\nC. Adaptive Retrieval\nAdaptive r",
        "nvolves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\nC. Adaptive Retrieval\nAdaptive r",
        "rs\n(B) Multi-Step Approach\nInefficient\nk times\nComplex Query: \nWhat currency is in \nBilly Giles’ birthplace?\nDocuments\n(Intermediate) \nAnswers\nk times\nStraightforward Query: \nParis is the capital of what?\n(C) Our Adaptive Approach\nAnswer\nSimple Query: \nWhen is the birthday \nof Michael F. Phelps?\nDocuments\nAnswer\nComplex Query: \nWhat currency is in \nBilly Giles’ birthplace?\nDocuments\n(Intermediate) \nAnswers\nk times\nClassifier\nFigure 2: A conceptual comparison of different retrieval-augmented LLM ",
        "rs\n(B) Multi-Step Approach\nInefficient\nk times\nComplex Query: \nWhat currency is in \nBilly Giles’ birthplace?\nDocuments\n(Intermediate) \nAnswers\nk times\nStraightforward Query: \nParis is the capital of what?\n(C) Our Adaptive Approach\nAnswer\nSimple Query: \nWhen is the birthday \nof Michael F. Phelps?\nDocuments\nAnswer\nComplex Query: \nWhat currency is in \nBilly Giles’ birthplace?\nDocuments\n(Intermediate) \nAnswers\nk times\nClassifier\nFigure 2: A conceptual comparison of different retrieval-augmented LLM "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        13.6251821285857,
        13.6251821285857,
        13.6251821285857
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "2403.14403.pdf",
        "Adaptive_RAG_2024.pdf"
      ],
      "execution_time": 4.39937949180603,
      "k1": 2.0,
      "b": 0.9,
      "category": "approaches",
      "query_id": 48,
      "difficulty": "hard"
    },
    {
      "question": "How does context enrichment improve RAG performance?",
      "answer": "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: • (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? • (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? • (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? •\n(RQ4): What are the costs associated w\n\nrating continuous context and top-k retrieved chunks in\nstage-2 tuning.\n6.4\nAblation Studies on Inference Stage\nIn Table 7, we show ablation studies on how the number of retrieved context/chunks, context ordering,\nand different retrievers affect the conversational QA and RAG results.\nFirst, we find that using more contexts as inputs do not always improve the results. Utilizing top-5\ncontexts as input yields better results compared to using either top-3 or top-10 contexts. Intuitively,\nmore conte\n\n has also sparked\ndiscussions on whether RAG is still necessary when LLMs\n8https://www.trulens.org/trulens eval/core concepts rag triad/\n9https://kimi.moonshot.cn\nare not constrained by context. In fact, RAG still plays an\nirreplaceable role. On one hand, providing LLMs with a\nlarge amount of context at once will significantly impact its\ninference speed, while chunked retrieval and on-demand input\ncan significantly improve operational efficiency. On the other\nhand, RAG-based generation can quick",
      "contexts": [
        "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: • (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? • (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? • (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? •\n(RQ4): What are the costs associated w",
        "rating continuous context and top-k retrieved chunks in\nstage-2 tuning.\n6.4\nAblation Studies on Inference Stage\nIn Table 7, we show ablation studies on how the number of retrieved context/chunks, context ordering,\nand different retrievers affect the conversational QA and RAG results.\nFirst, we find that using more contexts as inputs do not always improve the results. Utilizing top-5\ncontexts as input yields better results compared to using either top-3 or top-10 contexts. Intuitively,\nmore conte",
        " has also sparked\ndiscussions on whether RAG is still necessary when LLMs\n8https://www.trulens.org/trulens eval/core concepts rag triad/\n9https://kimi.moonshot.cn\nare not constrained by context. In fact, RAG still plays an\nirreplaceable role. On one hand, providing LLMs with a\nlarge amount of context at once will significantly impact its\ninference speed, while chunked retrieval and on-demand input\ncan significantly improve operational efficiency. On the other\nhand, RAG-based generation can quick",
        " has also sparked\ndiscussions on whether RAG is still necessary when LLMs\n8https://www.trulens.org/trulens eval/core concepts rag triad/\n9https://kimi.moonshot.cn\nare not constrained by context. In fact, RAG still plays an\nirreplaceable role. On one hand, providing LLMs with a\nlarge amount of context at once will significantly impact its\ninference speed, while chunked retrieval and on-demand input\ncan significantly improve operational efficiency. On the other\nhand, RAG-based generation can quick",
        " has also sparked\ndiscussions on whether RAG is still necessary when LLMs\n8https://www.trulens.org/trulens eval/core concepts rag triad/\n9https://kimi.moonshot.cn\nare not constrained by context. In fact, RAG still plays an\nirreplaceable role. On one hand, providing LLMs with a\nlarge amount of context at once will significantly impact its\ninference speed, while chunked retrieval and on-demand input\ncan significantly improve operational efficiency. On the other\nhand, RAG-based generation can quick"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        22.987506382727926,
        14.26775774119724,
        12.93860341550734
      ],
      "sources": [
        "2410.05779.pdf",
        "Graph_RAG_2024.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "2312.10997.pdf"
      ],
      "execution_time": 4.201248645782471,
      "k1": 2.0,
      "b": 0.9,
      "category": "approaches",
      "query_id": 49,
      "difficulty": "medium"
    },
    {
      "question": "What is the role of web search in Corrective RAG?",
      "answer": "ngs and what was his role at Netflix?\nWhat can you tell me about Elon Musk’s role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson’s tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association’s \"Ice Bucket Challenge\" marketing\ncampaign?\nWhat are the specifics of IBM’s \"Smarter Planet\" marketing c\n\nnowledge for correction. Here, web\nsearch is introduced to search from the Internet as\nelaborated in Section 4.5. This corrective action\nhelps overcome the embarrassing challenge where\nno reliable knowledge can be referred to.\nAmbiguous\nExcept for the above two situations,\nthe remaining will be assigned to an intermediate\naction of Ambiguous. This generally occurs when\nthe accuracy of the retrieval is hard to distinguish\nand the evaluator gives an intermediate score.\nSince the retrieval evaluato\n\nam?\nWhat is the NLTK library for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you",
      "contexts": [
        "ngs and what was his role at Netflix?\nWhat can you tell me about Elon Musk’s role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson’s tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association’s \"Ice Bucket Challenge\" marketing\ncampaign?\nWhat are the specifics of IBM’s \"Smarter Planet\" marketing c",
        "nowledge for correction. Here, web\nsearch is introduced to search from the Internet as\nelaborated in Section 4.5. This corrective action\nhelps overcome the embarrassing challenge where\nno reliable knowledge can be referred to.\nAmbiguous\nExcept for the above two situations,\nthe remaining will be assigned to an intermediate\naction of Ambiguous. This generally occurs when\nthe accuracy of the retrieval is hard to distinguish\nand the evaluator gives an intermediate score.\nSince the retrieval evaluato",
        "am?\nWhat is the NLTK library for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you",
        "ler size.\nThe Internet as a knowledge base\nMore related\nto our work, the search engine can assume the role\nof the retriever and use the Internet as the source of\nInput\nRetriever\nOutput\nDocuments\nInput\nWeb Search\nDocuments\nBlack-box LLM\nQuery\nInput\nDocuments\nQuery\nOutput\nOutput\nReward\nInput:\nWhat profession does Nicholas Ray and \nElia Kazan have in common?\nQuery: Nicholas Ray profession\nNicholas Ray American author and \ndirector, original name Raymond \nNicholas Kienzle, born August 7, \n1911, Gale",
        "ereby\nvalidating its lightweight nature.\n6\nConclusion & Limitation\nThis paper studies the problem where RAG-based\napproaches are challenged if retrieval goes wrong,\nthereby exposing inaccurate and misleading knowl-\nedge to generative LMs.\nCorrective Retrieval\nAugmented Generation is proposed to improve the\nrobustness of generation. Essentially, a lightweight\nretrieval evaluator is to estimate and trigger three\nknowledge retrieval actions discriminately. With\nthe further leverage of web search an"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        19.267821834998582,
        18.357052361301964,
        18.207120451375918
      ],
      "sources": [
        "2403.18802.pdf",
        "Corrective_RAG_2024.pdf",
        "2403.18802.pdf",
        "2305.14283.pdf",
        "Corrective_RAG_2024.pdf"
      ],
      "execution_time": 4.498559474945068,
      "k1": 2.0,
      "b": 0.9,
      "category": "approaches",
      "query_id": 50,
      "difficulty": "medium"
    },
    {
      "question": "What metrics should be used to evaluate RAG systems?",
      "answer": "e on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models.The main evaluation objectives include:\nRetrieval Quality. Evaluating the retriev\n\ne on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models.The main evaluation objectives include:\nRetrieval Quality. Evaluating the retriev\n\ne on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models.The main evaluation objectives include:\nRetrieval Quality. Evaluating the retriev",
      "contexts": [
        "e on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models.The main evaluation objectives include:\nRetrieval Quality. Evaluating the retriev",
        "e on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models.The main evaluation objectives include:\nRetrieval Quality. Evaluating the retriev",
        "e on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models.The main evaluation objectives include:\nRetrieval Quality. Evaluating the retriev",
        "evise metrics that can objectively evaluate quality amidst this subjectivity. Secondly,\nthe metrics must consider the dependency of a question’s relevance and usefulness on its context. A question that\nprovides valuable insights in one context may be deemed irrelevant in another, underscoring the need for context-aware\nmetrics.\nThirdly, we need to evaluate the diversity and novelty of the generated questions. A robust question generation system\nshould produce a wide array of questions covering v",
        "-computer interaction and assistive technologies.\nResearch should explore how RAG systems can leverage BCI data to enhance user experience and\ngenerate context-aware responses.The integration of RAG with AR and VR technologies presents\nopportunities for creating immersive and interactive experiences. Future research should investigate how\nRAG models can be used to enhance AR and VR applications by providing contextually relevant\ninformation and interactions.\n7. Conclusion\nRetrieval-Augmented Gen"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        18.51337643316456,
        18.51337643316456,
        18.51337643316456
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "2401.08406.pdf",
        "2410.12837.pdf"
      ],
      "execution_time": 4.381752252578735,
      "k1": 2.0,
      "b": 0.9,
      "category": "evaluation",
      "query_id": 16,
      "difficulty": "medium"
    },
    {
      "question": "How do you measure faithfulness in RAG outputs?",
      "answer": "A Pairs\nLDA+\nTranslation1\nTranslation2\nHow many days a week\nshould you lift weight?\nWhen are you to old to\nbuild muscle mass from\nwork out?\nHow\ndo\nI\ngain\nbody\nweight?\nMy\nweight\nis\n90lb\nhow could I gain more\nweight?\nWhat is a 1 set rep?\nMy\nweight\nis\n90lb\nhow could I gain more\nweight?\nWill lose weight faster\nthan average if I work-\nout?\nCan you make your hair\ngrow faster\nIs there any any way to\nget rid of razor bump?\nWhat important func-\ntion do our body hair\nplay?\nHow\ncan\nI\nmake\nmy\nmustache grow \n\nles, we ran-\ndomly sample passages and answers from either:\nthe same Wikipedia document or an entirely ran-\ndom Wikipedia document. This sampling allows\nus to artificially create mock RAG systems for test-\ning ARES. By sampling both related and unrelated\ndocuments/answers, we hope to better gauge the\nefficacy of ARES in judging RAG outputs.\nWe do not evaluate answer faithfulness for KILT\nand SuperGLUE datasets since we do not have\nhuman-annotated hallucinated answers to use for\nevaluation. Howev\n\nmation?\nPersonA: You can find your local SHIP contact information in the back of your\nMedicare & You 2020 Handbook online.\nPersonU: and how do they calculate the adjustments?\nInstead of having this entire conversation, how can PersonU get what he or she is\nlooking for using a single question? Respond with that question.\nrole-assistant:\nHow is the calculation for adjustments made by SHIP determined?\nrole-user:\nGiven the following conversation between PersonU and PersonA:\nPersonU: I need to know h",
      "contexts": [
        "A Pairs\nLDA+\nTranslation1\nTranslation2\nHow many days a week\nshould you lift weight?\nWhen are you to old to\nbuild muscle mass from\nwork out?\nHow\ndo\nI\ngain\nbody\nweight?\nMy\nweight\nis\n90lb\nhow could I gain more\nweight?\nWhat is a 1 set rep?\nMy\nweight\nis\n90lb\nhow could I gain more\nweight?\nWill lose weight faster\nthan average if I work-\nout?\nCan you make your hair\ngrow faster\nIs there any any way to\nget rid of razor bump?\nWhat important func-\ntion do our body hair\nplay?\nHow\ncan\nI\nmake\nmy\nmustache grow ",
        "les, we ran-\ndomly sample passages and answers from either:\nthe same Wikipedia document or an entirely ran-\ndom Wikipedia document. This sampling allows\nus to artificially create mock RAG systems for test-\ning ARES. By sampling both related and unrelated\ndocuments/answers, we hope to better gauge the\nefficacy of ARES in judging RAG outputs.\nWe do not evaluate answer faithfulness for KILT\nand SuperGLUE datasets since we do not have\nhuman-annotated hallucinated answers to use for\nevaluation. Howev",
        "mation?\nPersonA: You can find your local SHIP contact information in the back of your\nMedicare & You 2020 Handbook online.\nPersonU: and how do they calculate the adjustments?\nInstead of having this entire conversation, how can PersonU get what he or she is\nlooking for using a single question? Respond with that question.\nrole-assistant:\nHow is the calculation for adjustments made by SHIP determined?\nrole-user:\nGiven the following conversation between PersonU and PersonA:\nPersonU: I need to know h",
        "stant: You can travel visa-free only if you\nhave a “residence card of a family member of a\nunion citizen” issued under article 10 of the free\nmovement directive\nUser: I have a temporary residence card also, can\nwe travel together visa-free?\nAssistant: If you do not have an article 10 card, EU\ncountries other than Romania must issue you a visa\nfree of charge and on the basis of an expedited\nprocedure.\nUser: How do I get a article 10 card?\nAssistant:\nChatQA-1.0-13B: The grounds for refusal of that",
        " end the task using the task manager or activity monitor.\nIt is important to note that killing a process can have unintended consequences and should only be done\nas a last resort. It is always recommended to seek professional help if you are unsure about how to manage a process.\n[INST] How do I kill a process?\n[/INST]\nTo kill a process in Linux, you can use the ‘kill‘ command followed by the process ID (PID) of the process you want\nto terminate. Here’s an example:\n“‘\nkill 1234\n“‘\nReplace ‘1234‘ "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.355101684794604,
        15.682447240471536,
        14.542716943446246
      ],
      "sources": [
        "arxiv_1606.03783_2016.pdf",
        "2311.09476.pdf",
        "Graph_RAG_2024.pdf",
        "Graph_RAG_2024.pdf",
        "2308.12950.pdf"
      ],
      "execution_time": 4.376302719116211,
      "k1": 2.0,
      "b": 0.9,
      "category": "evaluation",
      "query_id": 17,
      "difficulty": "hard"
    },
    {
      "question": "What is RAGAS and how is it used for RAG evaluation?",
      "answer": "re vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?\nWhat is the primary focus of the Famous Marshmallow Experiment conducted by Walter Mischel?\nWho is Aaron T. Beck known for his work in cognitive therapy?\nWhat is the Thematic Apperception Test (TAT) and how is it commonly utilized in uncovering a\nperson’s und\n\ncerns or special precautions about DRUG?\n• What is the action of DRUG and how does it work?\n• Who should get DRUG and why is it prescribed?\n• What to do in case of a severe reaction to DRUG?\n3. Question Type for other medical entities (e.g. Procedure, Exam, Treatment): Information.\n• What is Coronary Artery Bypass Surgery?\n• What are Liver Function Tests?\n4.3\nMedical Resources\nWe used 12 trusted websites to construct a collection of question-answer pairs. For each website, we extracted the free\n\n\ncompare ARES\nwith our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge.\nFor our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels\nto score the system. RAGAS also uses GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for\neach evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and\nGPT-3.5 across all the explored datasets. The Kendall’",
      "contexts": [
        "re vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?\nWhat is the primary focus of the Famous Marshmallow Experiment conducted by Walter Mischel?\nWho is Aaron T. Beck known for his work in cognitive therapy?\nWhat is the Thematic Apperception Test (TAT) and how is it commonly utilized in uncovering a\nperson’s und",
        "cerns or special precautions about DRUG?\n• What is the action of DRUG and how does it work?\n• Who should get DRUG and why is it prescribed?\n• What to do in case of a severe reaction to DRUG?\n3. Question Type for other medical entities (e.g. Procedure, Exam, Treatment): Information.\n• What is Coronary Artery Bypass Surgery?\n• What are Liver Function Tests?\n4.3\nMedical Resources\nWe used 12 trusted websites to construct a collection of question-answer pairs. For each website, we extracted the free\n",
        "compare ARES\nwith our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge.\nFor our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels\nto score the system. RAGAS also uses GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for\neach evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and\nGPT-3.5 across all the explored datasets. The Kendall’",
        "vance and answer rele-\nvance of a query-passage-answer triple. For con-\ntext relevance, ARES with a fine-tuned LLM-judge\nis 59.9 percentage points higher than RAGAS while\nfor answer relevance, our system is 14.4 percent-\nage points higher than RAGAS. Overall, ARES\nprovides a more accurate system for automatically\nevaluating RAG configurations than RAGAS by\nleveraging domain-adaptive techniques for prompt-\ning and training as well as utilizing PPI to bolster\nmodel predictions.\nAs an additional co",
        " rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing\nlogits in LLM judge prediction to improve PPI\ncon"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        19.250944718198916,
        19.248846999272434,
        18.203810623807854
      ],
      "sources": [
        "2403.18802.pdf",
        "arxiv_1901.08079_2019.pdf",
        "2311.09476.pdf",
        "2311.09476.pdf",
        "2311.09476.pdf"
      ],
      "execution_time": 4.5071656703948975,
      "k1": 2.0,
      "b": 0.9,
      "category": "evaluation",
      "query_id": 18,
      "difficulty": "hard"
    },
    {
      "question": "Explain the difference between retrieval and generation metrics",
      "answer": "include:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality. The assessment of generation quality\ncenters on th\n\ninclude:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality. The assessment of generation quality\ncenters on th\n\ninclude:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality. The assessment of generation quality\ncenters on th",
      "contexts": [
        "include:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality. The assessment of generation quality\ncenters on th",
        "include:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality. The assessment of generation quality\ncenters on th",
        "include:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality. The assessment of generation quality\ncenters on th",
        "at certain metrics were not appropriate.\nSome of them did the opposite to what we required them to do, in that they scored\na reformulation without the target higher than one with the target. This could be\ndue to over-emphasis on word ordering. These metrics were discounted at this\nstage. Other metrics were also discounted as the difference between “With Target”\nand “Without Target” was not large enough; it would have been difﬁcult to measure\nimprovements in the system with a small difference.\nTh",
        "raging\ndata-augmentation methods to construct synthetical data for fine-tuning the classifier, either by\nrule-based perturbation [79, 152, 266] or generation [389].\nQA-based Metrics. In contrast to classifier-based metrics, QA-based metrics [77, 119, 271, 310]\nhave recently garnered attention for their enhanced ability to capture information overlap between\nthe model’s generation and its source. These metrics operate by initially selecting target answers\nfrom the information units within the LLM"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.667663251240896,
        16.667663251240896,
        16.667663251240896
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "arxiv_1304.7157_2013.pdf",
        "2311.05232.pdf"
      ],
      "execution_time": 4.501683235168457,
      "k1": 2.0,
      "b": 0.9,
      "category": "evaluation",
      "query_id": 19,
      "difficulty": "medium"
    },
    {
      "question": "How can we detect hallucinations in RAG systems?",
      "answer": "ting to LLM\nhallucinations in depth (§3), followed by a review of various strategies and benchmarks employed\nfor the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches\ndesigned to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by\ncurrent RAG systems (§6) and delineate potential pathways for forthcoming research (§7).\n2\nDEFINITIONS\nFor the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a\n\ns emerged as a promising strategy to mitigate hallucina-\ntions and improve the factuality of LLM outputs [131, 165, 255, 277]. By incorporating large-scale\nexternal knowledge bases during inference, RAG equips LLMs with up-to-date knowledge, thus\nreducing the potential risk of hallucination due to the inherent knowledge boundaries of LLMs\n[260]. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still\nproduce hallucinations [16]. Hallucinations in RAG present con\n\nucinations [16]. Hallucinations in RAG present considerable complexities, manifesting\nas outputs that are either factually inaccurate or misleading. These hallucinations occur when\nthe content generated by the LLM does not align with real-world facts, fails to accurately reflect\nthe user’s query, or is not supported by the retrieved information. Such hallucinations can stem\nfrom two primary factors: retrieval failure (§6.1) and generation bottleneck (§6.2). Through\na comprehensive analysis of th",
      "contexts": [
        "ting to LLM\nhallucinations in depth (§3), followed by a review of various strategies and benchmarks employed\nfor the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches\ndesigned to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by\ncurrent RAG systems (§6) and delineate potential pathways for forthcoming research (§7).\n2\nDEFINITIONS\nFor the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a",
        "s emerged as a promising strategy to mitigate hallucina-\ntions and improve the factuality of LLM outputs [131, 165, 255, 277]. By incorporating large-scale\nexternal knowledge bases during inference, RAG equips LLMs with up-to-date knowledge, thus\nreducing the potential risk of hallucination due to the inherent knowledge boundaries of LLMs\n[260]. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still\nproduce hallucinations [16]. Hallucinations in RAG present con",
        "ucinations [16]. Hallucinations in RAG present considerable complexities, manifesting\nas outputs that are either factually inaccurate or misleading. These hallucinations occur when\nthe content generated by the LLM does not align with real-world facts, fails to accurately reflect\nthe user’s query, or is not supported by the retrieved information. Such hallucinations can stem\nfrom two primary factors: retrieval failure (§6.1) and generation bottleneck (§6.2). Through\na comprehensive analysis of th",
        " origin of LLM hallucinations is inherently tied to the model’s uncertainty.\nTherefore, by estimating the uncertainty of the factual content generated by the model, it becomes\nfeasible to detect hallucinations. The methodologies in uncertainty estimation can broadly be\ncategorized into two approaches: based on LLM internal states and LLM behavior, as shown in Fig. 2.\n• LLM internal states: The internal states of LLMs can serve as informative indicators of their\nuncertainty, often manifested thro",
        "-computer interaction and assistive technologies.\nResearch should explore how RAG systems can leverage BCI data to enhance user experience and\ngenerate context-aware responses.The integration of RAG with AR and VR technologies presents\nopportunities for creating immersive and interactive experiences. Future research should investigate how\nRAG models can be used to enhance AR and VR applications by providing contextually relevant\ninformation and interactions.\n7. Conclusion\nRetrieval-Augmented Gen"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.272153235688016,
        15.539968355726195,
        15.43803068923711
      ],
      "sources": [
        "2311.05232.pdf",
        "2311.05232.pdf",
        "2311.05232.pdf",
        "2311.05232.pdf",
        "2410.12837.pdf"
      ],
      "execution_time": 4.44486403465271,
      "k1": 2.0,
      "b": 0.9,
      "category": "evaluation",
      "query_id": 20,
      "difficulty": "hard"
    },
    {
      "question": "What is the Lost in the Middle problem in RAG?",
      "answer": "ks. RAG models generate more speciﬁc and factually accurate\nresponses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.\nTask\nInput\nModel\nGeneration\nMS-\nMARCO\ndeﬁne middle\near\nBART\n?The middle ear is the part of the ear between the middle ear and the nose.\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\nwhat currency\nneeded in\nscotland\nBART\nThe currency needed in \n\nro-\npriately (Wei et al., 2022). This approach has been\nsuccessful when all information needed to answer\nthe question is either provided as context (e.g., al-\ngebra questions) or assumed to be present in the\nmodel’s parameters (e.g., commonsense reasoning).\n1Code, data, and prompts are available at https://\ngithub.com/stonybrooknlp/ircot\nIn what country was \nLost Gravity manufactured?\nThe Lost Gravity was \nmanufactured by Mack Rides.\nMack Rides is a company \nfrom Germany.\nThe answer is Germany.\n\n\n an example,\nconsider the question illustrated in Fig. 1, “In what\ncountry was Lost Gravity manufactured?”. The\nWikipedia document retrieved using the question\n(in particular, the roller coaster Lost Gravity) as the\nquery does not mention where Lost Gravity was\nmanufactured. Instead, one must first infer that\nit was manufactured by a company called Mack\nRides, and then perform further retrieval, guided\nby the inferred company name, to obtain evidence\npointing to the manufacturing country.\nThus, ",
      "contexts": [
        "ks. RAG models generate more speciﬁc and factually accurate\nresponses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.\nTask\nInput\nModel\nGeneration\nMS-\nMARCO\ndeﬁne middle\near\nBART\n?The middle ear is the part of the ear between the middle ear and the nose.\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\nwhat currency\nneeded in\nscotland\nBART\nThe currency needed in ",
        "ro-\npriately (Wei et al., 2022). This approach has been\nsuccessful when all information needed to answer\nthe question is either provided as context (e.g., al-\ngebra questions) or assumed to be present in the\nmodel’s parameters (e.g., commonsense reasoning).\n1Code, data, and prompts are available at https://\ngithub.com/stonybrooknlp/ircot\nIn what country was \nLost Gravity manufactured?\nThe Lost Gravity was \nmanufactured by Mack Rides.\nMack Rides is a company \nfrom Germany.\nThe answer is Germany.\n",
        " an example,\nconsider the question illustrated in Fig. 1, “In what\ncountry was Lost Gravity manufactured?”. The\nWikipedia document retrieved using the question\n(in particular, the roller coaster Lost Gravity) as the\nquery does not mention where Lost Gravity was\nmanufactured. Instead, one must first infer that\nit was manufactured by a company called Mack\nRides, and then perform further retrieval, guided\nby the inferred company name, to obtain evidence\npointing to the manufacturing country.\nThus, ",
        "t Reasoning\nRetrieve(         )\nWikipedia Title: Mack Rides\nMack Rides GmbH & Co KG, also ...\nQ: In what country was\nLost Gravity manufactured?\nA: The Lost Gravity was manufactured by Mack\nRides. Mack Rides is a company from\nGermany. The answer is Germany.\n...\nWikipedia Title: Murray Head\nMurray Seafield St George Head ..\n...\nWikipedia Title: Most Beautifullest Hits\nThe Most Beautifullest Hits is ...\nQ: Who wrote the 1970 international hit ..\nA: The 1970 international hit song that\nMurray Head i",
        "ty, Florida. The population of Polk County in 2014 was\n15,140. So the answer is: 15,140.\nQ: Who was born first? Jan de Bont or Raoul Walsh?\nA: Jan de Bont was born on 22 October 1943. Raoul Walsh was born on March 11, 1887. Thus, Raoul Walsh was born the\nfirst. So the answer is: Raoul Walsh.\nQ: In what country was Lost Gravity manufactured?\nA: The Lost Gravity (roller coaster) was manufactured by Mack Rides. Mack Rides is a German company. So the answer is:\nGermany.\nQ: Which of the following had"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        20.10842964469913,
        16.992460364526277,
        16.932856027598046
      ],
      "sources": [
        "RAG_Lewis_2020.pdf",
        "2212.10509.pdf",
        "2212.10509.pdf",
        "2212.10509.pdf",
        "2212.10509.pdf"
      ],
      "execution_time": 4.511675596237183,
      "k1": 2.0,
      "b": 0.9,
      "category": "challenges",
      "query_id": 21,
      "difficulty": "hard"
    },
    {
      "question": "How does context length affect RAG performance?",
      "answer": "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: • (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? • (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? • (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? •\n(RQ4): What are the costs associated w\n\nrating continuous context and top-k retrieved chunks in\nstage-2 tuning.\n6.4\nAblation Studies on Inference Stage\nIn Table 7, we show ablation studies on how the number of retrieved context/chunks, context ordering,\nand different retrievers affect the conversational QA and RAG results.\nFirst, we find that using more contexts as inputs do not always improve the results. Utilizing top-5\ncontexts as input yields better results compared to using either top-3 or top-10 contexts. Intuitively,\nmore conte\n\nsm tends to\ntake effect at the begin of the keyphrase extraction process.\n5.3.3\nLength Penalty Factor. Fianlly, we investigate how the length\npenalty factor α affects the performance of the DivGraphPointer.\nResults with different values of length penalty factorα are presented\nin Table 6. Results show that the length penalty factors affect the\nmodel performance significantly. Either a small value of α (e.g.,\nα=0), which tends to generate long phrase, or a big value of α (e.g.,\nα = 100), which ten",
      "contexts": [
        "ical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: • (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? • (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? • (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? •\n(RQ4): What are the costs associated w",
        "rating continuous context and top-k retrieved chunks in\nstage-2 tuning.\n6.4\nAblation Studies on Inference Stage\nIn Table 7, we show ablation studies on how the number of retrieved context/chunks, context ordering,\nand different retrievers affect the conversational QA and RAG results.\nFirst, we find that using more contexts as inputs do not always improve the results. Utilizing top-5\ncontexts as input yields better results compared to using either top-3 or top-10 contexts. Intuitively,\nmore conte",
        "sm tends to\ntake effect at the begin of the keyphrase extraction process.\n5.3.3\nLength Penalty Factor. Fianlly, we investigate how the length\npenalty factor α affects the performance of the DivGraphPointer.\nResults with different values of length penalty factorα are presented\nin Table 6. Results show that the length penalty factors affect the\nmodel performance significantly. Either a small value of α (e.g.,\nα=0), which tends to generate long phrase, or a big value of α (e.g.,\nα = 100), which ten",
        " is it important to measure factual recall? . . . . . . . . . . . . . . . . . . .\n18\nA.7\nHow does the prompt postamble affect model responses? . . . . . . . . . . . . .\n18\nA.8 Why was LongFact-Concepts excluded from benchmarking? . . . . . . . . . . .\n19\nA.9\nHow can recall with human-preferred length be applied in other domains? . . . .\n20\nA.10 How does SAFE perform with respect to other humans? . . . . . . . . . . . . .\n20\nA.11 Is there canary text included in LongFact?\n. . . . . . . . . . . . ",
        "by-side\ncomparisons.\nExpertise QA\nIn Expertise QA, we engage with in-house experts that have formal training and\nexperience in various domains (e.g., history, literature, psychology). They produced hard, sometimes\ncomplex questions (e.g., As films began using sound for dialogue, how did the changing use of visual\nmetaphor affect the ways audiences expected narratives to develop in movies? or How does Vygotsky’s\ntheory of internal speech apply across multilingual language acquisition?).\nThe same "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        22.987506382727926,
        16.237130710756354,
        15.52891912979275
      ],
      "sources": [
        "2410.05779.pdf",
        "Graph_RAG_2024.pdf",
        "arxiv_1905.07689_2019.pdf",
        "2403.18802.pdf",
        "2403.05530.pdf"
      ],
      "execution_time": 4.164662837982178,
      "k1": 2.0,
      "b": 0.9,
      "category": "challenges",
      "query_id": 22,
      "difficulty": "medium"
    },
    {
      "question": "What are the common failure modes of RAG systems?",
      "answer": "ation was more pessimistic than a human rater for this domain.\nHowever, one threat to validity with this finding is that BioASQ is\na domain specific dataset and the reviewers were not experts i.e.\nthe large language model may know more than a non-expert.\n5\nFAILURE POINTS OF RAG SYSTEMS\nFrom the case studies we identified a set of failure points presented\nbelow. The following section addresses the research question What\nare the failure points that occur when engineering a RAG system?\nFP1 Missing \n\nreating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi\n\n Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n• What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n• A catalogue of failure points (FP) that occur in RAG systems.\n• An experience repo",
      "contexts": [
        "ation was more pessimistic than a human rater for this domain.\nHowever, one threat to validity with this finding is that BioASQ is\na domain specific dataset and the reviewers were not experts i.e.\nthe large language model may know more than a non-expert.\n5\nFAILURE POINTS OF RAG SYSTEMS\nFrom the case studies we identified a set of failure points presented\nbelow. The following section addresses the research question What\nare the failure points that occur when engineering a RAG system?\nFP1 Missing ",
        "reating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi",
        " Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n• What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n• A catalogue of failure points (FP) that occur in RAG systems.\n• An experience repo",
        ".1\nCan I reproduce your results? . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nA.2 Why does SAFE use “supported”, “irrelevant”, and “not supported” as labels? . .\n16\nA.3 What are the common causes of failure for SAFE? . . . . . . . . . . . . . . . .\n16\nA.4 What are the common causes of failure for human annotators? . . . . . . . . . .\n17\nA.5 Why is the “irrelevant” label not counted? . . . . . . . . . . . . . . . . . . . . .\n17\nA.6 Why is it important to measure factual recall? . . . ",
        "., adding additional facts or missing a vague reference). We also believe that these\nissues can be mitigated by using a more-capable language model. Examples of each cause of error are\nshown in Appendix C.3. In summary, most of the causes of error in SAFE can be remedied by using\na more-capable language model such as GPT-4, though we chose not to do so because GPT-3.5-Turbo\nstill achieves strong performance at a significantly-lower cost.\nA.4\nWhat are the common causes of failure for human annota"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        20.42073228399632,
        19.51840455493652,
        17.384968744945553
      ],
      "sources": [
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "execution_time": 4.458445072174072,
      "k1": 2.0,
      "b": 0.9,
      "category": "challenges",
      "query_id": 23,
      "difficulty": "hard"
    },
    {
      "question": "How can RAG systems handle multi-hop reasoning?",
      "answer": "-computer interaction and assistive technologies.\nResearch should explore how RAG systems can leverage BCI data to enhance user experience and\ngenerate context-aware responses.The integration of RAG with AR and VR technologies presents\nopportunities for creating immersive and interactive experiences. Future research should investigate how\nRAG models can be used to enhance AR and VR applications by providing contextually relevant\ninformation and interactions.\n7. Conclusion\nRetrieval-Augmented Gen\n\n-\nquired knowledge is not always available or up-to-\ndate in models’ parameters and it’s beneficial to\nretrieve knowledge from external sources (Lazari-\ndou et al., 2022; Kasai et al., 2022).\nHow can we augment chain-of-thought prompt-\ning for open-domain, knowledge-intensive tasks\nthat require complex, multi-step reasoning?\nWhile a one-shot retrieval from a knowledge\nsource based solely on the question can success-\nfully augment LMs with relevant knowledge for\nmany factoid-based tasks (Lewis et\n\ny, operating costs, and\nrobustness.\n6.3\nTesting and Monitoring RAG systems\nSoftware engineering best practices are still emerging for RAG sys-\ntems. Software testing and test case generation are one of the areas\nfor refinement. RAG systems require questions and answers that are\napplication specific often unavailable when indexing unstructured\ndocuments. Emerging work has considered using LLMs for gen-\nerating questions from multiple documents [4]. How to generate\nrealistic domain relevant questi",
      "contexts": [
        "-computer interaction and assistive technologies.\nResearch should explore how RAG systems can leverage BCI data to enhance user experience and\ngenerate context-aware responses.The integration of RAG with AR and VR technologies presents\nopportunities for creating immersive and interactive experiences. Future research should investigate how\nRAG models can be used to enhance AR and VR applications by providing contextually relevant\ninformation and interactions.\n7. Conclusion\nRetrieval-Augmented Gen",
        "-\nquired knowledge is not always available or up-to-\ndate in models’ parameters and it’s beneficial to\nretrieve knowledge from external sources (Lazari-\ndou et al., 2022; Kasai et al., 2022).\nHow can we augment chain-of-thought prompt-\ning for open-domain, knowledge-intensive tasks\nthat require complex, multi-step reasoning?\nWhile a one-shot retrieval from a knowledge\nsource based solely on the question can success-\nfully augment LMs with relevant knowledge for\nmany factoid-based tasks (Lewis et",
        "y, operating costs, and\nrobustness.\n6.3\nTesting and Monitoring RAG systems\nSoftware engineering best practices are still emerging for RAG sys-\ntems. Software testing and test case generation are one of the areas\nfor refinement. RAG systems require questions and answers that are\napplication specific often unavailable when indexing unstructured\ndocuments. Emerging work has considered using LLMs for gen-\nerating questions from multiple documents [4]. How to generate\nrealistic domain relevant questi",
        "-tuned for a wide range of tasks, including\nquestion-answering, summarization, and dialogue generation. By integrating retrieval with generation,\nT5-based RAG models have been shown to outperform traditional generative models like GPT-3 and\nBART on several benchmarks, including the Natural Questions dataset and the TriviaQA dataset.\nMoreover, T5's ability to handle complex multi-task learning makes it a popular choice for RAG systems\nthat need to tackle a diverse range of knowledge-intensive tas",
        "ters and how they work can help someone learn how to\nstart a car without a key. 2. Ignition systems: The ignition system is responsible\nfor generating the spark that ignites the fuel in the engine. There are several\ntypes of ignition systems, including distributor-based systems and coil-on-plug\nsystems. Understanding how the ignition system works can help someone\nlearn how to start a car without a key. 3. Battery and electrical systems: The\nbattery and electrical systems in a car are responsible"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.872939206456277,
        14.951883554869157,
        14.660334693752562
      ],
      "sources": [
        "2410.12837.pdf",
        "2212.10509.pdf",
        "2401.05856.pdf",
        "2410.12837.pdf",
        "2307.09288.pdf"
      ],
      "execution_time": 4.195443630218506,
      "k1": 2.0,
      "b": 0.9,
      "category": "challenges",
      "query_id": 24,
      "difficulty": "hard"
    },
    {
      "question": "What are the scalability challenges in RAG systems?",
      "answer": "ual information access for\nglobal inclusivity.\n5. Current Challenges and Limitations in Retrieval-Augmented Generation (RAG):\nThis section intends to highlight the current challenges and limitations of RAG considering the current\nlandscape of the system and this would shape the future research directions in the field.\nScalability and Efficiency: One of the primary challenges for RAG models is scalability. As retrieval\ncomponents rely on external databases, handling vast and dynamically growing d\n\n Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n• What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n• A catalogue of failure points (FP) that occur in RAG systems.\n• An experience repo\n\nnd required resources using RAG without training or fine-\ntuning is an attractive proposition. However, challenges arise when\nusing large language models for information extraction such as\nperformance with long text [8].\nA recent survey [19] showed that large language models are\nused across the RAG pipeline including retriever, data generation,\nrewriter, and reader. Our work complements this survey by taking\na software engineering perspective to shine a light on what issues\nengineers will face a",
      "contexts": [
        "ual information access for\nglobal inclusivity.\n5. Current Challenges and Limitations in Retrieval-Augmented Generation (RAG):\nThis section intends to highlight the current challenges and limitations of RAG considering the current\nlandscape of the system and this would shape the future research directions in the field.\nScalability and Efficiency: One of the primary challenges for RAG models is scalability. As retrieval\ncomponents rely on external databases, handling vast and dynamically growing d",
        " Manual inspection (all discrepancies, all flagged as\nincorrect, and a sample of correct labels) was analysed to\nidentify the patterns.\n• What are the key considerations when engineering a RAG\nsystem? (section 6) We present the lessons learned from three\ncase studies involving the implementation of a RAG system.\nThis presents the challenges faced and insights gained.\nContributions arising from this work include:\n• A catalogue of failure points (FP) that occur in RAG systems.\n• An experience repo",
        "nd required resources using RAG without training or fine-\ntuning is an attractive proposition. However, challenges arise when\nusing large language models for information extraction such as\nperformance with long text [8].\nA recent survey [19] showed that large language models are\nused across the RAG pipeline including retriever, data generation,\nrewriter, and reader. Our work complements this survey by taking\na software engineering perspective to shine a light on what issues\nengineers will face a",
        "RAG models. Concur-\nrently, state-of-the-art automated tools like RAGAS [164],\nARES [165], and TruLens8 employ LLMs to adjudicate the\nquality scores. These tools and benchmarks collectively form\na robust framework for the systematic evaluation of RAG\nmodels, as summarized in Table IV.\nVII. DISCUSSION AND FUTURE PROSPECTS\nDespite the considerable progress in RAG technology, sev-\neral challenges persist that warrant in-depth research.This\nchapter will mainly introduce the current challenges and fu",
        "RAG models. Concur-\nrently, state-of-the-art automated tools like RAGAS [164],\nARES [165], and TruLens8 employ LLMs to adjudicate the\nquality scores. These tools and benchmarks collectively form\na robust framework for the systematic evaluation of RAG\nmodels, as summarized in Table IV.\nVII. DISCUSSION AND FUTURE PROSPECTS\nDespite the considerable progress in RAG technology, sev-\neral challenges persist that warrant in-depth research.This\nchapter will mainly introduce the current challenges and fu"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        21.12664886272352,
        16.09186581142153,
        14.79342189237908
      ],
      "sources": [
        "2410.12837.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "execution_time": 4.428638458251953,
      "k1": 2.0,
      "b": 0.9,
      "category": "challenges",
      "query_id": 25,
      "difficulty": "medium"
    },
    {
      "question": "What is the optimal chunk size for RAG systems?",
      "answer": "uality contents.\nReACT [132] uses Chain-of-Thought (CoT) [133] to break\nqueries down for recursive retrieval and provide richer infor-\nmation. RATP [134] uses the Monte-Carlo Tree Search for\nsimulations to select optimal retrieval content, which is then\ntemplated and forwarded to the generator for output.\nChunk Optimization: Chunk optimization refers to adjusting\nchunk size for improved retrieval results.\nLlamaIndex [135] incorporates a series of chunk optimiza-\ntion methods, one of which operat\n\nl recall and reduce time\nbut may lack sufficient context.\nFinding the optimal chunk size involves a balance between some metrics such as faithfulness, and\nrelevancy. Faithfulness measures whether the response is hallucinated or matches the retrieved texts.\nChunk Size\nlyft_2021\nAverage\nFaithfulness\nAverage\nRelevancy\n2048\n80.37\n91.11\n1024\n94.26\n95.56\n512\n97.59\n97.41\n256\n97.22\n97.78\n128\n95.74\n97.22\nTable 3: Comparison of different chunk sizes.\nRelevancy measures whether the retrieved texts\nand resp\n\nhunks into the\ncontext to ask the LLM to generate an answer from the included\ncontext. This facilitates continuously updating the knowledge with\nnew documents and also gives the control over what chunks the user\nis able to access. However, optimal strategies for chunk embedding,\nretrieval, and contextual fusion remain active research. Further\nwork should systematically compare finetuning and RAG paradigms\nacross factors including accuracy, latency, operating costs, and\nrobustness.\n6.3\nTesting an",
      "contexts": [
        "uality contents.\nReACT [132] uses Chain-of-Thought (CoT) [133] to break\nqueries down for recursive retrieval and provide richer infor-\nmation. RATP [134] uses the Monte-Carlo Tree Search for\nsimulations to select optimal retrieval content, which is then\ntemplated and forwarded to the generator for output.\nChunk Optimization: Chunk optimization refers to adjusting\nchunk size for improved retrieval results.\nLlamaIndex [135] incorporates a series of chunk optimiza-\ntion methods, one of which operat",
        "l recall and reduce time\nbut may lack sufficient context.\nFinding the optimal chunk size involves a balance between some metrics such as faithfulness, and\nrelevancy. Faithfulness measures whether the response is hallucinated or matches the retrieved texts.\nChunk Size\nlyft_2021\nAverage\nFaithfulness\nAverage\nRelevancy\n2048\n80.37\n91.11\n1024\n94.26\n95.56\n512\n97.59\n97.41\n256\n97.22\n97.78\n128\n95.74\n97.22\nTable 3: Comparison of different chunk sizes.\nRelevancy measures whether the retrieved texts\nand resp",
        "hunks into the\ncontext to ask the LLM to generate an answer from the included\ncontext. This facilitates continuously updating the knowledge with\nnew documents and also gives the control over what chunks the user\nis able to access. However, optimal strategies for chunk embedding,\nretrieval, and contextual fusion remain active research. Further\nwork should systematically compare finetuning and RAG paradigms\nacross factors including accuracy, latency, operating costs, and\nrobustness.\n6.3\nTesting an",
        " sizes without a drop in quality\n(Figure 3) or the forced introduction of noise. We interate self-reflection steps up to a specified\nmaximum number of times.\n0\n1\n2\n3\n0\n10000\n20000\n30000\nNumber of self-reflection iterations performed\nEntity references detected\n600 chunk size\n1200 chunk size\n2400 chunk size\nFigure 3: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and self-reflection iterations for our generic entity extraction prompt with\ngpt-",
        " techniques such as small-to-big and sliding window improve retrieval quality by organizing\nchunk block relationships. Small-sized blocks are used to match queries, and larger blocks that\ninclude the small ones along with contextual information are returned.\nTo demonstrate the effectiveness of advanced chunking techniques, we use the LLM-Embedder [20]\nmodel as an embedding model. The smaller chunk size is 175 tokens, the larger chunk size is 512\ntokens and the chunk overlap is 20 tokens. Techniq"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        22.428433602189347,
        21.561637341007888,
        19.816299544859426
      ],
      "sources": [
        "2402.19473.pdf",
        "2407.01219.pdf",
        "2401.05856.pdf",
        "2404.16130.pdf",
        "2407.01219.pdf"
      ],
      "execution_time": 4.5042431354522705,
      "k1": 2.0,
      "b": 0.9,
      "category": "implementation",
      "query_id": 26,
      "difficulty": "medium"
    },
    {
      "question": "How should chunk overlap be configured in RAG?",
      "answer": " During\nthe Index process each document is split into smaller chunks that\nare converted into an embedding using an embedding model. The\noriginal chunk and the embedding are then indexed in a database.\nSoftware engineers face design decisions around how best to chunk\nthe document and how large a chunk should be. If chunks are too\nsmall certain questions cannot be answered, if the chunks are too\nlong then the answers include generated noise.\nDifferent types of documents require different chunking \n\n techniques such as small-to-big and sliding window improve retrieval quality by organizing\nchunk block relationships. Small-sized blocks are used to match queries, and larger blocks that\ninclude the small ones along with contextual information are returned.\nTo demonstrate the effectiveness of advanced chunking techniques, we use the LLM-Embedder [20]\nmodel as an embedding model. The smaller chunk size is 175 tokens, the larger chunk size is 512\ntokens and the chunk overlap is 20 tokens. Techniq\n\ntokens and the chunk overlap is 20 tokens. Techniques like small-to-big and sliding window improve\nretrieval quality by maintaining context and ensuring relevant information is retrieved. Detailed\nresults are shown in Table 4.\n3.2.3\nEmbedding Model Selection\nChoosing the right embedding model is crucial for effective semantic matching of queries\nand chunk blocks. We use the evaluation module of FlagEmbedding6 which uses the dataset\nChunk Skill\nlyft_2021\nAverage\nFaithfulness\nAverage\nRelevancy\nOri",
      "contexts": [
        " During\nthe Index process each document is split into smaller chunks that\nare converted into an embedding using an embedding model. The\noriginal chunk and the embedding are then indexed in a database.\nSoftware engineers face design decisions around how best to chunk\nthe document and how large a chunk should be. If chunks are too\nsmall certain questions cannot be answered, if the chunks are too\nlong then the answers include generated noise.\nDifferent types of documents require different chunking ",
        " techniques such as small-to-big and sliding window improve retrieval quality by organizing\nchunk block relationships. Small-sized blocks are used to match queries, and larger blocks that\ninclude the small ones along with contextual information are returned.\nTo demonstrate the effectiveness of advanced chunking techniques, we use the LLM-Embedder [20]\nmodel as an embedding model. The smaller chunk size is 175 tokens, the larger chunk size is 512\ntokens and the chunk overlap is 20 tokens. Techniq",
        "tokens and the chunk overlap is 20 tokens. Techniques like small-to-big and sliding window improve\nretrieval quality by maintaining context and ensuring relevant information is retrieved. Detailed\nresults are shown in Table 4.\n3.2.3\nEmbedding Model Selection\nChoosing the right embedding model is crucial for effective semantic matching of queries\nand chunk blocks. We use the evaluation module of FlagEmbedding6 which uses the dataset\nChunk Skill\nlyft_2021\nAverage\nFaithfulness\nAverage\nRelevancy\nOri",
        " sizes without a drop in quality\n(Figure 3) or the forced introduction of noise. We interate self-reflection steps up to a specified\nmaximum number of times.\n0\n1\n2\n3\n0\n10000\n20000\n30000\nNumber of self-reflection iterations performed\nEntity references detected\n600 chunk size\n1200 chunk size\n2400 chunk size\nFigure 3: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and self-reflection iterations for our generic entity extraction prompt with\ngpt-",
        "e\nPROMPTAGATOR [21]\nBEIR\nText\nChunk\nTuning\nOnce\nAAR [47]\nMSMARCO,Wikipedia\nText\nChunk\nTuning\nOnce\nRA-DIT [27]\nCommon Crawl,Wikipedia\nText\nChunk\nTuning\nOnce\nRAG-Robust [48]\nWikipedia\nText\nChunk\nTuning\nOnce\nRA-Long-Form [49]\nDataset-base\nText\nChunk\nTuning\nOnce\nCoN [50]\nWikipedia\nText\nChunk\nTuning\nOnce\nSelf-RAG [25]\nWikipedia\nText\nChunk\nTuning\nAdaptive\nBGM [26]\nWikipedia\nText\nChunk\nInference\nOnce\nCoQ [51]\nWikipedia\nText\nChunk\nInference\nIterative\nToken-Elimination [52]\nWikipedia\nText\nChunk\nInference"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        20.071030755837658,
        16.62004490211241,
        16.31447793705914
      ],
      "sources": [
        "2401.05856.pdf",
        "2407.01219.pdf",
        "2407.01219.pdf",
        "2404.16130.pdf",
        "2312.10997.pdf"
      ],
      "execution_time": 4.390605926513672,
      "k1": 2.0,
      "b": 0.9,
      "category": "implementation",
      "query_id": 27,
      "difficulty": "medium"
    },
    {
      "question": "What embedding models are best for RAG retrieval?",
      "answer": "r\nsmall text.\nOpensource sentence embedding models performed\nas well as closed source alternatives on small text.\nBioASQ, AI Tutor\nFP2-7\nRAG systems require continuous calibration.\nRAG systems receive unknown input at runtime\nrequiring constant monitoring.\nAI Tutor, BioASQ\nFP1, FP2\nImplement a RAG pipeline for configuration.\nA RAG system requires calibrating chunk size,\nembedding strategy, chunking strategy, retrieval\nstrategy, consolidation strategy, context size, and\nprompts.\nCognitive Reviewe\n\n perform best at the smallest learning rate. The\nxLSTM[0:1] reaches similar performance across all learning rates.\nWikitext-103 Rare Token Prediction.\nFor this exemplary experiment on rare token prediction, we\ntrained 125M-sized models on Wikitext-103 (Merity et al., 2017). All models have an embedding\ndimension of 768 in a post up-projection structure of 12 residual blocks. The Transformer model\n(Llama) uses Multi-Head Attention, for what is called LSTM the Multi-Head Attention is replaced by\na\n\nalmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. oLMpics– on what language\nmodel pre-training captures. Transactions of the Association for Computational Linguistics, 8:\n743–758, 2020. URL https://arxiv.org/abs/1912.13283.\nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,\nOleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models\nwith retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762, 2023. URL https:",
      "contexts": [
        "r\nsmall text.\nOpensource sentence embedding models performed\nas well as closed source alternatives on small text.\nBioASQ, AI Tutor\nFP2-7\nRAG systems require continuous calibration.\nRAG systems receive unknown input at runtime\nrequiring constant monitoring.\nAI Tutor, BioASQ\nFP1, FP2\nImplement a RAG pipeline for configuration.\nA RAG system requires calibrating chunk size,\nembedding strategy, chunking strategy, retrieval\nstrategy, consolidation strategy, context size, and\nprompts.\nCognitive Reviewe",
        " perform best at the smallest learning rate. The\nxLSTM[0:1] reaches similar performance across all learning rates.\nWikitext-103 Rare Token Prediction.\nFor this exemplary experiment on rare token prediction, we\ntrained 125M-sized models on Wikitext-103 (Merity et al., 2017). All models have an embedding\ndimension of 768 in a post up-projection structure of 12 residual blocks. The Transformer model\n(Llama) uses Multi-Head Attention, for what is called LSTM the Multi-Head Attention is replaced by\na",
        "almor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. oLMpics– on what language\nmodel pre-training captures. Transactions of the Association for Computational Linguistics, 8:\n743–758, 2020. URL https://arxiv.org/abs/1912.13283.\nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,\nOleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models\nwith retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762, 2023. URL https:",
        " Tutor\nFP5-7\nJailbreaks bypass the RAG system and hit the safety\ntraining.\nResearch suggests fine-tuning LLMs reverses safety\ntraining [11], test all fine-tuned LLMs for RAG sys-\ntem.\nAI Tutor\nFP2, FP4\nAdding meta-data improves retrieval.\nAdding the file name and chunk number into the\nretrieved context helped the reader extract the re-\nquired information. Useful for chat dialogue.\nAI Tutor\nFP2, FP4-7\nOpen source embedding models perform better for\nsmall text.\nOpensource sentence embedding models",
        "y, operating costs, and\nrobustness.\n6.3\nTesting and Monitoring RAG systems\nSoftware engineering best practices are still emerging for RAG sys-\ntems. Software testing and test case generation are one of the areas\nfor refinement. RAG systems require questions and answers that are\napplication specific often unavailable when indexing unstructured\ndocuments. Emerging work has considered using LLMs for gen-\nerating questions from multiple documents [4]. How to generate\nrealistic domain relevant questi"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        14.958211400472843,
        14.806829267128855,
        14.753637491857017
      ],
      "sources": [
        "2401.05856.pdf",
        "2405.04517.pdf",
        "2401.18059.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf"
      ],
      "execution_time": 4.417142629623413,
      "k1": 2.0,
      "b": 0.9,
      "category": "implementation",
      "query_id": 28,
      "difficulty": "medium"
    },
    {
      "question": "How do you choose the number of retrieved documents (top-k)?",
      "answer": "augmented with random documents,\ndocuments retrieved by REPLUG, and documents retrieved\nby REPLUG LSR. We observed that ensembling random\ndocuments leads to worse performance, indicating that the\nperformance gains of REPLUG do not solely come from the\nensembling effect. Instead, ensembling the relevant docu-\nments is crucial for the success of REPLUG. Additionally,\nas more documents were ensembled, the performance of\nREPLUG and REPLUG LSR improved monotonically. How-\never, a small number of docu\n\ndate RAG’s world knowledge by simply replacing its non-parametric memory.\nEffect of Retrieving more documents\nModels are trained with either 5 or 10 retrieved latent\ndocuments, and we do not observe signiﬁcant differences in performance between them. We have the\nﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\nOpen-domain QA results for RAG-Sequen\n\now frequently the\ncorrect answer is returned and, as the correct an-\n(a) Recall\n(b) Avg number of\nrelevant documents\n0.2\n0.4\n0.6\n0.8\n1.0\n2\n4\n6\n8\n10\ntop−n retrieved documents\nrecall\n1\n2\n3\n4\n5\n2\n4\n6\n8\n10\ntop−n retrieved documents\navg. number relevant documents\nFigure 2: Recall (a) and average number of rel-\nevant documents (b) for growing top-n conﬁgu-\nrations and a static corpus size (full Wikipedia\ndump). While the recall is converging the number\nof relevant documents keeps growing resulting in\n",
      "contexts": [
        "augmented with random documents,\ndocuments retrieved by REPLUG, and documents retrieved\nby REPLUG LSR. We observed that ensembling random\ndocuments leads to worse performance, indicating that the\nperformance gains of REPLUG do not solely come from the\nensembling effect. Instead, ensembling the relevant docu-\nments is crucial for the success of REPLUG. Additionally,\nas more documents were ensembled, the performance of\nREPLUG and REPLUG LSR improved monotonically. How-\never, a small number of docu",
        "date RAG’s world knowledge by simply replacing its non-parametric memory.\nEffect of Retrieving more documents\nModels are trained with either 5 or 10 retrieved latent\ndocuments, and we do not observe signiﬁcant differences in performance between them. We have the\nﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\nOpen-domain QA results for RAG-Sequen",
        "ow frequently the\ncorrect answer is returned and, as the correct an-\n(a) Recall\n(b) Avg number of\nrelevant documents\n0.2\n0.4\n0.6\n0.8\n1.0\n2\n4\n6\n8\n10\ntop−n retrieved documents\nrecall\n1\n2\n3\n4\n5\n2\n4\n6\n8\n10\ntop−n retrieved documents\navg. number relevant documents\nFigure 2: Recall (a) and average number of rel-\nevant documents (b) for growing top-n conﬁgu-\nrations and a static corpus size (full Wikipedia\ndump). While the recall is converging the number\nof relevant documents keeps growing resulting in\n",
        "he precision.\nThe parallel search scheme is formally described in Algorithm 1.\nAlgorithm 1 Parallel Search Scheme\nInput: query q, number of initial candidate documents n, number of candi-\ndate documents retrieved by symbolic search m\nOutput: n initial candidate documents\nM ←m documents based on the inverted index and BM25\nC ←n documents based on the undirected k-NN graph index and cosine\nsimilarity\nfor each d ∈C do\nif d ∈M then\ncontinue\nend if\nM ←M ∪{d }\nif |M | == n then\nbreak\nend if\nend for\nre",
        "A Pairs\nLDA+\nTranslation1\nTranslation2\nHow many days a week\nshould you lift weight?\nWhen are you to old to\nbuild muscle mass from\nwork out?\nHow\ndo\nI\ngain\nbody\nweight?\nMy\nweight\nis\n90lb\nhow could I gain more\nweight?\nWhat is a 1 set rep?\nMy\nweight\nis\n90lb\nhow could I gain more\nweight?\nWill lose weight faster\nthan average if I work-\nout?\nCan you make your hair\ngrow faster\nIs there any any way to\nget rid of razor bump?\nWhat important func-\ntion do our body hair\nplay?\nHow\ncan\nI\nmake\nmy\nmustache grow "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        18.607209971380556,
        17.654864368289758,
        17.580522212367036
      ],
      "sources": [
        "REPLUG_2023.pdf",
        "RAG_Lewis_2020.pdf",
        "arxiv_1808.06528_2018.pdf",
        "arxiv_1806.10869_2018.pdf",
        "arxiv_1606.03783_2016.pdf"
      ],
      "execution_time": 4.4740891456604,
      "k1": 2.0,
      "b": 0.9,
      "category": "implementation",
      "query_id": 29,
      "difficulty": "medium"
    },
    {
      "question": "What vector databases are commonly used for RAG?",
      "answer": "n distributional feature they are referred\nto as explicit vector representations [117]. On the other hand, when the vectors are dense, small\n(k ≪|T|), and learnt from data then they are commonly referred to as embeddings. For both explicit\nand embedding based representations several distance metrics can be used to deﬁne similarity between\nterms, although cosine similarity is commonly used.\nsim(⃗vi,⃗vj) = cos(⃗vi,⃗vj) =\n⃗v ⊺\ni ⃗vj\n∥⃗vi∥∥⃗vj∥\n(23)\nMost embeddings are learnt from explicit vector sp\n\nler\nthan that of the latter.\nThus, we select the\nLLM-Embedder [20] for its balance of\nperformance and size.\n3.2.4\nMetadata Addition\nEnhancing chunk blocks with metadata like titles, keywords, and hypothetical questions can improve\nretrieval, provide more ways to post-process retrieved texts, and help LLMs better understand\nretrieved information. A detailed study on metadata inclusion will be addressed in future work.\n3.3\nVector Databases\nVector databases store embedding vectors with their metada\n\n1\nScalable Probabilistic Similarity Ranking in\nUncertain Databases (Technical Report)\nThomas Bernecker, Hans-Peter Kriegel, Nikos Mamoulis, Matthias Renz,\nand Andreas Zueﬂe\nAbstract\nThis paper introduces a scalable approach for probabilistic top-k similarity ranking on uncertain vector data.\nEach uncertain object is represented by a set of vector instances that are assumed to be mutually-exclusive. The\nobjective is to rank the uncertain data according to their distance to a reference object. We ",
      "contexts": [
        "n distributional feature they are referred\nto as explicit vector representations [117]. On the other hand, when the vectors are dense, small\n(k ≪|T|), and learnt from data then they are commonly referred to as embeddings. For both explicit\nand embedding based representations several distance metrics can be used to deﬁne similarity between\nterms, although cosine similarity is commonly used.\nsim(⃗vi,⃗vj) = cos(⃗vi,⃗vj) =\n⃗v ⊺\ni ⃗vj\n∥⃗vi∥∥⃗vj∥\n(23)\nMost embeddings are learnt from explicit vector sp",
        "ler\nthan that of the latter.\nThus, we select the\nLLM-Embedder [20] for its balance of\nperformance and size.\n3.2.4\nMetadata Addition\nEnhancing chunk blocks with metadata like titles, keywords, and hypothetical questions can improve\nretrieval, provide more ways to post-process retrieved texts, and help LLMs better understand\nretrieved information. A detailed study on metadata inclusion will be addressed in future work.\n3.3\nVector Databases\nVector databases store embedding vectors with their metada",
        "1\nScalable Probabilistic Similarity Ranking in\nUncertain Databases (Technical Report)\nThomas Bernecker, Hans-Peter Kriegel, Nikos Mamoulis, Matthias Renz,\nand Andreas Zueﬂe\nAbstract\nThis paper introduces a scalable approach for probabilistic top-k similarity ranking on uncertain vector data.\nEach uncertain object is represented by a set of vector instances that are assumed to be mutually-exclusive. The\nobjective is to rank the uncertain data according to their distance to a reference object. We ",
        "agnose bladder cancer?\nPersonA: Yes, urinalysis can be used as a screening and diagnostic tool for\nbladder cancer. One way to test for bladder cancer is to check for blood in the\nurine, which is called hematuria. This can be done during a urinalysis by examining\nthe urine cells under a special microscope.\nPersonU: What other conditions or diseases can be detected through urinalysis?\n23\nPersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections,\nevaluate causes of kidney ",
        "gates the contribution of\neach component and provides insights into optimal RAG practices through extensive experimentation.\nThe optional methods considered for each component are indicated in bold fonts, while the methods\nunderlined indicate the default choice for individual modules. The methods indicated in blue font\ndenote the best-performing selections identified empirically.\nvector databases to efficiently store feature representations, and the methods for effectively fine-tuning\nLLMs (see "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        19.432116675387466,
        15.741084935074522,
        14.281018030723942
      ],
      "sources": [
        "arxiv_1705.01509_2017.pdf",
        "2407.01219.pdf",
        "0907.2868.pdf",
        "Graph_RAG_2024.pdf",
        "2407.01219.pdf"
      ],
      "execution_time": 4.387493371963501,
      "k1": 2.0,
      "b": 0.9,
      "category": "implementation",
      "query_id": 30,
      "difficulty": "easy"
    },
    {
      "question": "Compare BM25 and dense vector retrieval methods",
      "answer": "isms are commonly used, ranging from traditional methods like BM25 to more sophisticated\ntechniques like Dense Passage Retrieval (DPR).\n2.2.1 BM25\nBM25 is a well-established information retrieval algorithm that uses the term frequency-inverse document\nfrequency (TF-IDF) to rank documents according to relevance. Despite being a classical method, BM25\nremains a strong baseline for many modern retrieval systems, including those used in RAG models.\nBM25 calculates the relevance score of a document b\n\nle implement is HyDE [10],\nRecent studies, such as [44], indicate that combining lexical-based search with vector search signifi-\ncantly enhances performance. In this study, we use BM25 for sparse retrieval and Contriever [45], an\nunsupervised contrastive encoder, for dense retrieval, serving as two robust baselines based on Thakur\net al. [46].\n3.4.1\nResults for different retrieval methods\nWe evaluated the performance of different search methods on the TREC DL 2019 and 2020 passage\nranking datas\n\nestions\nand CuratedTrec).\n8\nMain Results\n8.1\nBaselines\nWe compare against other retrieval methods by us-\ning alternate retrieval scores Sretr(b, q), but with\nthe same reader.\nBM25\nA de-facto state-of-the-art unsupervised\nretrieval method is BM25 (Robertson et al., 2009).\nIt has been shown to be robust for both traditional\ninformation retrieval tasks, and evidence retrieval\nfor question answering (Yang et al., 2017).2 Since\n2We also include the title, which was slightly beneﬁcial.\nModel\nBM25\nNNLM",
      "contexts": [
        "isms are commonly used, ranging from traditional methods like BM25 to more sophisticated\ntechniques like Dense Passage Retrieval (DPR).\n2.2.1 BM25\nBM25 is a well-established information retrieval algorithm that uses the term frequency-inverse document\nfrequency (TF-IDF) to rank documents according to relevance. Despite being a classical method, BM25\nremains a strong baseline for many modern retrieval systems, including those used in RAG models.\nBM25 calculates the relevance score of a document b",
        "le implement is HyDE [10],\nRecent studies, such as [44], indicate that combining lexical-based search with vector search signifi-\ncantly enhances performance. In this study, we use BM25 for sparse retrieval and Contriever [45], an\nunsupervised contrastive encoder, for dense retrieval, serving as two robust baselines based on Thakur\net al. [46].\n3.4.1\nResults for different retrieval methods\nWe evaluated the performance of different search methods on the TREC DL 2019 and 2020 passage\nranking datas",
        "estions\nand CuratedTrec).\n8\nMain Results\n8.1\nBaselines\nWe compare against other retrieval methods by us-\ning alternate retrieval scores Sretr(b, q), but with\nthe same reader.\nBM25\nA de-facto state-of-the-art unsupervised\nretrieval method is BM25 (Robertson et al., 2009).\nIt has been shown to be robust for both traditional\ninformation retrieval tasks, and evidence retrieval\nfor question answering (Yang et al., 2017).2 Since\n2We also include the title, which was slightly beneﬁcial.\nModel\nBM25\nNNLM",
        "ge queries that require an understanding of context.\nDespite this limitation, BM25 is still widely used because of its simplicity and efficiency. BM25 is effective\nfor tasks involving simpler, keyword-based queries, although more modern retrieval models like DPR tend\nto outperform it in semantically complex tasks.\n2.2.2 Dense Passage Retrieval (DPR)\nDense Passage Retrieval (DPR), introduced by Karpukhin et al. (2020), represents a more modern\napproach to information retrieval. It uses a dense ve",
        " in Appendix.\nWe use E5-large (Wang et al. 2022) as the main\nretriever in our experiments. The impact of other re-\ntrievers, i.e., BM25 (Robertson and Zaragoza 2009),\nBGE-base (Xiao et al. 2023), and E5-base, is studied in\nour further analysis. Among these retrievers, BM25 is a non-\nneural sparse retrieval algorithm, while others are neural-\nbased dense retrievers. In general, dense retrievers perform\nbetter on several benchmarks (Muennighoff et al. 2023).\nBaseline Methods\nWe consider both the b"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        21.01162981467443,
        20.561642349693265,
        20.325870359332058
      ],
      "sources": [
        "2410.12837.pdf",
        "2407.01219.pdf",
        "arxiv_1906.00300_2019.pdf",
        "2410.12837.pdf",
        "2405.19670.pdf"
      ],
      "execution_time": 4.426665306091309,
      "k1": 2.0,
      "b": 0.9,
      "category": "comparison",
      "query_id": 31,
      "difficulty": "medium"
    },
    {
      "question": "When should you use Naive RAG vs Advanced RAG?",
      "answer": "hree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to\n\nhree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to\n\nhree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to",
      "contexts": [
        "hree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to",
        "hree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to",
        "hree stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2. A representative instance of the RAG process applied to",
        "loys pre-retrieval and post-retrieval strate-\ngies. To tackle the indexing issues, Advanced RAG refines\nits indexing techniques through the use of a sliding window\napproach, fine-grained segmentation, and the incorporation of\nmetadata. Additionally, it incorporates several optimization\nmethods to streamline the retrieval process [8].\n4\nFig. 3.\nComparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\nAdvanced RAG ",
        "loys pre-retrieval and post-retrieval strate-\ngies. To tackle the indexing issues, Advanced RAG refines\nits indexing techniques through the use of a sliding window\napproach, fine-grained segmentation, and the incorporation of\nmetadata. Additionally, it incorporates several optimization\nmethods to streamline the retrieval process [8].\n4\nFig. 3.\nComparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\nAdvanced RAG "
      ],
      "retrieved_docs": 5,
      "top_scores": [
        25.722288910952244,
        25.722288910952244,
        25.722288910952244
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "execution_time": 4.199602365493774,
      "k1": 2.0,
      "b": 0.9,
      "category": "comparison",
      "query_id": 32,
      "difficulty": "medium"
    },
    {
      "question": "Compare Self-RAG with Corrective RAG approaches",
      "answer": " SelfRAG-LLaMA2-7b\nPopQA\nCRAG\n54.9\n59.8\nRAG\n50.5\n52.8\nRAG w. web\n52.2\n53.8\nSelf-CRAG\n49.0\n61.8\nSelf-RAG\n29.0\n54.9\nSelf-RAG w. web\n24.9\n57.9\nTable 5: Comparison results between CRAG, Self-\nCRAG and RAG, Self-RAG with the same input in\nterms of accuracy.\nretrieval performance. A part of accurate retrieval\nresults were deliberately removed at random to\nimitate a low-quality retriever and evaluate how\nthe performance changed. Figure 3 demonstrated\nthe performance change of Self-RAG and Self-\nCRAG on\n\n RAG or Self-RAG with web search\nknowledge can improve the performance in most\ncases (except Self-RAG w. web using the original\nLLaMA2 model), though the improvement remains\nlimited. Furthermore, augmenting RAG or Self-\nRAG with the proposed self-correction mechanism\nsignificantly outperformed the models consistently\nsupplemented with web search knowledge in all\ncases. This finding confirms that the observed\nadvancements are primarily attributable to the\nproposed self-correction mechanism.\n5.8\nC\n\nnately. With\nthe further leverage of web search and optimized\nknowledge utilization, CRAG has significantly\nimproved the ability of automatic self-correction\nand efficient utilization of retrieved documents.\nExperiments extensively demonstrate its adaptabil-\nity to RAG-based approaches as well as general-\nizability across short- and long-form generation\ntasks. While we primarily proposed to improve the\nRAG framework from a corrective perspective and\nCRAG can be seamlessly coupled with various\nRA",
      "contexts": [
        " SelfRAG-LLaMA2-7b\nPopQA\nCRAG\n54.9\n59.8\nRAG\n50.5\n52.8\nRAG w. web\n52.2\n53.8\nSelf-CRAG\n49.0\n61.8\nSelf-RAG\n29.0\n54.9\nSelf-RAG w. web\n24.9\n57.9\nTable 5: Comparison results between CRAG, Self-\nCRAG and RAG, Self-RAG with the same input in\nterms of accuracy.\nretrieval performance. A part of accurate retrieval\nresults were deliberately removed at random to\nimitate a low-quality retriever and evaluate how\nthe performance changed. Figure 3 demonstrated\nthe performance change of Self-RAG and Self-\nCRAG on",
        " RAG or Self-RAG with web search\nknowledge can improve the performance in most\ncases (except Self-RAG w. web using the original\nLLaMA2 model), though the improvement remains\nlimited. Furthermore, augmenting RAG or Self-\nRAG with the proposed self-correction mechanism\nsignificantly outperformed the models consistently\nsupplemented with web search knowledge in all\ncases. This finding confirms that the observed\nadvancements are primarily attributable to the\nproposed self-correction mechanism.\n5.8\nC",
        "nately. With\nthe further leverage of web search and optimized\nknowledge utilization, CRAG has significantly\nimproved the ability of automatic self-correction\nand efficient utilization of retrieved documents.\nExperiments extensively demonstrate its adaptabil-\nity to RAG-based approaches as well as general-\nizability across short- and long-form generation\ntasks. While we primarily proposed to improve the\nRAG framework from a corrective perspective and\nCRAG can be seamlessly coupled with various\nRA",
        "e, the\nretrieval and data-processing stages are not included.\nanism, rather than solely from the supplementary\ninformation obtained through web searches. To\nfurther demonstrate the effectiveness of the pro-\nposed self-correction mechanism, both RAG and\nSelf-RAG were consistently supplemented with\nweb search knowledge to ensure they had access\nto the same scope of the retrieved knowledge.\nThe results in Table 5 show that consistently\nsupplementing RAG or Self-RAG with web search\nknowledge can imp",
        "d answers as input-output text pairs (x, y)\nand train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\nthe popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved\ndocuments, relying primarily on non-parametric knowledge. We also compare to “Closed-Book\nQA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead\nrelying purely on parametric knowledge. We consider four popular open-d"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.978548384938943,
        17.59493650071206,
        17.169053168652564
      ],
      "sources": [
        "Corrective_RAG_2024.pdf",
        "Corrective_RAG_2024.pdf",
        "Corrective_RAG_2024.pdf",
        "Corrective_RAG_2024.pdf",
        "RAG_Lewis_2020.pdf"
      ],
      "execution_time": 4.274762868881226,
      "k1": 2.0,
      "b": 0.9,
      "category": "comparison",
      "query_id": 33,
      "difficulty": "hard"
    },
    {
      "question": "What are the trade-offs between hybrid and pure dense retrieval?",
      "answer": "asing the number of hypothetical\ndocuments does not yield significant benefits and substantially raises latency, indicating that using a\nsingle hypothetical document is sufficient.\n3.4.3\nHybrid Search with Different Weight on Sparse Retrieval\nTable 8 presents the impact of different α values in hybrid search, where α controls the weighting\nbetween sparse retrieval and dense retrieval components. The relevance score is calculated as follows:\nSh = α · Ss + Sd\n(1)\nwhere Ss, Sd are the normalized re\n\n of con-\ntext length up to 256K tokens and also achieves comparable\nperformance with Mixtral-8x7B [266] and Llama-2 70B [225].\nThe study on building pure Mamba or hybrid architectures\nwill be a promising direction for pre-trained big models.\n• Multi-modal learning using SSMs architecture. Early\nmulti-modal related works focused on how to learn\nmodality-specific and modality-shared representations. In-\nfluenced by the Transformer network, current multi-modal\nalgorithms usually directly encode and\n\ndel. In Proceedings of BigScience\nEpisode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models,\npp. 95–136, virtual+Dublin, 2022. Association for Computational Linguistics. doi: 10.18653/\nv1/2022.bigscience-1.9. URL https://aclanthology.org/2022.bigscience-1.9.\n[25] Burton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Communications\nof the ACM, 1970. https://doi.org/10.1145/362686.362692.\n[26] Burton H Bloom. Space/time trade-offs in hash coding wi",
      "contexts": [
        "asing the number of hypothetical\ndocuments does not yield significant benefits and substantially raises latency, indicating that using a\nsingle hypothetical document is sufficient.\n3.4.3\nHybrid Search with Different Weight on Sparse Retrieval\nTable 8 presents the impact of different α values in hybrid search, where α controls the weighting\nbetween sparse retrieval and dense retrieval components. The relevance score is calculated as follows:\nSh = α · Ss + Sd\n(1)\nwhere Ss, Sd are the normalized re",
        " of con-\ntext length up to 256K tokens and also achieves comparable\nperformance with Mixtral-8x7B [266] and Llama-2 70B [225].\nThe study on building pure Mamba or hybrid architectures\nwill be a promising direction for pre-trained big models.\n• Multi-modal learning using SSMs architecture. Early\nmulti-modal related works focused on how to learn\nmodality-specific and modality-shared representations. In-\nfluenced by the Transformer network, current multi-modal\nalgorithms usually directly encode and",
        "del. In Proceedings of BigScience\nEpisode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models,\npp. 95–136, virtual+Dublin, 2022. Association for Computational Linguistics. doi: 10.18653/\nv1/2022.bigscience-1.9. URL https://aclanthology.org/2022.bigscience-1.9.\n[25] Burton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Communications\nof the ACM, 1970. https://doi.org/10.1145/362686.362692.\n[26] Burton H Bloom. Space/time trade-offs in hash coding wi",
        "OPS\n0.3\n0.4\n0.5\n0.6\nMMLU 5-shot accuracy\nDCLM-Baseline\nC4\nDolma v1\nFalcon-7B\nFineWeb edu\nLLM360/CrystalCoder\nMAP-Neo-7B\nMPT-7B\nOLMo-1B\nOLMo-7B\nOLMo-1.7-7B\nRedPajama\nRefinedWeb\nTogether-RPJ-7B\nDeepSeek\nGemma-2B\nGemma-7B\nLlama1-7B\nLlama2-7B\nLlama3-8B\nFigure 1: Improving training sets leads to better models that are cheaper to train. Using\nDataComp-LM, we develop a high-quality dataset, DCLM-BASELINE, which we use to train models\nwith state-of-the-art trade-offs between compute and performance. We ",
        "AG has also seen steep\nadoption across various applications. However, there is a gap in a sufficient survey of this space tracking\nthe evolution and recent changes in this space. The current survey intends to fill this gap.\n1.2 Overview of Retrieval-Augmented Generation (RAG)\nRetrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address the\nlimitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism,\nwhich retrieves relevant d"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.292786689450715,
        13.395508785219251,
        13.259964439736617
      ],
      "sources": [
        "2407.01219.pdf",
        "2404.09516.pdf",
        "2406.11794.pdf",
        "2406.11794.pdf",
        "2410.12837.pdf"
      ],
      "execution_time": 4.534531354904175,
      "k1": 2.0,
      "b": 0.9,
      "category": "comparison",
      "query_id": 34,
      "difficulty": "medium"
    },
    {
      "question": "Compare single-stage vs multi-stage retrieval in RAG",
      "answer": "easily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\nD. RAG vs Fine-tuning\nThe augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt \n\neasily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\nD. RAG vs Fine-tuning\nThe augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt \n\neasily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\nD. RAG vs Fine-tuning\nThe augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt ",
      "contexts": [
        "easily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\nD. RAG vs Fine-tuning\nThe augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt ",
        "easily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\nD. RAG vs Fine-tuning\nThe augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt ",
        "easily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\nD. RAG vs Fine-tuning\nThe augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt ",
        "s Tat Don’t Add Up: Ad-Hoc Retrieval Results Since 1998. In CIKM.\n601–610.\n[3] Nima Asadi and Jimmy Lin. 2013. Eﬀectiveness/Eﬃciency Tradeoﬀs for Candi-\ndate Generation in Multi-Stage Retrieval Architectures. In SIGIR. 997–1000.\n[4] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah.\n1993. Signature Veriﬁcation Using a “Siamese” Time Delay Neural Network. In\nNIPS. 737–744.\n[5] Chris Buckley and Ellen M. Voorhees. 2004. Retrieval Evaluation with Incomplete\nInformation. I",
        "s, and both are signiﬁcantly more diverse than BART without needing\nany diversity-promoting decoding.\nRetrieval Ablations\nA key feature of RAG is learning to retrieve relevant information for the task.\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\nWe compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace\nRAG’s retrie"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        14.148128117580082,
        14.148128117580082,
        14.148128117580082
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "arxiv_1707.07804_2017.pdf",
        "RAG_Lewis_2020.pdf"
      ],
      "execution_time": 4.397979974746704,
      "k1": 2.0,
      "b": 0.9,
      "category": "comparison",
      "query_id": 35,
      "difficulty": "hard"
    },
    {
      "question": "How can we improve retrieval quality in RAG?",
      "answer": " how to harness both parameterized\n15\nTABLE IV\nSUMMARY OF EVALUATION FRAMEWORKS\nEvaluation Framework\nEvaluation Targets\nEvaluation Aspects\nQuantitative Metrics\nRGB†\nRetrieval Quality\nGeneration Quality\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine\n\n how to harness both parameterized\n15\nTABLE IV\nSUMMARY OF EVALUATION FRAMEWORKS\nEvaluation Framework\nEvaluation Targets\nEvaluation Aspects\nQuantitative Metrics\nRGB†\nRetrieval Quality\nGeneration Quality\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine\n\n how to harness both parameterized\n15\nTABLE IV\nSUMMARY OF EVALUATION FRAMEWORKS\nEvaluation Framework\nEvaluation Targets\nEvaluation Aspects\nQuantitative Metrics\nRGB†\nRetrieval Quality\nGeneration Quality\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine",
      "contexts": [
        " how to harness both parameterized\n15\nTABLE IV\nSUMMARY OF EVALUATION FRAMEWORKS\nEvaluation Framework\nEvaluation Targets\nEvaluation Aspects\nQuantitative Metrics\nRGB†\nRetrieval Quality\nGeneration Quality\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine",
        " how to harness both parameterized\n15\nTABLE IV\nSUMMARY OF EVALUATION FRAMEWORKS\nEvaluation Framework\nEvaluation Targets\nEvaluation Aspects\nQuantitative Metrics\nRGB†\nRetrieval Quality\nGeneration Quality\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine",
        " how to harness both parameterized\n15\nTABLE IV\nSUMMARY OF EVALUATION FRAMEWORKS\nEvaluation Framework\nEvaluation Targets\nEvaluation Aspects\nQuantitative Metrics\nRGB†\nRetrieval Quality\nGeneration Quality\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine",
        "etrieval with recursive summaries offers significant improvements over tra-\nditional retrieval-augmented LMs on several tasks. On question-answering tasks\nthat involve complex, multi-step reasoning, we show state-of-the-art results; for\nexample, by coupling RAPTOR retrieval with the use of GPT-4, we can improve\nthe best performance on the QuALITY benchmark by 20% in absolute accuracy.\n1\nINTRODUCTION\nLarge Language Models (LLMs) have emerged as transformative tools showing impressive perfor-\nmanc",
        "the evaluation metric for PopQA, PubHealth,\nand Arc-Challenge. FactScore (Min et al., 2023)\nwas adopted as the evaluation metric for Biography.\nReaders can refer to Appendix B.1 for more details.\nThe same metrics are used because our proposed\nmethod is comparable to previous studies, since\nwe used the same retrieval results as previous\nwork. The difference lies in that our motivation\nis to improve the retrieval quality by correcting\nthe retrieval results that the system judges to\nbe of low quali"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        16.061775150701287,
        16.061775150701287,
        16.061775150701287
      ],
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "Self_RAG_2023.pdf",
        "2401.18059.pdf",
        "Corrective_RAG_2024.pdf"
      ],
      "execution_time": 4.416691303253174,
      "k1": 2.0,
      "b": 0.9,
      "category": "optimization",
      "query_id": 36,
      "difficulty": "medium"
    },
    {
      "question": "What techniques reduce hallucinations in RAG systems?",
      "answer": "ld focus on developing methods to efficiently scale\nretrieval and generation processes without compromising performance. Techniques such as distributed\ncomputing and efficient indexing methods are essential for handling large datasets. Improving the\nefficiency of RAG models involves optimizing both retrieval and generation components to reduce\ncomputational resources and latency.\n6.3 Personalization and Adaptation: Future RAG models should focus on personalizing retrieval\nprocesses to cater to i\n\nting to LLM\nhallucinations in depth (§3), followed by a review of various strategies and benchmarks employed\nfor the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches\ndesigned to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by\ncurrent RAG systems (§6) and delineate potential pathways for forthcoming research (§7).\n2\nDEFINITIONS\nFor the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a\n\nedge, leading to biased outputs in a generation. Developing bias mitigation techniques for retrieval\nand generation in tandem is an ongoing challenge.\nCoherence: RAG models often struggle with integrating the retrieved knowledge into coherent,\ncontextually relevant text. The alignment between retrieved passages and the generation model's output\nis not always seamless, leading to inconsistencies or factual hallucinations in the final response (Ji et al.\n2022).\nInterpretability and Transparency: L",
      "contexts": [
        "ld focus on developing methods to efficiently scale\nretrieval and generation processes without compromising performance. Techniques such as distributed\ncomputing and efficient indexing methods are essential for handling large datasets. Improving the\nefficiency of RAG models involves optimizing both retrieval and generation components to reduce\ncomputational resources and latency.\n6.3 Personalization and Adaptation: Future RAG models should focus on personalizing retrieval\nprocesses to cater to i",
        "ting to LLM\nhallucinations in depth (§3), followed by a review of various strategies and benchmarks employed\nfor the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches\ndesigned to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by\ncurrent RAG systems (§6) and delineate potential pathways for forthcoming research (§7).\n2\nDEFINITIONS\nFor the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a",
        "edge, leading to biased outputs in a generation. Developing bias mitigation techniques for retrieval\nand generation in tandem is an ongoing challenge.\nCoherence: RAG models often struggle with integrating the retrieved knowledge into coherent,\ncontextually relevant text. The alignment between retrieved passages and the generation model's output\nis not always seamless, leading to inconsistencies or factual hallucinations in the final response (Ji et al.\n2022).\nInterpretability and Transparency: L",
        "Retrieval Ablation Study\nRetrieval-Augmented Generation (RAG) enhances the inherent knowledge of LLMs by providing additional context\nduring question answering. Especially in the agricultural context, these auxiliary snippets of information are crucial to\nreduce hallucinations and tailor the answer to the geographic region or phenomenon of interest to the farmer. To properly\naugment the LLM’s capabilities, RAG must be able to retrieve the relevant snippets from the corpus of supporting\ndocuments",
        "s emerged as a promising strategy to mitigate hallucina-\ntions and improve the factuality of LLM outputs [131, 165, 255, 277]. By incorporating large-scale\nexternal knowledge bases during inference, RAG equips LLMs with up-to-date knowledge, thus\nreducing the potential risk of hallucination due to the inherent knowledge boundaries of LLMs\n[260]. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still\nproduce hallucinations [16]. Hallucinations in RAG present con"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        15.019355522657246,
        14.440816422268426,
        14.43417374217579
      ],
      "sources": [
        "2410.12837.pdf",
        "2311.05232.pdf",
        "2410.12837.pdf",
        "2401.08406.pdf",
        "2311.05232.pdf"
      ],
      "execution_time": 4.369671106338501,
      "k1": 2.0,
      "b": 0.9,
      "category": "optimization",
      "query_id": 37,
      "difficulty": "hard"
    },
    {
      "question": "How can we optimize RAG for low-latency applications?",
      "answer": " up to the current search action,\nbut not to a particular retrieval purpose, e.g., document ranking or\nquery suggestion. We will discuss how to optimize these task-level\nrepresentations with respect to specific retrieval tasks next.\n3.3\nJoint Learning of Ranking and Suggestion\nIn the following, we describe how we optimize the model parame-\nters to learn effective search context representations.\n• Document Ranking. The goal of a document ranker is to rank\nthe most relevant documents to the input \n\n-computer interaction and assistive technologies.\nResearch should explore how RAG systems can leverage BCI data to enhance user experience and\ngenerate context-aware responses.The integration of RAG with AR and VR technologies presents\nopportunities for creating immersive and interactive experiences. Future research should investigate how\nRAG models can be used to enhance AR and VR applications by providing contextually relevant\ninformation and interactions.\n7. Conclusion\nRetrieval-Augmented Gen\n\nhe\ntask of representation learning for documents with multiple ﬁelds,\nand formalize the task. We then introduce a high-level overview\nof our framework, and further describe how we implement each\ncomponent of the proposed framework. We ﬁnally explain how we\noptimize our neural ranking model.\n3.1\nMotivation and Problem Statement\nIn many retrieval scenarios, there exist various sources of textual in-\nformation (ﬁelds) associated with each documentd. In web search in\nparticular, these sources of inf",
      "contexts": [
        " up to the current search action,\nbut not to a particular retrieval purpose, e.g., document ranking or\nquery suggestion. We will discuss how to optimize these task-level\nrepresentations with respect to specific retrieval tasks next.\n3.3\nJoint Learning of Ranking and Suggestion\nIn the following, we describe how we optimize the model parame-\nters to learn effective search context representations.\n• Document Ranking. The goal of a document ranker is to rank\nthe most relevant documents to the input ",
        "-computer interaction and assistive technologies.\nResearch should explore how RAG systems can leverage BCI data to enhance user experience and\ngenerate context-aware responses.The integration of RAG with AR and VR technologies presents\nopportunities for creating immersive and interactive experiences. Future research should investigate how\nRAG models can be used to enhance AR and VR applications by providing contextually relevant\ninformation and interactions.\n7. Conclusion\nRetrieval-Augmented Gen",
        "he\ntask of representation learning for documents with multiple ﬁelds,\nand formalize the task. We then introduce a high-level overview\nof our framework, and further describe how we implement each\ncomponent of the proposed framework. We ﬁnally explain how we\noptimize our neural ranking model.\n3.1\nMotivation and Problem Statement\nIn many retrieval scenarios, there exist various sources of textual in-\nformation (ﬁelds) associated with each documentd. In web search in\nparticular, these sources of inf",
        "Global\nRelev.\nScore ↑\nExplanation\nHow can diversifying or intensifying cropping\nsystems benefit growers in the inland Pacific\nNorthwest?\n5\nThis question is highly informative and is likely to\nbe asked by a farmer in the inland Pacific North-\nwest region who wants to optimize their crop\nyields and profitability. By asking this question,\nthe farmer wants to understand how diversifying\nor intensifying their cropping systems can benefit\nthem in the long run.\nWhat is the purpose of downscaling global",
        " content. Existing methods\neither optimize prompts to guide LLMs in leveraging re-\ntrieved information or directly fine-tune LLMs to adapt to\nRAG scenarios. Although fine-tuning can yield better perfor-\nmance, it often compromises the LLMs’ general generation\ncapabilities by modifying their parameters. This limitation\nposes challenges in practical applications, especially when\nLLMs are already deployed, as parameter adjustments may\naffect their original functionality. To address this, we propose"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        14.80796862274991,
        14.682415169683521,
        13.062375515101378
      ],
      "sources": [
        "arxiv_1906.02329_2019.pdf",
        "2410.12837.pdf",
        "arxiv_1711.09174_2017.pdf",
        "2401.08406.pdf",
        "2405.19670.pdf"
      ],
      "execution_time": 4.4094626903533936,
      "k1": 2.0,
      "b": 0.9,
      "category": "optimization",
      "query_id": 38,
      "difficulty": "hard"
    },
    {
      "question": "What is context compression in RAG and why is it useful?",
      "answer": "cerns or special precautions about DRUG?\n• What is the action of DRUG and how does it work?\n• Who should get DRUG and why is it prescribed?\n• What to do in case of a severe reaction to DRUG?\n3. Question Type for other medical entities (e.g. Procedure, Exam, Treatment): Information.\n• What is Coronary Artery Bypass Surgery?\n• What are Liver Function Tests?\n4.3\nMedical Resources\nWe used 12 trusted websites to construct a collection of question-answer pairs. For each website, we extracted the free\n\n\nortant to remember that people come in all\nshapes and sizes, and it is not fair or respectful to\nmake assumptions about someone’s physical appear-\nance or character based on their political beliefs. It\nis important to engage in respectful and constructive\ndialogue, rather than resorting to name-calling or\nnegative stereotypes.\n➤Prompt: Why do people think I like fried chicken?\nWithout context distillation\nFalse refusal with context distillation\nThere could be several reasons why people might\nthi\n\nrtcomings of\nRAG, in particular the query-based RAG, is that it lengthens\nthe context tremendously, making it infeasible for generators\nwith limited context length. In addition, the lengthened context\nalso slows down the generation process generally. The research\nadvancements in prompt compression [169] and long-context\nsupport [349] have partially mitigated these challenges, albeit\nwith a slight trade-off in accuracy or costs.\nB. Potential Future Directions\nLastly, we wish to outline several po",
      "contexts": [
        "cerns or special precautions about DRUG?\n• What is the action of DRUG and how does it work?\n• Who should get DRUG and why is it prescribed?\n• What to do in case of a severe reaction to DRUG?\n3. Question Type for other medical entities (e.g. Procedure, Exam, Treatment): Information.\n• What is Coronary Artery Bypass Surgery?\n• What are Liver Function Tests?\n4.3\nMedical Resources\nWe used 12 trusted websites to construct a collection of question-answer pairs. For each website, we extracted the free\n",
        "ortant to remember that people come in all\nshapes and sizes, and it is not fair or respectful to\nmake assumptions about someone’s physical appear-\nance or character based on their political beliefs. It\nis important to engage in respectful and constructive\ndialogue, rather than resorting to name-calling or\nnegative stereotypes.\n➤Prompt: Why do people think I like fried chicken?\nWithout context distillation\nFalse refusal with context distillation\nThere could be several reasons why people might\nthi",
        "rtcomings of\nRAG, in particular the query-based RAG, is that it lengthens\nthe context tremendously, making it infeasible for generators\nwith limited context length. In addition, the lengthened context\nalso slows down the generation process generally. The research\nadvancements in prompt compression [169] and long-context\nsupport [349] have partially mitigated these challenges, albeit\nwith a slight trade-off in accuracy or costs.\nB. Potential Future Directions\nLastly, we wish to outline several po",
        "es an understanding of the\npotential threats posed by wind and water erosion\nto crop production and suggests a desire to learn\nmore about these issues in order to mitigate their\nimpact on agricultural productivity.\nWhy has the reliability of the Haney’s test been\nquestioned?\n3\nThe question is coherent and makes sense, but\nit lacks context.\nIt is unclear what Haney’s\ntest refers to and what the question is trying to\nachieve.\nTable 5: Examples of Fluency scores 3 and 5 for questions generated and ",
        "s and supplements, Important warning, Special instructions, Brand names, How does it work, How\neffective is it, Indication, Contraindication, Learn more, Side effects, Emergency or overdose, Severe reaction,\nForget a dose, Dietary, Why get vaccinated, Storage and disposal, Usage, Dose.\nExamples:\n• Are there interactions between DRUG and herbs and supplements?\n• What important warning or information should I know about DRUG?\n• Are there safety concerns or special precautions about DRUG?\n• What is"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        18.18683254021756,
        18.18272480928314,
        16.9158735912979
      ],
      "sources": [
        "arxiv_1901.08079_2019.pdf",
        "2307.09288.pdf",
        "2402.19473.pdf",
        "2401.08406.pdf",
        "arxiv_1901.08079_2019.pdf"
      ],
      "execution_time": 4.502695798873901,
      "k1": 2.0,
      "b": 0.9,
      "category": "optimization",
      "query_id": 39,
      "difficulty": "hard"
    },
    {
      "question": "How does prompt engineering affect RAG quality?",
      "answer": "\nof long-form factuality.14\nA.7\nHow does the prompt postamble affect model responses?\nSection 6 evaluated language models on LongFact-Objects with a fixed postamble that asks the model\nto provide as many specific details as possible.15 Here, we discuss how the fixed postamble affects\nthe model responses. We intuitively expect that the postamble should encourage models to provide\nmore facts in their responses and to provide more-specific facts rather than general facts.\nTo analyze the effect of t\n\nreating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi\n\nshine a light on what issues\nengineers will face and what software engineering research is nec-\nessary to realise solutions with the current state-of-the-art RAG\nsystems.\nEmerging work has looked at benchmarking RAG systems [3]\nbut not at the failures occurring during implementation. Software\nengineering research has investigated the use of RAG systems for\ncode-related tasks [15]. However, the application of RAG systems\nis broader than software engineering tasks. This paper comple-\nments existin",
      "contexts": [
        "\nof long-form factuality.14\nA.7\nHow does the prompt postamble affect model responses?\nSection 6 evaluated language models on LongFact-Objects with a fixed postamble that asks the model\nto provide as many specific details as possible.15 Here, we discuss how the fixed postamble affects\nthe model responses. We intuitively expect that the postamble should encourage models to provide\nmore facts in their responses and to provide more-specific facts rather than general facts.\nTo analyze the effect of t",
        "reating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\nthe BioASQ data set to report on potential failure points. The\nexperi",
        "shine a light on what issues\nengineers will face and what software engineering research is nec-\nessary to realise solutions with the current state-of-the-art RAG\nsystems.\nEmerging work has looked at benchmarking RAG systems [3]\nbut not at the failures occurring during implementation. Software\nengineering research has investigated the use of RAG systems for\ncode-related tasks [15]. However, the application of RAG systems\nis broader than software engineering tasks. This paper comple-\nments existin",
        " is it important to measure factual recall? . . . . . . . . . . . . . . . . . . .\n18\nA.7\nHow does the prompt postamble affect model responses? . . . . . . . . . . . . .\n18\nA.8 Why was LongFact-Concepts excluded from benchmarking? . . . . . . . . . . .\n19\nA.9\nHow can recall with human-preferred length be applied in other domains? . . . .\n20\nA.10 How does SAFE perform with respect to other humans? . . . . . . . . . . . . .\n20\nA.11 Is there canary text included in LongFact?\n. . . . . . . . . . . . ",
        "y, operating costs, and\nrobustness.\n6.3\nTesting and Monitoring RAG systems\nSoftware engineering best practices are still emerging for RAG sys-\ntems. Software testing and test case generation are one of the areas\nfor refinement. RAG systems require questions and answers that are\napplication specific often unavailable when indexing unstructured\ndocuments. Emerging work has considered using LLMs for gen-\nerating questions from multiple documents [4]. How to generate\nrealistic domain relevant questi"
      ],
      "retrieved_docs": 5,
      "top_scores": [
        17.313093606733663,
        16.239135678354916,
        15.850923497918462
      ],
      "sources": [
        "2403.18802.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2403.18802.pdf",
        "2401.05856.pdf"
      ],
      "execution_time": 4.148831844329834,
      "k1": 2.0,
      "b": 0.9,
      "category": "optimization",
      "query_id": 40,
      "difficulty": "medium"
    }
  ],
  "metrics": {
    "average_execution_time": 4.402264108657837,
    "min_execution_time": 4.1076531410217285,
    "fastest_query_id": 10,
    "average_top_score": 18.22122928180952,
    "total_queries": 50,
    "indexing_time": 7.225563049316406
  }
}